{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"community/get-involved","text":"Welcome to the AWS Copilot Community \u2764\ufe0f This is a place to share your [ applications, articles, videos ] and any other resources related to the AWS Copilot CLI \ud83d\udc69\u200d\u2708\ufe0f. You can get involved with the CLI by: Creating issues in the GitHub repository Joining the chat with fellow Copilots","title":"Get Involved"},{"location":"community/get-involved#welcome-to-the-aws-copilot-community","text":"This is a place to share your [ applications, articles, videos ] and any other resources related to the AWS Copilot CLI \ud83d\udc69\u200d\u2708\ufe0f. You can get involved with the CLI by: Creating issues in the GitHub repository Joining the chat with fellow Copilots","title":"Welcome to the AWS Copilot Community \u2764\ufe0f"},{"location":"community/guides","text":"Share your applications, videos, and blog posts with fellow Copilots! Blog posts Title Description AWS Copilot \u2013 ECS by @ksivamuthu Here we have a three-part series devoted to Copilot! Sivamuthu Kumar demos launching a .NET coffeeshop app, exploring pipelines, storage, addons, and logs along the way. Deploying a Telegram Bot to AWS ECS with AWS Copilot by Christian Prado Ciokler Christian takes you through building a Node.js app, step-by-step, with Copilot on Docker Desktop Windows Subsystem for Linux (WSL) 2. Deploying a Containerized Web App with AWS Copilot by @edkruegerdata and Dylan Rossi Edward and Dylan start from the basics, including IAM account creation and AWS CLI installation. (Note: you'll also need to install AWS Copilot.) They demonstrate using Postman to test the deployed app. Pilot your containers like a boss with AWS Copilot! by @FlolightC Florian simplifies getting started with AWS Copilot by demoing deployment of a sample app from start to finish, including links to both required and optional resources. Use AWS Copilot CLI to deploy containers on an existing infrastructure - Tutorial by @dannysteenman Danny explains how to use your existing VPC and subnets with AWS Copilot to quickly set up a working container environment. Follow along as he deploys a Django app with an RDS Postgres database and Elasticache Redis cluster. Automatically deploying your container application with AWS Copilot by @nathankpeck Nathan shows how to set up a release pipeline with the CLI that builds, pushes, and deploys an application. Finally, he sets up integration tests for validation before releasing to production. Deploying containers with the AWS Copilot CLI by @maartenbruntink Maarten shows how to use the AWS Copilot CLI to deploy the sample Docker voting app , which showcases how to set up your own Redis and Postgres servers. In the second part , he automates the release process. AWS Copilot: an application-first CLI for containers on AWS by @efekarakus Efe walks through the design tenets of the CLI: why they were chosen, how they map to Copilot features, and the vision for how the CLI will evolve in the future. Introducing AWS Copilot by @nathankpeck Nathan explains how with the AWS Copilot CLI you can go from idea to implementation much faster, with the confidence that the infrastructure you have deployed has production-ready configuration. Videos Title Description Build, Operate, and Observe a Containerized Application on AWS Fargate by @nathankpeck AWS's indefatigable Nathan Peck is back! In this session for AWS Application Modernization Day, he champions containers and demonstrates the deployment of a load-balanced web service and a load-testing scheduled job. Using Jenkins and AWS Copilot CLI to Deploy to AWS App Runner by @DarinPope Darin, a developer advocate for CloudBees, uses AWS Copilot and a Jenkinsfile to deploy to AWS AppRunner. Deploy a Web App with AWS CoPilot by @edkruegerdata This three-part series includes \"Creating an Administrator User Group in AWS,\" \"Create an IAM Account on AWS,\" and \"Deploying the Web App.\" Container Day: Amazon ECS Edition-- Demo: deploy a modern application on AWS Fargate with ECS Copilot by @efekarakus and @realadamjkeller Efe goes beyond the basics, demoing how to grow an application ; he highlights the alias field, secrets and environment variables, addons, and more. Copilot\u2013 What AWS ECS and Fargate Container Management Should Have Been by @vfarcic Viktor lauds Copilot for its simplicity and ease of use in this clear, step-by-step demo. AWS Copilot CLI v1.8.0 Release Highlights by @realadamjkeller Adam walks through the features that shipped with version 1.8.0 of the AWS Copilot CLI, including alias , the svc status update, and container dependency. DevBeardOps with !Cobus & !Darko: Playing with Containers by @cobusbernard and @darkosubotica Learn alongside Cobus and Darko as they experiment with AWS Copilot, from installation to pipeline setup. Containers from the Couch series by @realadamjkeller , @brentContained , and guests Join Adam and Brent to learn about many of the existing features of AWS Copilot with fun demos. Watch as they set up a three-tier application with autoscaling ; create a continuous delivery pipeline with integration tests ; show how easy it is to 'exec' with Copilot ; and highlight ephemeral storage and scheduled job features. AWS Copilot and another 6 ways to easily deploy apps to AWS by Kirill Shirinkin Kirill Shirinkin explores ways to easily deploy a new product on AWS - including a new favorite one, AWS Copilot. AWS re:Invent 2020: AWS Copilot: Simplifying container development by @efekarakus Learn about the motivation behind AWS Copilot, get an overview of the existing commands and a demo of how to deploy a three-tier application. How to Deploy a .NET Application to Amazon Elastic Container Service (ECS) with AWS Copilot by @ignacioafuentes Get a demo on how to build and deploy a .NET application on Amazon ECS and AWS Fargate. AWS What's Next by @nathankpeck and @efekarakus Nathan and Efe discuss what makes AWS Copilot unique compared to other infrastructure provisioning tools and then demo an overview of the existing commands. Code samples Repository Description Key features github.com/efekarakus/day2-with-copilot A REST API coffeeshop application that goes beyond the basics, demoed in the \"Container Day\" video , above. alias , secrets, DynamoDB, Redis github.com/bvtujo/copilot-wordpress A WordPress installation launched by AWS Copilot with step-by-step instructions and options for customization. EFS , MySQL RDS cluster, autoscaling #2378 Show-and-tell with sample templates for launching isolated tasks in private subnets. VPC Endpoints, custom environments github.com/copilot-example-voting-app , #1745 A voting application distributed over three ECS services created with AWS Copilot. Amazon Aurora PostgreSQL database, service discovery, autoscaling #1925 Show-and-tell explaining how you can do continuous deployments from branches with AWS Copilot pipelines. Branch-based deploys, AWS CodePipeline Workshops Title Description ECS Workshop In this workshop, we deploy a three-tier microservices application using the AWS Copilot CLI. (Optional accompanying video here .)","title":"Guides and Resources"},{"location":"community/guides#blog-posts","text":"Title Description AWS Copilot \u2013 ECS by @ksivamuthu Here we have a three-part series devoted to Copilot! Sivamuthu Kumar demos launching a .NET coffeeshop app, exploring pipelines, storage, addons, and logs along the way. Deploying a Telegram Bot to AWS ECS with AWS Copilot by Christian Prado Ciokler Christian takes you through building a Node.js app, step-by-step, with Copilot on Docker Desktop Windows Subsystem for Linux (WSL) 2. Deploying a Containerized Web App with AWS Copilot by @edkruegerdata and Dylan Rossi Edward and Dylan start from the basics, including IAM account creation and AWS CLI installation. (Note: you'll also need to install AWS Copilot.) They demonstrate using Postman to test the deployed app. Pilot your containers like a boss with AWS Copilot! by @FlolightC Florian simplifies getting started with AWS Copilot by demoing deployment of a sample app from start to finish, including links to both required and optional resources. Use AWS Copilot CLI to deploy containers on an existing infrastructure - Tutorial by @dannysteenman Danny explains how to use your existing VPC and subnets with AWS Copilot to quickly set up a working container environment. Follow along as he deploys a Django app with an RDS Postgres database and Elasticache Redis cluster. Automatically deploying your container application with AWS Copilot by @nathankpeck Nathan shows how to set up a release pipeline with the CLI that builds, pushes, and deploys an application. Finally, he sets up integration tests for validation before releasing to production. Deploying containers with the AWS Copilot CLI by @maartenbruntink Maarten shows how to use the AWS Copilot CLI to deploy the sample Docker voting app , which showcases how to set up your own Redis and Postgres servers. In the second part , he automates the release process. AWS Copilot: an application-first CLI for containers on AWS by @efekarakus Efe walks through the design tenets of the CLI: why they were chosen, how they map to Copilot features, and the vision for how the CLI will evolve in the future. Introducing AWS Copilot by @nathankpeck Nathan explains how with the AWS Copilot CLI you can go from idea to implementation much faster, with the confidence that the infrastructure you have deployed has production-ready configuration.","title":"Blog posts"},{"location":"community/guides#videos","text":"Title Description Build, Operate, and Observe a Containerized Application on AWS Fargate by @nathankpeck AWS's indefatigable Nathan Peck is back! In this session for AWS Application Modernization Day, he champions containers and demonstrates the deployment of a load-balanced web service and a load-testing scheduled job. Using Jenkins and AWS Copilot CLI to Deploy to AWS App Runner by @DarinPope Darin, a developer advocate for CloudBees, uses AWS Copilot and a Jenkinsfile to deploy to AWS AppRunner. Deploy a Web App with AWS CoPilot by @edkruegerdata This three-part series includes \"Creating an Administrator User Group in AWS,\" \"Create an IAM Account on AWS,\" and \"Deploying the Web App.\" Container Day: Amazon ECS Edition-- Demo: deploy a modern application on AWS Fargate with ECS Copilot by @efekarakus and @realadamjkeller Efe goes beyond the basics, demoing how to grow an application ; he highlights the alias field, secrets and environment variables, addons, and more. Copilot\u2013 What AWS ECS and Fargate Container Management Should Have Been by @vfarcic Viktor lauds Copilot for its simplicity and ease of use in this clear, step-by-step demo. AWS Copilot CLI v1.8.0 Release Highlights by @realadamjkeller Adam walks through the features that shipped with version 1.8.0 of the AWS Copilot CLI, including alias , the svc status update, and container dependency. DevBeardOps with !Cobus & !Darko: Playing with Containers by @cobusbernard and @darkosubotica Learn alongside Cobus and Darko as they experiment with AWS Copilot, from installation to pipeline setup. Containers from the Couch series by @realadamjkeller , @brentContained , and guests Join Adam and Brent to learn about many of the existing features of AWS Copilot with fun demos. Watch as they set up a three-tier application with autoscaling ; create a continuous delivery pipeline with integration tests ; show how easy it is to 'exec' with Copilot ; and highlight ephemeral storage and scheduled job features. AWS Copilot and another 6 ways to easily deploy apps to AWS by Kirill Shirinkin Kirill Shirinkin explores ways to easily deploy a new product on AWS - including a new favorite one, AWS Copilot. AWS re:Invent 2020: AWS Copilot: Simplifying container development by @efekarakus Learn about the motivation behind AWS Copilot, get an overview of the existing commands and a demo of how to deploy a three-tier application. How to Deploy a .NET Application to Amazon Elastic Container Service (ECS) with AWS Copilot by @ignacioafuentes Get a demo on how to build and deploy a .NET application on Amazon ECS and AWS Fargate. AWS What's Next by @nathankpeck and @efekarakus Nathan and Efe discuss what makes AWS Copilot unique compared to other infrastructure provisioning tools and then demo an overview of the existing commands.","title":"Videos"},{"location":"community/guides#code-samples","text":"Repository Description Key features github.com/efekarakus/day2-with-copilot A REST API coffeeshop application that goes beyond the basics, demoed in the \"Container Day\" video , above. alias , secrets, DynamoDB, Redis github.com/bvtujo/copilot-wordpress A WordPress installation launched by AWS Copilot with step-by-step instructions and options for customization. EFS , MySQL RDS cluster, autoscaling #2378 Show-and-tell with sample templates for launching isolated tasks in private subnets. VPC Endpoints, custom environments github.com/copilot-example-voting-app , #1745 A voting application distributed over three ECS services created with AWS Copilot. Amazon Aurora PostgreSQL database, service discovery, autoscaling #1925 Show-and-tell explaining how you can do continuous deployments from branches with AWS Copilot pipelines. Branch-based deploys, AWS CodePipeline","title":"Code samples"},{"location":"community/guides#workshops","text":"Title Description ECS Workshop In this workshop, we deploy a three-tier microservices application using the AWS Copilot CLI. (Optional accompanying video here .)","title":"Workshops"},{"location":"docs/credentials","text":"This section explains our recommendations around credentials to provide the best experience with the AWS Copilot CLI. Application credentials Copilot uses the AWS credentials from the default credential provider chain to store and look up your application's metadata : which services and environments belong to it. Tip We recommend using a named profile to store your application's credentials. The most convenient way is having the [default] profile point to your application's credentials: # ~/.aws/credentials [default] aws_access_key_id = AKIAIOSFODNN7EXAMPLE aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # ~/.aws/config [default] region = us-west-2 Alternatively, you can set the AWS_PROFILE environment variable to point to a different named profile. For example, we can have a [my-app] profile that can be used for your Copilot application instead of the [default] profile. Note You cannot use the AWS account root user credentials for your application. Please first create an IAM user instead as described here . # ~/.aws/config [profile my-app] credential_process = /opt/bin/awscreds-custom --username helen region = us-west-2 # Then you can run your Copilot commands leveraging the alternative profile: $ export AWS_PROFILE = my-app $ copilot deploy Caution We do not recommend using the environment variables: AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , AWS_SESSION_TOKEN directly to look up your application's metadata because if they're overridden or expired, Copilot will not be able to look up your services or environments. To learn more about all the supported config file settings: Configuration and credential file settings . Environment credentials Copilot environments can be created in AWS accounts and regions separate from your application's. While initializing an environment, Copilot will prompt you to enter temporary credentials or a named profile to create your environment: $ copilot env init Name: prod-iad Which credentials would you like to use to create prod-iad? > Enter temporary credentials > [ profile default ] > [ profile test ] > [ profile prod-iad ] > [ profile prod-pdx ] Unlike the Application credentials , the AWS credentials for an environment are only needed for creation or deletion. Therefore, it's safe to use the values from temporary environment variables. Copilot prompts or takes the credentials as flags because the default chain is reserved for your application credentials.","title":"Credentials"},{"location":"docs/credentials#application-credentials","text":"Copilot uses the AWS credentials from the default credential provider chain to store and look up your application's metadata : which services and environments belong to it. Tip We recommend using a named profile to store your application's credentials. The most convenient way is having the [default] profile point to your application's credentials: # ~/.aws/credentials [default] aws_access_key_id = AKIAIOSFODNN7EXAMPLE aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # ~/.aws/config [default] region = us-west-2 Alternatively, you can set the AWS_PROFILE environment variable to point to a different named profile. For example, we can have a [my-app] profile that can be used for your Copilot application instead of the [default] profile. Note You cannot use the AWS account root user credentials for your application. Please first create an IAM user instead as described here . # ~/.aws/config [profile my-app] credential_process = /opt/bin/awscreds-custom --username helen region = us-west-2 # Then you can run your Copilot commands leveraging the alternative profile: $ export AWS_PROFILE = my-app $ copilot deploy Caution We do not recommend using the environment variables: AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , AWS_SESSION_TOKEN directly to look up your application's metadata because if they're overridden or expired, Copilot will not be able to look up your services or environments. To learn more about all the supported config file settings: Configuration and credential file settings .","title":"Application credentials"},{"location":"docs/credentials#environment-credentials","text":"Copilot environments can be created in AWS accounts and regions separate from your application's. While initializing an environment, Copilot will prompt you to enter temporary credentials or a named profile to create your environment: $ copilot env init Name: prod-iad Which credentials would you like to use to create prod-iad? > Enter temporary credentials > [ profile default ] > [ profile test ] > [ profile prod-iad ] > [ profile prod-pdx ] Unlike the Application credentials , the AWS credentials for an environment are only needed for creation or deletion. Therefore, it's safe to use the values from temporary environment variables. Copilot prompts or takes the credentials as flags because the default chain is reserved for your application credentials.","title":"Environment credentials"},{"location":"docs/overview","text":"Welcome to the AWS Copilot CLI \ud83c\udf89 The Copilot CLI is a tool for developers to build, release, and operate production-ready containerized applications on AWS App Runner, Amazon ECS, and AWS Fargate. From getting started, pushing to staging, and releasing to production, Copilot can help manage the entire lifecycle of your application development. Installing You can install AWS Copilot through Homebrew or by downloading the binaries directly. If you don't want to use Homebrew, you can install manually . $ brew install aws/tap/copilot-cli","title":"Overview"},{"location":"docs/overview#installing","text":"You can install AWS Copilot through Homebrew or by downloading the binaries directly. If you don't want to use Homebrew, you can install manually . $ brew install aws/tap/copilot-cli","title":"Installing"},{"location":"docs/commands/app-delete","text":"app delete $ copilot app delete [ flags ] What does it do? copilot app delete deletes all resources associated with an application. What are the flags? -h, --help help for delete --yes Skips confirmation prompt. Examples Force delete the application. $ copilot app delete --yes","title":"app delete"},{"location":"docs/commands/app-delete#app-delete","text":"$ copilot app delete [ flags ]","title":"app delete"},{"location":"docs/commands/app-delete#what-does-it-do","text":"copilot app delete deletes all resources associated with an application.","title":"What does it do?"},{"location":"docs/commands/app-delete#what-are-the-flags","text":"-h, --help help for delete --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/app-delete#examples","text":"Force delete the application. $ copilot app delete --yes","title":"Examples"},{"location":"docs/commands/app-init","text":"app init $ copilot app init [ name ] [ flags ] What does it do? copilot app init creates a new application within the directory that will contain your service(s). After you answer the questions, the CLI creates AWS Identity and Access Management roles to manage the release infrastructure for your services. You'll also see a new sub-directory created under your working directory: copilot/ . The copilot directory will hold the manifest files and additional infrastructure for your services. Typically, you don't need to run app init ( init does all the same work) unless you want to use a custom domain name or AWS tags. What are the flags? Like all commands in the Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: --domain string Optional. Your existing custom domain name. -h, --help help for init --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) The --domain flag allows you to specify a domain name registered with Amazon Route 53 in your app's account. This will allow all the services in your app to share the same domain name. You'll be able to access your services at: https://{svcName}.{envName}.{appName}.{domain} The --resource-tags flags allows you to add your custom tags to all the resources in your app. For example: copilot app init --resource-tags department=MyDept,team=MyTeam Examples Create a new application named \"my-app\". $ copilot app init my-app Create a new application with an existing domain name in Amazon Route53. $ copilot app init --domain example.com Create a new application with resource tags. $ copilot app init --resource-tags department = MyDept,team = MyTeam What does it look like?","title":"app init"},{"location":"docs/commands/app-init#app-init","text":"$ copilot app init [ name ] [ flags ]","title":"app init"},{"location":"docs/commands/app-init#what-does-it-do","text":"copilot app init creates a new application within the directory that will contain your service(s). After you answer the questions, the CLI creates AWS Identity and Access Management roles to manage the release infrastructure for your services. You'll also see a new sub-directory created under your working directory: copilot/ . The copilot directory will hold the manifest files and additional infrastructure for your services. Typically, you don't need to run app init ( init does all the same work) unless you want to use a custom domain name or AWS tags.","title":"What does it do?"},{"location":"docs/commands/app-init#what-are-the-flags","text":"Like all commands in the Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: --domain string Optional. Your existing custom domain name. -h, --help help for init --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) The --domain flag allows you to specify a domain name registered with Amazon Route 53 in your app's account. This will allow all the services in your app to share the same domain name. You'll be able to access your services at: https://{svcName}.{envName}.{appName}.{domain} The --resource-tags flags allows you to add your custom tags to all the resources in your app. For example: copilot app init --resource-tags department=MyDept,team=MyTeam","title":"What are the flags?"},{"location":"docs/commands/app-init#examples","text":"Create a new application named \"my-app\". $ copilot app init my-app Create a new application with an existing domain name in Amazon Route53. $ copilot app init --domain example.com Create a new application with resource tags. $ copilot app init --resource-tags department = MyDept,team = MyTeam","title":"Examples"},{"location":"docs/commands/app-init#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/app-ls","text":"app ls $ copilot app ls [ flags ] What does it do? copilot app ls lists all the Copilot applications in your account. What are the flags? -h, --help help for ls Examples List all the applications in your account and region. $ copilot app ls What does it look like?","title":"app ls"},{"location":"docs/commands/app-ls#app-ls","text":"$ copilot app ls [ flags ]","title":"app ls"},{"location":"docs/commands/app-ls#what-does-it-do","text":"copilot app ls lists all the Copilot applications in your account.","title":"What does it do?"},{"location":"docs/commands/app-ls#what-are-the-flags","text":"-h, --help help for ls","title":"What are the flags?"},{"location":"docs/commands/app-ls#examples","text":"List all the applications in your account and region. $ copilot app ls","title":"Examples"},{"location":"docs/commands/app-ls#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/app-show","text":"app show $ copilot app show [ flags ] What does it do? copilot app show shows configuration, environments and services for an application. What are the flags? -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the application. Examples Shows info about the application \"my-app\". $ copilot app show -n my-app What does it look like?","title":"app show"},{"location":"docs/commands/app-show#app-show","text":"$ copilot app show [ flags ]","title":"app show"},{"location":"docs/commands/app-show#what-does-it-do","text":"copilot app show shows configuration, environments and services for an application.","title":"What does it do?"},{"location":"docs/commands/app-show#what-are-the-flags","text":"-h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the application.","title":"What are the flags?"},{"location":"docs/commands/app-show#examples","text":"Shows info about the application \"my-app\". $ copilot app show -n my-app","title":"Examples"},{"location":"docs/commands/app-show#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/app-upgrade","text":"app upgrade $ copilot app upgrade [ flags ] What does it do? copilot app upgrade upgrades the template of an application to the latest version. What are the flags? -h, --help help for upgrade -n, --name string Name of the application. Examples Upgrade the application \"my-app\" to the latest version $ copilot app upgrade -n my-app","title":"app upgrade"},{"location":"docs/commands/app-upgrade#app-upgrade","text":"$ copilot app upgrade [ flags ]","title":"app upgrade"},{"location":"docs/commands/app-upgrade#what-does-it-do","text":"copilot app upgrade upgrades the template of an application to the latest version.","title":"What does it do?"},{"location":"docs/commands/app-upgrade#what-are-the-flags","text":"-h, --help help for upgrade -n, --name string Name of the application.","title":"What are the flags?"},{"location":"docs/commands/app-upgrade#examples","text":"Upgrade the application \"my-app\" to the latest version $ copilot app upgrade -n my-app","title":"Examples"},{"location":"docs/commands/completion","text":"completion $ copilot completion [shell] [flags] What does it do? copilot completion prints shell completion code for bash, zsh or fish. The code must be evaluated to provide interactive completion of commands. See the help menu for instructions on how to setup auto-completion for your respective shell. What are the flags? -h, --help help for completion Examples Install zsh completion. $ source < ( copilot completion zsh ) $ copilot completion zsh > \" ${ fpath [1] } /_copilot\" # to autoload on startup Install bash completion on macOS using homebrew. $ brew install bash-completion # if running 3.2 $ brew install bash-completion@2 # if running Bash 4.1+ $ copilot completion bash > /usr/local/etc/bash_completion.d Install bash completion on linux $ source < ( copilot completion bash ) $ copilot completion bash > copilot.sh $ sudo mv copilot.sh /etc/bash_completion.d/copilot Install fish completion on linux $ source < ( copilot completion fish ) $ copilot completion fish > ~/.config/fish/completions/copilot.fish","title":"completion"},{"location":"docs/commands/completion#completion","text":"$ copilot completion [shell] [flags]","title":"completion"},{"location":"docs/commands/completion#what-does-it-do","text":"copilot completion prints shell completion code for bash, zsh or fish. The code must be evaluated to provide interactive completion of commands. See the help menu for instructions on how to setup auto-completion for your respective shell.","title":"What does it do?"},{"location":"docs/commands/completion#what-are-the-flags","text":"-h, --help help for completion","title":"What are the flags?"},{"location":"docs/commands/completion#examples","text":"Install zsh completion. $ source < ( copilot completion zsh ) $ copilot completion zsh > \" ${ fpath [1] } /_copilot\" # to autoload on startup Install bash completion on macOS using homebrew. $ brew install bash-completion # if running 3.2 $ brew install bash-completion@2 # if running Bash 4.1+ $ copilot completion bash > /usr/local/etc/bash_completion.d Install bash completion on linux $ source < ( copilot completion bash ) $ copilot completion bash > copilot.sh $ sudo mv copilot.sh /etc/bash_completion.d/copilot Install fish completion on linux $ source < ( copilot completion fish ) $ copilot completion fish > ~/.config/fish/completions/copilot.fish","title":"Examples"},{"location":"docs/commands/deploy","text":"copilot deploy $ copilot deploy What does it do? This command is used to run either copilot svc deploy or copilot job deploy under the hood. The steps involved in copilot deploy are the same as those involved in copilot svc deploy and copilot job deploy : Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and job or service. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. --force Optional. Force a new service deployment using the existing image. -h, --help help for deploy -n, --name string Name of the service or job. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The container image tag. Examples Deploys a service named \"frontend\" to a \"test\" environment. $ copilot deploy --name frontend --env test Deploys a job named \"mailer\" with additional resource tags to a \"prod\" environment. $ copilot deploy -n mailer -e prod --resource-tags source/revision = bb133e7,deployment/initiator = manual","title":"deploy"},{"location":"docs/commands/deploy#copilot-deploy","text":"$ copilot deploy","title":"copilot deploy"},{"location":"docs/commands/deploy#what-does-it-do","text":"This command is used to run either copilot svc deploy or copilot job deploy under the hood. The steps involved in copilot deploy are the same as those involved in copilot svc deploy and copilot job deploy : Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and job or service.","title":"What does it do?"},{"location":"docs/commands/deploy#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. --force Optional. Force a new service deployment using the existing image. -h, --help help for deploy -n, --name string Name of the service or job. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The container image tag.","title":"What are the flags?"},{"location":"docs/commands/deploy#examples","text":"Deploys a service named \"frontend\" to a \"test\" environment. $ copilot deploy --name frontend --env test Deploys a job named \"mailer\" with additional resource tags to a \"prod\" environment. $ copilot deploy -n mailer -e prod --resource-tags source/revision = bb133e7,deployment/initiator = manual","title":"Examples"},{"location":"docs/commands/docs","text":"docs $ copilot docs [ flags ] What does it do? copilot docs open the copilot docs in your browser. What are the flags? -h, --help help for docs","title":"docs"},{"location":"docs/commands/docs#docs","text":"$ copilot docs [ flags ]","title":"docs"},{"location":"docs/commands/docs#what-does-it-do","text":"copilot docs open the copilot docs in your browser.","title":"What does it do?"},{"location":"docs/commands/docs#what-are-the-flags","text":"-h, --help help for docs","title":"What are the flags?"},{"location":"docs/commands/env-delete","text":"env delete $ copilot env delete [ flags ] What does it do? copilot env delete deletes an environment from your application. If there are running applications in your environment, you need to first run copilot svc delete . After you answer the questions, you should see that the AWS CloudFormation stack for your environment has been deleted. What are the flags? -h, --help help for delete -n, --name string Name of the environment. --yes Skips confirmation prompt. -a, --app string Name of the application. Examples Delete the \"test\" environment. $ copilot env delete --name test Delete the \"test\" environment without prompting. $ copilot env delete --name test --yes","title":"env delete"},{"location":"docs/commands/env-delete#env-delete","text":"$ copilot env delete [ flags ]","title":"env delete"},{"location":"docs/commands/env-delete#what-does-it-do","text":"copilot env delete deletes an environment from your application. If there are running applications in your environment, you need to first run copilot svc delete . After you answer the questions, you should see that the AWS CloudFormation stack for your environment has been deleted.","title":"What does it do?"},{"location":"docs/commands/env-delete#what-are-the-flags","text":"-h, --help help for delete -n, --name string Name of the environment. --yes Skips confirmation prompt. -a, --app string Name of the application.","title":"What are the flags?"},{"location":"docs/commands/env-delete#examples","text":"Delete the \"test\" environment. $ copilot env delete --name test Delete the \"test\" environment without prompting. $ copilot env delete --name test --yes","title":"Examples"},{"location":"docs/commands/env-init","text":"env init $ copilot env init [ flags ] What does it do? copilot env init creates a new environment where your services will live. After you answer the questions, the CLI creates the common infrastructure that's shared between your services such as a VPC, an Application Load Balancer, and an ECS Cluster. Additionally, you can customize your Copilot environment by either configuring the default environment resources or importing existing resources for your environment. You create environments using a named profile to specify which AWS account and region you'd like the environment to be in. What are the flags? Like all commands in the AWS Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: Common Flags --aws-access-key-id string Optional. An AWS access key. --aws-secret-access-key string Optional. An AWS secret access key. --aws-session-token string Optional. An AWS session token for temporary credentials. --default-config Optional. Skip prompting and use default environment configuration. -n, --name string Name of the environment. --prod If the environment contains production services. --profile string Name of the profile. --region string Optional. An AWS region where the environment will be created. Import Existing Resources Flags --import-private-subnets strings Optional. Use existing private subnet IDs. --import-public-subnets strings Optional. Use existing public subnet IDs. --import-vpc-id string Optional. Use an existing VPC ID. Configure Default Resources Flags --override-private-cidrs strings Optional. CIDR to use for private subnets (default 10.0.2.0/24,10.0.3.0/24). --override-public-cidrs strings Optional. CIDR to use for public subnets (default 10.0.0.0/24,10.0.1.0/24). --override-vpc-cidr ipNet Optional. Global CIDR to use for VPC (default 10.0.0.0/16). Global Flags -a, --app string Name of the application. Examples Creates a test environment in your \"default\" AWS profile using default config. $ copilot env init --name test --profile default --default-config Creates a prod-iad environment using your \"prod-admin\" AWS profile using existing VPC. $ copilot env init --name prod-iad --profile prod-admin --prod \\ --import-vpc-id vpc-099c32d2b98cdcf47 \\ --import-public-subnets subnet-013e8b691862966cf,subnet-014661ebb7ab8681a \\ --import-private-subnets subnet-055fafef48fb3c547,subnet-00c9e76f288363e7f What does it look like?","title":"env init"},{"location":"docs/commands/env-init#env-init","text":"$ copilot env init [ flags ]","title":"env init"},{"location":"docs/commands/env-init#what-does-it-do","text":"copilot env init creates a new environment where your services will live. After you answer the questions, the CLI creates the common infrastructure that's shared between your services such as a VPC, an Application Load Balancer, and an ECS Cluster. Additionally, you can customize your Copilot environment by either configuring the default environment resources or importing existing resources for your environment. You create environments using a named profile to specify which AWS account and region you'd like the environment to be in.","title":"What does it do?"},{"location":"docs/commands/env-init#what-are-the-flags","text":"Like all commands in the AWS Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: Common Flags --aws-access-key-id string Optional. An AWS access key. --aws-secret-access-key string Optional. An AWS secret access key. --aws-session-token string Optional. An AWS session token for temporary credentials. --default-config Optional. Skip prompting and use default environment configuration. -n, --name string Name of the environment. --prod If the environment contains production services. --profile string Name of the profile. --region string Optional. An AWS region where the environment will be created. Import Existing Resources Flags --import-private-subnets strings Optional. Use existing private subnet IDs. --import-public-subnets strings Optional. Use existing public subnet IDs. --import-vpc-id string Optional. Use an existing VPC ID. Configure Default Resources Flags --override-private-cidrs strings Optional. CIDR to use for private subnets (default 10.0.2.0/24,10.0.3.0/24). --override-public-cidrs strings Optional. CIDR to use for public subnets (default 10.0.0.0/24,10.0.1.0/24). --override-vpc-cidr ipNet Optional. Global CIDR to use for VPC (default 10.0.0.0/16). Global Flags -a, --app string Name of the application.","title":"What are the flags?"},{"location":"docs/commands/env-init#examples","text":"Creates a test environment in your \"default\" AWS profile using default config. $ copilot env init --name test --profile default --default-config Creates a prod-iad environment using your \"prod-admin\" AWS profile using existing VPC. $ copilot env init --name prod-iad --profile prod-admin --prod \\ --import-vpc-id vpc-099c32d2b98cdcf47 \\ --import-public-subnets subnet-013e8b691862966cf,subnet-014661ebb7ab8681a \\ --import-private-subnets subnet-055fafef48fb3c547,subnet-00c9e76f288363e7f","title":"Examples"},{"location":"docs/commands/env-init#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/env-ls","text":"env ls $ copilot env ls [ flags ] What does it do? copilot env ls lists all the environments in your application. What are the flags? -h, --help help for ls --json Optional. Outputs in JSON format. -a, --app string Name of the application. You can use the --json flag if you'd like to programmatically parse the results. Examples Lists all the environments for the frontend application. $ copilot env ls -a frontend What does it look like?","title":"env ls"},{"location":"docs/commands/env-ls#env-ls","text":"$ copilot env ls [ flags ]","title":"env ls"},{"location":"docs/commands/env-ls#what-does-it-do","text":"copilot env ls lists all the environments in your application.","title":"What does it do?"},{"location":"docs/commands/env-ls#what-are-the-flags","text":"-h, --help help for ls --json Optional. Outputs in JSON format. -a, --app string Name of the application. You can use the --json flag if you'd like to programmatically parse the results.","title":"What are the flags?"},{"location":"docs/commands/env-ls#examples","text":"Lists all the environments for the frontend application. $ copilot env ls -a frontend","title":"Examples"},{"location":"docs/commands/env-ls#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/env-show","text":"env show $ copilot env show [ flags ] What does it do? copilot env show shows displays information about a particular environment, including: The region and account the environment is in Whether or not the environment is production The services currently deployed in the environment The tags associated with that environment You can optionally pass in a --resources flag which will include the AWS resources associated specifically with the environment. What are the flags? -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the environment. --resources Optional. Show the resources in your environment. You can use the --json flag if you'd like to programmatically parse the results. Examples Shows info about the environment \"test\". $ copilot env show -n test","title":"env show"},{"location":"docs/commands/env-show#env-show","text":"$ copilot env show [ flags ]","title":"env show"},{"location":"docs/commands/env-show#what-does-it-do","text":"copilot env show shows displays information about a particular environment, including: The region and account the environment is in Whether or not the environment is production The services currently deployed in the environment The tags associated with that environment You can optionally pass in a --resources flag which will include the AWS resources associated specifically with the environment.","title":"What does it do?"},{"location":"docs/commands/env-show#what-are-the-flags","text":"-h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the environment. --resources Optional. Show the resources in your environment. You can use the --json flag if you'd like to programmatically parse the results.","title":"What are the flags?"},{"location":"docs/commands/env-show#examples","text":"Shows info about the environment \"test\". $ copilot env show -n test","title":"Examples"},{"location":"docs/commands/init","text":"init $ copilot init What does it do? copilot init is your starting point if you want to deploy your container app on Amazon ECS. Run it within a directory with your Dockerfile, and init will ask you questions about your application so we can get it up and running quickly. After you answer all the questions, copilot init will set up an ECR repository for you and ask you if you'd like to deploy. If you opt to deploy, it'll create a new test environment (complete with a networking stack and roles), build your Dockerfile, push it to Amazon ECR, and deploy your service or job. If you have an existing app, and want to add another service or job to that app, you can run copilot init - and you'll be prompted to select an existing app to add your service or job to. What are the flags? Like all commands in the Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: -a, --app string Name of the application. --deploy Deploy your service or job to a \"test\" environment. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image. -h, --help help for init -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile. -n, --name string Name of the service or job. --port uint16 Optional. The port on which your service listens. --retries int Optional. The number of times to try restarting the job on a failure. --schedule string The schedule on which to run this job. Accepts cron expressions of the format ( M H DoM M DoW ) and schedule definition strings. For example: \"0 * * * *\" , \"@daily\" , \"@weekly\" , \"@every 1h30m\" . AWS Schedule Expressions of the form \"rate(10 minutes)\" or \"cron(0 12 L * ? 2021)\" are also accepted. --tag string Optional. The container image tag. --timeout string Optional. The total execution time for the task, including retries. Accepts valid Go duration strings. For example: \"2h\" , \"1h30m\" , \"900s\" . -t, --type string Type of service to create. Must be one of: \"Request-Driven Web Service\" , \"Load Balanced Web Service\" , \"Backend Service\" , \"Scheduled Job\" .","title":"init"},{"location":"docs/commands/init#init","text":"$ copilot init","title":"init"},{"location":"docs/commands/init#what-does-it-do","text":"copilot init is your starting point if you want to deploy your container app on Amazon ECS. Run it within a directory with your Dockerfile, and init will ask you questions about your application so we can get it up and running quickly. After you answer all the questions, copilot init will set up an ECR repository for you and ask you if you'd like to deploy. If you opt to deploy, it'll create a new test environment (complete with a networking stack and roles), build your Dockerfile, push it to Amazon ECR, and deploy your service or job. If you have an existing app, and want to add another service or job to that app, you can run copilot init - and you'll be prompted to select an existing app to add your service or job to.","title":"What does it do?"},{"location":"docs/commands/init#what-are-the-flags","text":"Like all commands in the Copilot CLI, if you don't provide required flags, we'll prompt you for all the information we need to get you going. You can skip the prompts by providing information via flags: -a, --app string Name of the application. --deploy Deploy your service or job to a \"test\" environment. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image. -h, --help help for init -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile. -n, --name string Name of the service or job. --port uint16 Optional. The port on which your service listens. --retries int Optional. The number of times to try restarting the job on a failure. --schedule string The schedule on which to run this job. Accepts cron expressions of the format ( M H DoM M DoW ) and schedule definition strings. For example: \"0 * * * *\" , \"@daily\" , \"@weekly\" , \"@every 1h30m\" . AWS Schedule Expressions of the form \"rate(10 minutes)\" or \"cron(0 12 L * ? 2021)\" are also accepted. --tag string Optional. The container image tag. --timeout string Optional. The total execution time for the task, including retries. Accepts valid Go duration strings. For example: \"2h\" , \"1h30m\" , \"900s\" . -t, --type string Type of service to create. Must be one of: \"Request-Driven Web Service\" , \"Load Balanced Web Service\" , \"Backend Service\" , \"Scheduled Job\" .","title":"What are the flags?"},{"location":"docs/commands/job-delete","text":"job delete $ copilot job delete [ flags ] What does it do? copilot job delete deletes all resources associated with your job in a particular environment. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the job. --yes Skips confirmation prompt. Examples Delete the \"report-generator\" job from the my-app application. $ copilot job delete --name report-generator --app my-app Delete the \"report-generator\" job from just the prod environment. $ copilot job delete --name report-generator --env prod Delete the \"report-generator\" job from the my-app application from outside of the workspace. $ copilot job delete --name report-generator --app my-app Delete the \"report-generator\" job without the confirmation prompt. $ copilot job delete --name report-generator --yes","title":"job delete"},{"location":"docs/commands/job-delete#job-delete","text":"$ copilot job delete [ flags ]","title":"job delete"},{"location":"docs/commands/job-delete#what-does-it-do","text":"copilot job delete deletes all resources associated with your job in a particular environment.","title":"What does it do?"},{"location":"docs/commands/job-delete#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the job. --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/job-delete#examples","text":"Delete the \"report-generator\" job from the my-app application. $ copilot job delete --name report-generator --app my-app Delete the \"report-generator\" job from just the prod environment. $ copilot job delete --name report-generator --env prod Delete the \"report-generator\" job from the my-app application from outside of the workspace. $ copilot job delete --name report-generator --app my-app Delete the \"report-generator\" job without the confirmation prompt. $ copilot job delete --name report-generator --yes","title":"Examples"},{"location":"docs/commands/job-deploy","text":"job deploy $ copilot job deploy What does it do? job deploy takes your local code and configuration and deploys your job. The steps involved in job deploy are: Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and job What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for deploy -n, --name string Name of the job. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The container image tag. Examples Deploys a job named \"report-gen\" to a \"test\" environment. $ copilot job deploy --name report-gen --env test Deploys a job with additional resource tags. $ copilot job deploy --resource-tags source/revision = bb133e7,deployment/initiator = manual `","title":"job deploy"},{"location":"docs/commands/job-deploy#job-deploy","text":"$ copilot job deploy","title":"job deploy"},{"location":"docs/commands/job-deploy#what-does-it-do","text":"job deploy takes your local code and configuration and deploys your job. The steps involved in job deploy are: Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and job","title":"What does it do?"},{"location":"docs/commands/job-deploy#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for deploy -n, --name string Name of the job. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The container image tag.","title":"What are the flags?"},{"location":"docs/commands/job-deploy#examples","text":"Deploys a job named \"report-gen\" to a \"test\" environment. $ copilot job deploy --name report-gen --env test Deploys a job with additional resource tags. $ copilot job deploy --resource-tags source/revision = bb133e7,deployment/initiator = manual `","title":"Examples"},{"location":"docs/commands/job-init","text":"job init $ copilot job init What does it do? copilot job init creates a new job to run your code for you. After running this command, the CLI creates sub-directory with your app name in your local copilot directory where you'll find a manifest file . Feel free to update your manifest file to change the default configs for your job. The CLI also sets up an ECR repository with a policy for all environments to be able to pull from it. Then, your job gets registered to AWS System Manager Parameter Store so that the CLI can keep track of your it. After that, if you already have an environment set up, you can run copilot job deploy to deploy your job in that environment. What are the flags? -a, --app string Name of the application. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image. -h, --help help for init -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile. -t, --job-type string Type of job to create. Must be one of: \"Scheduled Job\" . -n, --name string Name of the job. --retries int Optional. The number of times to try restarting the job on a failure. -s, --schedule string The schedule on which to run this job. Accepts cron expressions of the format ( M H DoM M DoW ) and schedule definition strings. For example: \"0 * * * *\" , \"@daily\" , \"@weekly\" , \"@every 1h30m\" . AWS Schedule Expressions of the form \"rate(10 minutes)\" or \"cron(0 12 L * ? 2021)\" are also accepted. --timeout string Optional. The total execution time for the task, including retries. Accepts valid Go duration strings. For example: \"2h\" , \"1h30m\" , \"900s\" . Examples Creates a \"reaper\" scheduled task to run once per day. $ copilot job init --name reaper --dockerfile ./frontend/Dockerfile --schedule \"@daily\" Creates a \"report-generator\" scheduled task with retries. $ copilot job init --name report-generator --schedule \"@monthly\" --retries 3 --timeout 900s","title":"job init"},{"location":"docs/commands/job-init#job-init","text":"$ copilot job init","title":"job init"},{"location":"docs/commands/job-init#what-does-it-do","text":"copilot job init creates a new job to run your code for you. After running this command, the CLI creates sub-directory with your app name in your local copilot directory where you'll find a manifest file . Feel free to update your manifest file to change the default configs for your job. The CLI also sets up an ECR repository with a policy for all environments to be able to pull from it. Then, your job gets registered to AWS System Manager Parameter Store so that the CLI can keep track of your it. After that, if you already have an environment set up, you can run copilot job deploy to deploy your job in that environment.","title":"What does it do?"},{"location":"docs/commands/job-init#what-are-the-flags","text":"-a, --app string Name of the application. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image. -h, --help help for init -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile. -t, --job-type string Type of job to create. Must be one of: \"Scheduled Job\" . -n, --name string Name of the job. --retries int Optional. The number of times to try restarting the job on a failure. -s, --schedule string The schedule on which to run this job. Accepts cron expressions of the format ( M H DoM M DoW ) and schedule definition strings. For example: \"0 * * * *\" , \"@daily\" , \"@weekly\" , \"@every 1h30m\" . AWS Schedule Expressions of the form \"rate(10 minutes)\" or \"cron(0 12 L * ? 2021)\" are also accepted. --timeout string Optional. The total execution time for the task, including retries. Accepts valid Go duration strings. For example: \"2h\" , \"1h30m\" , \"900s\" .","title":"What are the flags?"},{"location":"docs/commands/job-init#examples","text":"Creates a \"reaper\" scheduled task to run once per day. $ copilot job init --name reaper --dockerfile ./frontend/Dockerfile --schedule \"@daily\" Creates a \"report-generator\" scheduled task with retries. $ copilot job init --name report-generator --schedule \"@monthly\" --retries 3 --timeout 900s","title":"Examples"},{"location":"docs/commands/job-ls","text":"job ls $ copilot job ls What does it do? copilot job ls lists all the Copilot jobs for a particular application. What are the flags? -a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format. --local Only show jobs in the workspace. Example Lists all the jobs for the \"myapp\" application. $ copilot job ls --app myapp","title":"job ls"},{"location":"docs/commands/job-ls#job-ls","text":"$ copilot job ls","title":"job ls"},{"location":"docs/commands/job-ls#what-does-it-do","text":"copilot job ls lists all the Copilot jobs for a particular application.","title":"What does it do?"},{"location":"docs/commands/job-ls#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format. --local Only show jobs in the workspace.","title":"What are the flags?"},{"location":"docs/commands/job-ls#example","text":"Lists all the jobs for the \"myapp\" application. $ copilot job ls --app myapp","title":"Example"},{"location":"docs/commands/job-package","text":"job package $ copilot job package What does it do? copilot job package produces the CloudFormation template(s) used to deploy a job to an environment. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for package -n, --name string Name of the job. --output-dir string Optional. Writes the stack template and template configuration to a directory. --tag string Optional. The container image tag. Examples Prints the CloudFormation template for the \"report-generator\" job parametrized for the \"test\" environment. $ copilot job package -n report-generator -e test Writes the CloudFormation stack and configuration to an \"infrastructure/\" sub-directory instead of printing. $ copilot job package -n report-generator -e test --output-dir ./infrastructure $ ls ./infrastructure report-generator-test.stack.yml report-generator-test.params.yml","title":"job package"},{"location":"docs/commands/job-package#job-package","text":"$ copilot job package","title":"job package"},{"location":"docs/commands/job-package#what-does-it-do","text":"copilot job package produces the CloudFormation template(s) used to deploy a job to an environment.","title":"What does it do?"},{"location":"docs/commands/job-package#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for package -n, --name string Name of the job. --output-dir string Optional. Writes the stack template and template configuration to a directory. --tag string Optional. The container image tag.","title":"What are the flags?"},{"location":"docs/commands/job-package#examples","text":"Prints the CloudFormation template for the \"report-generator\" job parametrized for the \"test\" environment. $ copilot job package -n report-generator -e test Writes the CloudFormation stack and configuration to an \"infrastructure/\" sub-directory instead of printing. $ copilot job package -n report-generator -e test --output-dir ./infrastructure $ ls ./infrastructure report-generator-test.stack.yml report-generator-test.params.yml","title":"Examples"},{"location":"docs/commands/pipeline-delete","text":"pipeline delete $ copilot pipeline delete [ flags ] What does it do? copilot pipeline delete deletes the pipeline associated with your workspace. What are the flags? --delete-secret Deletes AWS Secrets Manager secret associated with a pipeline source repository. -h, --help help for delete --yes Skips confirmation prompt. Examples Delete the pipeline associated with your workspace. $ copilot pipeline delete","title":"pipeline delete"},{"location":"docs/commands/pipeline-delete#pipeline-delete","text":"$ copilot pipeline delete [ flags ]","title":"pipeline delete"},{"location":"docs/commands/pipeline-delete#what-does-it-do","text":"copilot pipeline delete deletes the pipeline associated with your workspace.","title":"What does it do?"},{"location":"docs/commands/pipeline-delete#what-are-the-flags","text":"--delete-secret Deletes AWS Secrets Manager secret associated with a pipeline source repository. -h, --help help for delete --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/pipeline-delete#examples","text":"Delete the pipeline associated with your workspace. $ copilot pipeline delete","title":"Examples"},{"location":"docs/commands/pipeline-init","text":"pipeline init $ copilot pipeline init [ flags ] What does it do? copilot pipeline init creates a pipeline manifest for the services in your workspace, using the environments associated with the application. What are the flags? -a, --app string Name of the application. -e, --environments strings Environments to add to the pipeline. -b, --git-branch string Branch used to trigger your pipeline. -u, --url string The repository URL to trigger your pipeline. -h, --help help for init Examples Create a pipeline for the services in your workspace. $ copilot pipeline init \\ --url https://github.com/gitHubUserName/myFrontendApp.git \\ --environments \"test,prod\"","title":"pipeline init"},{"location":"docs/commands/pipeline-init#pipeline-init","text":"$ copilot pipeline init [ flags ]","title":"pipeline init"},{"location":"docs/commands/pipeline-init#what-does-it-do","text":"copilot pipeline init creates a pipeline manifest for the services in your workspace, using the environments associated with the application.","title":"What does it do?"},{"location":"docs/commands/pipeline-init#what-are-the-flags","text":"-a, --app string Name of the application. -e, --environments strings Environments to add to the pipeline. -b, --git-branch string Branch used to trigger your pipeline. -u, --url string The repository URL to trigger your pipeline. -h, --help help for init","title":"What are the flags?"},{"location":"docs/commands/pipeline-init#examples","text":"Create a pipeline for the services in your workspace. $ copilot pipeline init \\ --url https://github.com/gitHubUserName/myFrontendApp.git \\ --environments \"test,prod\"","title":"Examples"},{"location":"docs/commands/pipeline-ls","text":"pipeline ls $ copilot pipeline ls [ flags ] What does it do? copilot pipeline ls lists all the deployed pipelines in an application. What are the flags? -a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format. Examples Lists all the pipelines for the \"phonetool\" application. $ copilot pipeline ls -a phonetool","title":"pipeline ls"},{"location":"docs/commands/pipeline-ls#pipeline-ls","text":"$ copilot pipeline ls [ flags ]","title":"pipeline ls"},{"location":"docs/commands/pipeline-ls#what-does-it-do","text":"copilot pipeline ls lists all the deployed pipelines in an application.","title":"What does it do?"},{"location":"docs/commands/pipeline-ls#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format.","title":"What are the flags?"},{"location":"docs/commands/pipeline-ls#examples","text":"Lists all the pipelines for the \"phonetool\" application. $ copilot pipeline ls -a phonetool","title":"Examples"},{"location":"docs/commands/pipeline-show","text":"pipeline show $ copilot pipeline show [ flags ] What does it do? copilot pipeline show shows configuration information about a deployed pipeline for an application, including the account, region, and stages. What are the flags? -a, --app string Name of the application. -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the pipeline. --resources Optional. Show the resources in your pipeline. Examples Shows info about the pipeline in the \"myapp\" application. $ copilot pipeline show --app myapp --resources What does it look like?","title":"pipeline show"},{"location":"docs/commands/pipeline-show#pipeline-show","text":"$ copilot pipeline show [ flags ]","title":"pipeline show"},{"location":"docs/commands/pipeline-show#what-does-it-do","text":"copilot pipeline show shows configuration information about a deployed pipeline for an application, including the account, region, and stages.","title":"What does it do?"},{"location":"docs/commands/pipeline-show#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the pipeline. --resources Optional. Show the resources in your pipeline.","title":"What are the flags?"},{"location":"docs/commands/pipeline-show#examples","text":"Shows info about the pipeline in the \"myapp\" application. $ copilot pipeline show --app myapp --resources","title":"Examples"},{"location":"docs/commands/pipeline-show#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/pipeline-status","text":"pipeline status $ copilot pipeline status [ flags ] What does it do? copilot pipeline status shows the status of the stages in a deployed pipeline. What are the flags? -a, --app string Name of the application. -h, --help help for status --json Optional. Outputs in JSON format. -n, --name string Name of the pipeline. Examples Shows status of the pipeline \"pipeline-myapp-myrepo\". $ copilot pipeline status -n pipeline-myapp-myrepo What does it look like?","title":"pipeline status"},{"location":"docs/commands/pipeline-status#pipeline-status","text":"$ copilot pipeline status [ flags ]","title":"pipeline status"},{"location":"docs/commands/pipeline-status#what-does-it-do","text":"copilot pipeline status shows the status of the stages in a deployed pipeline.","title":"What does it do?"},{"location":"docs/commands/pipeline-status#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for status --json Optional. Outputs in JSON format. -n, --name string Name of the pipeline.","title":"What are the flags?"},{"location":"docs/commands/pipeline-status#examples","text":"Shows status of the pipeline \"pipeline-myapp-myrepo\". $ copilot pipeline status -n pipeline-myapp-myrepo","title":"Examples"},{"location":"docs/commands/pipeline-status#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/pipeline-update","text":"pipeline update $ copilot pipeline update [ flags ] What does it do? copilot pipeline update deploys a pipeline for the services in your workspace, using the environments associated with the application from a pipeline manifest. What are the flags? -h, --help help for update --yes Skips confirmation prompt. Examples Deploys an updated pipeline for the services in your workspace. $ copilot pipeline update","title":"pipeline update"},{"location":"docs/commands/pipeline-update#pipeline-update","text":"$ copilot pipeline update [ flags ]","title":"pipeline update"},{"location":"docs/commands/pipeline-update#what-does-it-do","text":"copilot pipeline update deploys a pipeline for the services in your workspace, using the environments associated with the application from a pipeline manifest.","title":"What does it do?"},{"location":"docs/commands/pipeline-update#what-are-the-flags","text":"-h, --help help for update --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/pipeline-update#examples","text":"Deploys an updated pipeline for the services in your workspace. $ copilot pipeline update","title":"Examples"},{"location":"docs/commands/secret-init","text":"secret init $ copilot secret init What does it do? copilot secret init creates or updates secrets as SecureString parameters in SSM Parameter Store for your application. A secret can have different values in each of your existing environments, and is accessible by your services or jobs from the same application and environment. Attention Secrets are not supported for Request-Driven Web Services. What are the flags? -a, --app string Name of the application. --cli-input-yaml string Optional. A YAML file in which the secret values are specified. Mutually exclusive with the -n, --name and --values flags. -h, --help help for init -n, --name string The name of the secret. Mutually exclusive with the --cli-input-yaml flag. --overwrite Optional. Whether to overwrite an existing secret. --values stringToString Values of the secret in each environment. Specified as <environment>=<value> separated by commas. Mutually exclusive with the --cli-input-yaml flag. (default []) How can I use it? Create a secret with prompts. You will be prompted for the name of the secret, and its values in each of your existing environments. $ copilot secret init Create a secret named db_password in multiple environments. You will be prompted for the db_password 's values you want for each of your existing environments. $ copilot secret init --name db_password Create secrets from input.yml . For the format of the YAML file, please see below . $ copilot secret init --cli-input-yaml input.yml Info It is recommended that you specify your secret's values through our prompts (e.g. by running copilot secret init --name ) or from an input file by using the --cli-input-yaml flag. While the --values flag is a convenient way to specify secret values, your input may appear in your shell history as plaintext. What's next? Copilot will create SSM parameters named /copilot/<app name>/<env name>/secrets/<secret name> . Using the parameter names, you can then modify the secrets section in your service's or job's manifest to reference the secrets that were created. For example, suppose you have an application my-app , and you've created a secret db_host in your prod and dev environments. You can modify your service's manifest as follows: environments : prod : secrets : DB_PASSWORD : /copilot/my-app/prod/secrets/db_password dev : secrets : DB_PASSWORD : /copilot/my-app/dev/secrets/db_password Once you deploy this updated manifest, your service or job will be able to access the environment variable DB_PASSWORD . It will have the value of the SSM parameter /copilot/my-app/prod/secrets/db_password if the service/job is deployed in a prod environment, and /copilot/my-app/dev/secrets/db_password if it's deployed in a dev environment. This works because ECS Agent will resolve the SSM parameter when it starts up your task, and set the environment variable for you. How do I use the --cli-input-yaml flag? You can specify multiple secrets and their values in each of your existing environments in a file. Then you can use the file as the input to --cli-input-yaml flag. Copilot will read from the file and create or update the secrets accordingly. The YAML file should be formatted as follows: <secret A> : <env 1> : <the value of secret A in env 1> <env 2> : <the value of secret A in env 2> <env 3> : <the value of secret A in env 3> <secret B> : <env 1> : <the value of secret B in env 1> <env 2> : <the value of secret B in env 2> Here is an example input file that creates secrets db_host and db_password in dev , test and prod , and notification_email in dev and test environments. Note that notification_email won't be created for the prod environment since it doesn't have a value for prod . db_host : dev : dev.db.host.com test : test.db.host.com prod : prod.db.host.com db_password : dev : dev-db-pwd test : test-db-pwd prod : prod-db-pwd notification_email : dev : dev@email.com test : test@email.com","title":"secret init"},{"location":"docs/commands/secret-init#secret-init","text":"$ copilot secret init","title":"secret init"},{"location":"docs/commands/secret-init#what-does-it-do","text":"copilot secret init creates or updates secrets as SecureString parameters in SSM Parameter Store for your application. A secret can have different values in each of your existing environments, and is accessible by your services or jobs from the same application and environment. Attention Secrets are not supported for Request-Driven Web Services.","title":"What does it do?"},{"location":"docs/commands/secret-init#what-are-the-flags","text":"-a, --app string Name of the application. --cli-input-yaml string Optional. A YAML file in which the secret values are specified. Mutually exclusive with the -n, --name and --values flags. -h, --help help for init -n, --name string The name of the secret. Mutually exclusive with the --cli-input-yaml flag. --overwrite Optional. Whether to overwrite an existing secret. --values stringToString Values of the secret in each environment. Specified as <environment>=<value> separated by commas. Mutually exclusive with the --cli-input-yaml flag. (default [])","title":"What are the flags?"},{"location":"docs/commands/secret-init#how-can-i-use-it","text":"Create a secret with prompts. You will be prompted for the name of the secret, and its values in each of your existing environments. $ copilot secret init Create a secret named db_password in multiple environments. You will be prompted for the db_password 's values you want for each of your existing environments. $ copilot secret init --name db_password Create secrets from input.yml . For the format of the YAML file, please see below . $ copilot secret init --cli-input-yaml input.yml Info It is recommended that you specify your secret's values through our prompts (e.g. by running copilot secret init --name ) or from an input file by using the --cli-input-yaml flag. While the --values flag is a convenient way to specify secret values, your input may appear in your shell history as plaintext.","title":"How can I use it?"},{"location":"docs/commands/secret-init#whats-next","text":"Copilot will create SSM parameters named /copilot/<app name>/<env name>/secrets/<secret name> . Using the parameter names, you can then modify the secrets section in your service's or job's manifest to reference the secrets that were created. For example, suppose you have an application my-app , and you've created a secret db_host in your prod and dev environments. You can modify your service's manifest as follows: environments : prod : secrets : DB_PASSWORD : /copilot/my-app/prod/secrets/db_password dev : secrets : DB_PASSWORD : /copilot/my-app/dev/secrets/db_password Once you deploy this updated manifest, your service or job will be able to access the environment variable DB_PASSWORD . It will have the value of the SSM parameter /copilot/my-app/prod/secrets/db_password if the service/job is deployed in a prod environment, and /copilot/my-app/dev/secrets/db_password if it's deployed in a dev environment. This works because ECS Agent will resolve the SSM parameter when it starts up your task, and set the environment variable for you.","title":"What's next?"},{"location":"docs/commands/secret-init#how-do-i-use-the-cli-input-yaml-flag","text":"You can specify multiple secrets and their values in each of your existing environments in a file. Then you can use the file as the input to --cli-input-yaml flag. Copilot will read from the file and create or update the secrets accordingly. The YAML file should be formatted as follows: <secret A> : <env 1> : <the value of secret A in env 1> <env 2> : <the value of secret A in env 2> <env 3> : <the value of secret A in env 3> <secret B> : <env 1> : <the value of secret B in env 1> <env 2> : <the value of secret B in env 2> Here is an example input file that creates secrets db_host and db_password in dev , test and prod , and notification_email in dev and test environments. Note that notification_email won't be created for the prod environment since it doesn't have a value for prod . db_host : dev : dev.db.host.com test : test.db.host.com prod : prod.db.host.com db_password : dev : dev-db-pwd test : test-db-pwd prod : prod-db-pwd notification_email : dev : dev@email.com test : test@email.com","title":"How do I use the --cli-input-yaml flag?"},{"location":"docs/commands/storage-init","text":"storage init $ copilot storage init What does it do? copilot storage init creates a new storage resource attached to one of your workloads, accessible from inside your service container via a friendly environment variable. You can specify either S3 , DynamoDB or Aurora as the resource type. After running this command, the CLI creates an addons subdirectory inside your copilot/service directory if it does not exist. When you run copilot svc deploy , your newly initialized storage resource is created in the environment you're deploying to. By default, only the service you specify during storage init will have access to that storage resource. What are the flags? Required Flags -n, --name string Name of the storage resource to create. -t, --storage-type string Type of storage to add. Must be one of: \"DynamoDB\" , \"S3\" , \"Aurora\" . -w, --workload string Name of the service or job to associate with storage. DynamoDB Flags --lsi stringArray Optional. Attribute to use as an alternate sort key. May be specified up to 5 times. Must be of the format '<keyName>:<dataType>' . --no-lsi Optional. Don 't ask about configuring alternate sort keys. --no-sort Optional. Skip configuring sort keys. --partition-key string Partition key for the DDB table. Must be of the format ' <keyName>:<dataType> '. --sort-key string Optional. Sort key for the DDB table. Must be of the format ' <keyName>:<dataType> ' . Aurora Serverless Flags --engine string The database engine used in the cluster. Must be either \"MySQL\" or \"PostgreSQL\" . --parameter-group string Optional. The name of the parameter group to associate with the cluster. --initial-db string The initial database to create in the cluster. How can I use it? Create an S3 bucket named \"my-bucket\" attached to the \"frontend\" service. $ copilot storage init -n my-bucket -t S3 -w frontend Create a basic DynamoDB table named \"my-table\" attached to the \"frontend\" service with a sort key specified. $ copilot storage init -n my-table -t DynamoDB -w frontend --partition-key Email:S --sort-key UserId:N --no-lsi Create a DynamoDB table with multiple alternate sort keys. $ copilot storage init \\ -n my-table -t DynamoDB -w frontend \\ --partition-key Email:S \\ --sort-key UserId:N \\ --lsi Points:N \\ --lsi Goodness:N Create an RDS Aurora Serverless cluster using PostgreSQL as the database engine. $ copilot storage init \\ -n my-cluster -t Aurora -w frontend --engine PostgreSQL What happens under the hood? Copilot writes a Cloudformation template specifying the S3 bucket or DDB table to the addons dir. When you run copilot svc deploy , the CLI merges this template with all the other templates in the addons directory to create a nested stack associated with your service. This nested stack describes all the additional resources you've associated with that service and is deployed wherever your service is deployed. This means that after running $ copilot storage init -n bucket -t S3 -w fe $ copilot svc deploy -n fe -e test $ copilot svc deploy -n fe -e prod there will be two buckets deployed, one in the \"test\" env and one in the \"prod\" env, accessible only to the \"fe\" service in its respective environment.","title":"storage init"},{"location":"docs/commands/storage-init#storage-init","text":"$ copilot storage init","title":"storage init"},{"location":"docs/commands/storage-init#what-does-it-do","text":"copilot storage init creates a new storage resource attached to one of your workloads, accessible from inside your service container via a friendly environment variable. You can specify either S3 , DynamoDB or Aurora as the resource type. After running this command, the CLI creates an addons subdirectory inside your copilot/service directory if it does not exist. When you run copilot svc deploy , your newly initialized storage resource is created in the environment you're deploying to. By default, only the service you specify during storage init will have access to that storage resource.","title":"What does it do?"},{"location":"docs/commands/storage-init#what-are-the-flags","text":"Required Flags -n, --name string Name of the storage resource to create. -t, --storage-type string Type of storage to add. Must be one of: \"DynamoDB\" , \"S3\" , \"Aurora\" . -w, --workload string Name of the service or job to associate with storage. DynamoDB Flags --lsi stringArray Optional. Attribute to use as an alternate sort key. May be specified up to 5 times. Must be of the format '<keyName>:<dataType>' . --no-lsi Optional. Don 't ask about configuring alternate sort keys. --no-sort Optional. Skip configuring sort keys. --partition-key string Partition key for the DDB table. Must be of the format ' <keyName>:<dataType> '. --sort-key string Optional. Sort key for the DDB table. Must be of the format ' <keyName>:<dataType> ' . Aurora Serverless Flags --engine string The database engine used in the cluster. Must be either \"MySQL\" or \"PostgreSQL\" . --parameter-group string Optional. The name of the parameter group to associate with the cluster. --initial-db string The initial database to create in the cluster.","title":"What are the flags?"},{"location":"docs/commands/storage-init#how-can-i-use-it","text":"Create an S3 bucket named \"my-bucket\" attached to the \"frontend\" service. $ copilot storage init -n my-bucket -t S3 -w frontend Create a basic DynamoDB table named \"my-table\" attached to the \"frontend\" service with a sort key specified. $ copilot storage init -n my-table -t DynamoDB -w frontend --partition-key Email:S --sort-key UserId:N --no-lsi Create a DynamoDB table with multiple alternate sort keys. $ copilot storage init \\ -n my-table -t DynamoDB -w frontend \\ --partition-key Email:S \\ --sort-key UserId:N \\ --lsi Points:N \\ --lsi Goodness:N Create an RDS Aurora Serverless cluster using PostgreSQL as the database engine. $ copilot storage init \\ -n my-cluster -t Aurora -w frontend --engine PostgreSQL","title":"How can I use it?"},{"location":"docs/commands/storage-init#what-happens-under-the-hood","text":"Copilot writes a Cloudformation template specifying the S3 bucket or DDB table to the addons dir. When you run copilot svc deploy , the CLI merges this template with all the other templates in the addons directory to create a nested stack associated with your service. This nested stack describes all the additional resources you've associated with that service and is deployed wherever your service is deployed. This means that after running $ copilot storage init -n bucket -t S3 -w fe $ copilot svc deploy -n fe -e test $ copilot svc deploy -n fe -e prod there will be two buckets deployed, one in the \"test\" env and one in the \"prod\" env, accessible only to the \"fe\" service in its respective environment.","title":"What happens under the hood?"},{"location":"docs/commands/svc-delete","text":"svc delete $ copilot svc delete [ flags ] What does it do? copilot svc delete deletes all resources associated with your service in a particular environment. What are the flags? -e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the service. --yes Skips confirmation prompt. Examples Force delete the application with environments \"test\" and \"prod\". $ copilot svc delete --name test --yes","title":"svc delete"},{"location":"docs/commands/svc-delete#svc-delete","text":"$ copilot svc delete [ flags ]","title":"svc delete"},{"location":"docs/commands/svc-delete#what-does-it-do","text":"copilot svc delete deletes all resources associated with your service in a particular environment.","title":"What does it do?"},{"location":"docs/commands/svc-delete#what-are-the-flags","text":"-e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the service. --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/svc-delete#examples","text":"Force delete the application with environments \"test\" and \"prod\". $ copilot svc delete --name test --yes","title":"Examples"},{"location":"docs/commands/svc-deploy","text":"svc deploy $ copilot svc deploy What does it do? copilot svc deploy takes your local code and configuration and deploys it. The steps involved in service deploy are: Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and service What are the flags? -e, --env string Name of the environment. --force Optional. Force a new service deployment using the existing image. -h, --help help for deploy -n, --name string Name of the service. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The service ' s image tag.","title":"svc deploy"},{"location":"docs/commands/svc-deploy#svc-deploy","text":"$ copilot svc deploy","title":"svc deploy"},{"location":"docs/commands/svc-deploy#what-does-it-do","text":"copilot svc deploy takes your local code and configuration and deploys it. The steps involved in service deploy are: Build your local Dockerfile into an image Tag it with the value from --tag or the latest git sha (if you're in a git directory) Push the image to ECR Package your manifest file and addons into CloudFormation Create / update your ECS task definition and service","title":"What does it do?"},{"location":"docs/commands/svc-deploy#what-are-the-flags","text":"-e, --env string Name of the environment. --force Optional. Force a new service deployment using the existing image. -h, --help help for deploy -n, --name string Name of the service. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. ( default []) --tag string Optional. The service ' s image tag.","title":"What are the flags?"},{"location":"docs/commands/svc-exec","text":"svc exec $ copilot svc exec What does it do? copilot svc exec executes a command in a running container part of a service. What are the flags? -a, --app string Name of the application. -c, --command string Optional. The command that is passed to a running container. (default \"/bin/bash\") --container string Optional. The specific container you want to exec in. By default the first essential container will be used. -e, --env string Name of the environment. -h, --help help for exec -n, --name string Name of the service, job, or task group. --task-id string Optional. ID of the task you want to exec in. --yes Optional. Whether to update the Session Manager Plugin. Examples Start an interactive bash session with a task part of the \"frontend\" service. $ copilot svc exec -a my-app -e test -n frontend Runs the 'ls' command in the task prefixed with ID \"8c38184\" within the \"backend\" service. $ copilot svc exec -a my-app -e test --name backend --task-id 8c38184 --command \"ls\" What does it look like? Info Please make sure exec: true is set in your manifest before deploying the service. Please note that this will update the service's Fargate Platform Version to 1.4.0. Updating the Platform Version results in replacing your service which will result in downtime for your service. exec is not supported for Windows containers.","title":"svc exec"},{"location":"docs/commands/svc-exec#svc-exec","text":"$ copilot svc exec","title":"svc exec"},{"location":"docs/commands/svc-exec#what-does-it-do","text":"copilot svc exec executes a command in a running container part of a service.","title":"What does it do?"},{"location":"docs/commands/svc-exec#what-are-the-flags","text":"-a, --app string Name of the application. -c, --command string Optional. The command that is passed to a running container. (default \"/bin/bash\") --container string Optional. The specific container you want to exec in. By default the first essential container will be used. -e, --env string Name of the environment. -h, --help help for exec -n, --name string Name of the service, job, or task group. --task-id string Optional. ID of the task you want to exec in. --yes Optional. Whether to update the Session Manager Plugin.","title":"What are the flags?"},{"location":"docs/commands/svc-exec#examples","text":"Start an interactive bash session with a task part of the \"frontend\" service. $ copilot svc exec -a my-app -e test -n frontend Runs the 'ls' command in the task prefixed with ID \"8c38184\" within the \"backend\" service. $ copilot svc exec -a my-app -e test --name backend --task-id 8c38184 --command \"ls\"","title":"Examples"},{"location":"docs/commands/svc-exec#what-does-it-look-like","text":"Info Please make sure exec: true is set in your manifest before deploying the service. Please note that this will update the service's Fargate Platform Version to 1.4.0. Updating the Platform Version results in replacing your service which will result in downtime for your service. exec is not supported for Windows containers.","title":"What does it look like?"},{"location":"docs/commands/svc-init","text":"svc init $ copilot svc init What does it do? copilot svc init creates a new service to run your code for you. After running this command, the CLI creates sub-directory with your app name in your local copilot directory where you'll find a manifest file . Feel free to update your manifest file to change the default configs for your service. The CLI also sets up an ECR repository with a policy for all environments to be able to pull from it. Then, your service gets registered to AWS System Manager Parameter Store so that the CLI can keep track of it. After that, if you already have an environment set up, you can run copilot deploy to deploy your service in that environment. What are the flags? Flags -a, --app string Name of the application. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image. -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile. -n, --name string Name of the service. --port uint16 The port on which your service listens. -t, --svc-type string Type of service to create. Must be one of: \"Request-Driven Web Service\" , \"Load Balanced Web Service\" , \"Backend Service\" . To create a \"frontend\" load balanced web service you could run: $ copilot svc init --name frontend --svc-type \"Load Balanced Web Service\" --dockerfile ./frontend/Dockerfile What does it look like?","title":"svc init"},{"location":"docs/commands/svc-init#svc-init","text":"$ copilot svc init","title":"svc init"},{"location":"docs/commands/svc-init#what-does-it-do","text":"copilot svc init creates a new service to run your code for you. After running this command, the CLI creates sub-directory with your app name in your local copilot directory where you'll find a manifest file . Feel free to update your manifest file to change the default configs for your service. The CLI also sets up an ECR repository with a policy for all environments to be able to pull from it. Then, your service gets registered to AWS System Manager Parameter Store so that the CLI can keep track of it. After that, if you already have an environment set up, you can run copilot deploy to deploy your service in that environment.","title":"What does it do?"},{"location":"docs/commands/svc-init#what-are-the-flags","text":"Flags -a, --app string Name of the application. -d, --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image. -i, --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile. -n, --name string Name of the service. --port uint16 The port on which your service listens. -t, --svc-type string Type of service to create. Must be one of: \"Request-Driven Web Service\" , \"Load Balanced Web Service\" , \"Backend Service\" . To create a \"frontend\" load balanced web service you could run: $ copilot svc init --name frontend --svc-type \"Load Balanced Web Service\" --dockerfile ./frontend/Dockerfile","title":"What are the flags?"},{"location":"docs/commands/svc-init#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/svc-logs","text":"svc logs $ copilot svc logs What does it do? copilot svc logs displays the logs of a deployed service. What are the flags? -a, --app string Name of the application. --end-time string Optional. Only return logs before a specific date ( RFC3339 ) . Defaults to all logs. Only one of end-time / follow may be used. -e, --env string Name of the environment. --follow Optional. Specifies if the logs should be streamed. -h, --help help for logs --json Optional. Outputs in JSON format. --limit int Optional. The maximum number of log events returned. ( default 10 ) -n, --name string Name of the service. --since duration Optional. Only return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. Only one of start-time / since may be used. --start-time string Optional. Only return logs after a specific date ( RFC3339 ) . Defaults to all logs. Only one of start-time / since may be used. --tasks strings Optional. Only return logs from specific task IDs. Examples Displays logs of the service \"my-svc\" in environment \"test\". $ copilot svc logs -n my-svc -e test Displays logs in the last hour. $ copilot svc logs --since 1h Displays logs from 2006-01-02T15:04:05 to 2006-01-02T15:05:05. $ copilot svc logs --start-time 2006 -01-02T15:04:05+00:00 --end-time 2006 -01-02T15:05:05+00:00","title":"svc logs"},{"location":"docs/commands/svc-logs#svc-logs","text":"$ copilot svc logs","title":"svc logs"},{"location":"docs/commands/svc-logs#what-does-it-do","text":"copilot svc logs displays the logs of a deployed service.","title":"What does it do?"},{"location":"docs/commands/svc-logs#what-are-the-flags","text":"-a, --app string Name of the application. --end-time string Optional. Only return logs before a specific date ( RFC3339 ) . Defaults to all logs. Only one of end-time / follow may be used. -e, --env string Name of the environment. --follow Optional. Specifies if the logs should be streamed. -h, --help help for logs --json Optional. Outputs in JSON format. --limit int Optional. The maximum number of log events returned. ( default 10 ) -n, --name string Name of the service. --since duration Optional. Only return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. Only one of start-time / since may be used. --start-time string Optional. Only return logs after a specific date ( RFC3339 ) . Defaults to all logs. Only one of start-time / since may be used. --tasks strings Optional. Only return logs from specific task IDs.","title":"What are the flags?"},{"location":"docs/commands/svc-logs#examples","text":"Displays logs of the service \"my-svc\" in environment \"test\". $ copilot svc logs -n my-svc -e test Displays logs in the last hour. $ copilot svc logs --since 1h Displays logs from 2006-01-02T15:04:05 to 2006-01-02T15:05:05. $ copilot svc logs --start-time 2006 -01-02T15:04:05+00:00 --end-time 2006 -01-02T15:05:05+00:00","title":"Examples"},{"location":"docs/commands/svc-ls","text":"svc ls $ copilot svc ls What does it do? copilot svc ls lists all the Copilot services for a particular application. What are the flags? -a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format. --local Only show services in the workspace. What does it look like?","title":"svc ls"},{"location":"docs/commands/svc-ls#svc-ls","text":"$ copilot svc ls","title":"svc ls"},{"location":"docs/commands/svc-ls#what-does-it-do","text":"copilot svc ls lists all the Copilot services for a particular application.","title":"What does it do?"},{"location":"docs/commands/svc-ls#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for ls --json Optional. Outputs in JSON format. --local Only show services in the workspace.","title":"What are the flags?"},{"location":"docs/commands/svc-ls#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/svc-package","text":"svc package $ copilot svc package What does it do? copilot svc package produces the CloudFormation template(s) used to deploy a service to an environment. What are the flags? -e, --env string Name of the environment. -h, --help help for package -n, --name string Name of the service. --output-dir string Optional. Writes the stack template and template configuration to a directory. --tag string Optional. The service ' s image tag. Example Write the CloudFormation stack and configuration to a \"infrastructure/\" sub-directory instead of printing. $ copilot svc package -n frontend -e test --output-dir ./infrastructure $ ls ./infrastructure frontend.stack.yml frontend-test.config.yml","title":"svc package"},{"location":"docs/commands/svc-package#svc-package","text":"$ copilot svc package","title":"svc package"},{"location":"docs/commands/svc-package#what-does-it-do","text":"copilot svc package produces the CloudFormation template(s) used to deploy a service to an environment.","title":"What does it do?"},{"location":"docs/commands/svc-package#what-are-the-flags","text":"-e, --env string Name of the environment. -h, --help help for package -n, --name string Name of the service. --output-dir string Optional. Writes the stack template and template configuration to a directory. --tag string Optional. The service ' s image tag.","title":"What are the flags?"},{"location":"docs/commands/svc-package#example","text":"Write the CloudFormation stack and configuration to a \"infrastructure/\" sub-directory instead of printing. $ copilot svc package -n frontend -e test --output-dir ./infrastructure $ ls ./infrastructure frontend.stack.yml frontend-test.config.yml","title":"Example"},{"location":"docs/commands/svc-pause","text":"svc pause $ copilot svc pause [ flags ] What does it do? Note svc pause is only supported by services of type \"Request-Driven Web Service\". copilot svc pause pauses the App Runner Service associated with your service within a specific environment. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for pause -n, --name string Name of the service. --yes Skips confirmation prompt. Examples Pause running App Runner service \"my-svc\". $ copilot svc pause -n my-svc","title":"svc pause"},{"location":"docs/commands/svc-pause#svc-pause","text":"$ copilot svc pause [ flags ]","title":"svc pause"},{"location":"docs/commands/svc-pause#what-does-it-do","text":"Note svc pause is only supported by services of type \"Request-Driven Web Service\". copilot svc pause pauses the App Runner Service associated with your service within a specific environment.","title":"What does it do?"},{"location":"docs/commands/svc-pause#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for pause -n, --name string Name of the service. --yes Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/svc-pause#examples","text":"Pause running App Runner service \"my-svc\". $ copilot svc pause -n my-svc","title":"Examples"},{"location":"docs/commands/svc-resume","text":"svc resume $ copilot svc resume [ flags ] What does it do? Note svc resume is only supported by services of type \"Request-Driven Web Service\". copilot svc resume resumes the App Runner Service associated with your service within a specific environment. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for resume -n, --name string Name of the service. Examples Resume paused App Runner service \"my-svc\". $ copilot svc resume -n my-svc","title":"svc resume"},{"location":"docs/commands/svc-resume#svc-resume","text":"$ copilot svc resume [ flags ]","title":"svc resume"},{"location":"docs/commands/svc-resume#what-does-it-do","text":"Note svc resume is only supported by services of type \"Request-Driven Web Service\". copilot svc resume resumes the App Runner Service associated with your service within a specific environment.","title":"What does it do?"},{"location":"docs/commands/svc-resume#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for resume -n, --name string Name of the service.","title":"What are the flags?"},{"location":"docs/commands/svc-resume#examples","text":"Resume paused App Runner service \"my-svc\". $ copilot svc resume -n my-svc","title":"Examples"},{"location":"docs/commands/svc-show","text":"svc show $ copilot svc show What does it do? copilot svc show shows info about a deployed service, including endpoints, capacity and related resources per environment. What are the flags? -a, --app string Name of the application. -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the service. --resources Optional. Show the resources in your service. What does it look like?","title":"svc show"},{"location":"docs/commands/svc-show#svc-show","text":"$ copilot svc show","title":"svc show"},{"location":"docs/commands/svc-show#what-does-it-do","text":"copilot svc show shows info about a deployed service, including endpoints, capacity and related resources per environment.","title":"What does it do?"},{"location":"docs/commands/svc-show#what-are-the-flags","text":"-a, --app string Name of the application. -h, --help help for show --json Optional. Outputs in JSON format. -n, --name string Name of the service. --resources Optional. Show the resources in your service.","title":"What are the flags?"},{"location":"docs/commands/svc-show#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/svc-status","text":"svc status $ copilot svc status What does it do? copilot svc status shows the health status of a deployed service, including service status, task status, and related CloudWatch alarms. What are the flags? -a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for status --json Optional. Outputs in JSON format. -n, --name string Name of the service. What does it look like?","title":"svc status"},{"location":"docs/commands/svc-status#svc-status","text":"$ copilot svc status","title":"svc status"},{"location":"docs/commands/svc-status#what-does-it-do","text":"copilot svc status shows the health status of a deployed service, including service status, task status, and related CloudWatch alarms.","title":"What does it do?"},{"location":"docs/commands/svc-status#what-are-the-flags","text":"-a, --app string Name of the application. -e, --env string Name of the environment. -h, --help help for status --json Optional. Outputs in JSON format. -n, --name string Name of the service.","title":"What are the flags?"},{"location":"docs/commands/svc-status#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/commands/task-delete","text":"task delete $ copilot task delete What does it do? copilot task delete stops running instances of the task, and deletes associated resources. Info Tasks created with versions of Copilot earlier than v1.2.0 cannot be stopped by copilot task delete . Customers using tasks launched with earlier versions should manually stop any running tasks via the ECS console after running the command. What are the flags? -a, --app string Name of the application. --default Optional. Delete a task which was launched in the default cluster and subnets. Cannot be specified with 'app' or 'env'. -e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the service. --yes Optional. Skips confirmation prompt. Example Delete the \"test\" task from the default cluster. $ copilot task delete --name test --default Delete the \"db-migrate\" task from the prod environment. $ copilot task delete --name db-migrate --env prod Delete the \"test\" task without confirmation prompt. $ copilot task delete --name test --yes","title":"task delete"},{"location":"docs/commands/task-delete#task-delete","text":"$ copilot task delete","title":"task delete"},{"location":"docs/commands/task-delete#what-does-it-do","text":"copilot task delete stops running instances of the task, and deletes associated resources. Info Tasks created with versions of Copilot earlier than v1.2.0 cannot be stopped by copilot task delete . Customers using tasks launched with earlier versions should manually stop any running tasks via the ECS console after running the command.","title":"What does it do?"},{"location":"docs/commands/task-delete#what-are-the-flags","text":"-a, --app string Name of the application. --default Optional. Delete a task which was launched in the default cluster and subnets. Cannot be specified with 'app' or 'env'. -e, --env string Name of the environment. -h, --help help for delete -n, --name string Name of the service. --yes Optional. Skips confirmation prompt.","title":"What are the flags?"},{"location":"docs/commands/task-delete#example","text":"Delete the \"test\" task from the default cluster. $ copilot task delete --name test --default Delete the \"db-migrate\" task from the prod environment. $ copilot task delete --name db-migrate --env prod Delete the \"test\" task without confirmation prompt. $ copilot task delete --name test --yes","title":"Example"},{"location":"docs/commands/task-exec","text":"task exec $ copilot task exec What does it do? copilot task exec executes a command in a running container part of a task. What are the flags? -a, --app string Name of the application. -c, --command string Optional. The command that is passed to a running container. (default \"/bin/bash\") --default Optional. Execute commands in running tasks in default cluster and default subnets. Cannot be specified with 'app' or 'env'. -e, --env string Name of the environment. -h, --help help for exec -n, --name string Name of the service, job, or task group. --task-id string Optional. ID of the task you want to exec in. Examples Start an interactive bash session with a task in task group \"db-migrate\" in the \"test\" environment under the current workspace. $ copilot task exec -e test -n db-migrate Runs the 'cat progress.csv' command in the task prefixed with ID \"1848c38\" part of the \"db-migrate\" task group. $ copilot task exec --name db-migrate --task-id 1848c38 --command \"cat progress.csv\" Start an interactive bash session with a task prefixed with ID \"38c3818\" in the default cluster. $ copilot task exec --default --task-id 38c3818 Info copilot task exec cannot be performed without certain task role permissions. If you are using existing task role to run the tasks, please make sure it has the following permissions in order to make copilot task exec work. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"ssmmessages:CreateControlChannel\" , \"ssmmessages:OpenControlChannel\" , \"ssmmessages:CreateDataChannel\" , \"ssmmessages:OpenDataChannel\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : [ \"logs:CreateLogStream\" , \"logs:DescribeLogGroups\" , \"logs:DescribeLogStreams\" , \"logs:PutLogEvents\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" } ] }","title":"task exec"},{"location":"docs/commands/task-exec#task-exec","text":"$ copilot task exec","title":"task exec"},{"location":"docs/commands/task-exec#what-does-it-do","text":"copilot task exec executes a command in a running container part of a task.","title":"What does it do?"},{"location":"docs/commands/task-exec#what-are-the-flags","text":"-a, --app string Name of the application. -c, --command string Optional. The command that is passed to a running container. (default \"/bin/bash\") --default Optional. Execute commands in running tasks in default cluster and default subnets. Cannot be specified with 'app' or 'env'. -e, --env string Name of the environment. -h, --help help for exec -n, --name string Name of the service, job, or task group. --task-id string Optional. ID of the task you want to exec in.","title":"What are the flags?"},{"location":"docs/commands/task-exec#examples","text":"Start an interactive bash session with a task in task group \"db-migrate\" in the \"test\" environment under the current workspace. $ copilot task exec -e test -n db-migrate Runs the 'cat progress.csv' command in the task prefixed with ID \"1848c38\" part of the \"db-migrate\" task group. $ copilot task exec --name db-migrate --task-id 1848c38 --command \"cat progress.csv\" Start an interactive bash session with a task prefixed with ID \"38c3818\" in the default cluster. $ copilot task exec --default --task-id 38c3818 Info copilot task exec cannot be performed without certain task role permissions. If you are using existing task role to run the tasks, please make sure it has the following permissions in order to make copilot task exec work. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"ssmmessages:CreateControlChannel\" , \"ssmmessages:OpenControlChannel\" , \"ssmmessages:CreateDataChannel\" , \"ssmmessages:OpenDataChannel\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : [ \"logs:CreateLogStream\" , \"logs:DescribeLogGroups\" , \"logs:DescribeLogStreams\" , \"logs:PutLogEvents\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" } ] }","title":"Examples"},{"location":"docs/commands/task-run","text":"task run $ copilot task run What does it do? copilot task run deploys and runs one-off tasks. Generally, the steps involved in task run are: Create an ECR repository and a log group for your task Build and push the image to ECR Create or update your ECS task definition Run and wait for the tasks to start Info Tasks with the same group name share the same set of resources, including the CloudFormation stack, ECR repository, CloudWatch log group and task definition. If the tasks are deployed to a Copilot environment (i.e. by specifying --env ), only public subnets that are created by that environment will be used. If you are using the --default flag and get an error saying there's no default cluster, run aws ecs create-cluster and then re-run the Copilot command. What are the flags? --app string Optional. Name of the application. Cannot be specified with 'default', 'subnets' or 'security-groups'. --cluster string Optional. The short name or full ARN of the cluster to run the task in. --command string Optional. The command that is passed to \"docker run\" to override the default command. --count int Optional. The number of tasks to set up. (default 1) --cpu int Optional. The number of CPU units to reserve for each task. (default 256) --default Optional. Run tasks in default cluster and default subnets. Cannot be specified with 'app', 'env' or 'subnets'. --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image (default \"Dockerfile\"). --entrypoint string Optional. The entrypoint that is passed to \"docker run\" to override the default entrypoint. --env string Optional. Name of the environment. Cannot be specified with 'default', 'subnets' or 'security-groups'. --env-vars stringToString Optional. Environment variables specified by key=value separated by commas. (default []) --execution-role string Optional. The role that grants the container agent permission to make AWS API calls. --follow Optional. Specifies if the logs should be streamed. --generate-cmd string Optional. Generate a command with a pre-filled value for each flag. To use it for an ECS service, specify --generate-cmd <cluster name>/<service name>. Alternatively, if the service or job is created with Copilot, specify --generate-cmd <application>/<environment>/<service or job name>. Cannot be specified with any other flags. -h, --help help for run --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile. --memory int Optional. The amount of memory to reserve in MiB for each task. (default 512) --platform-arch string Optional. Architecture of the task. Must be specified along with 'platform-os'. --platform-os string Optional. Operating system of the task. Must be specified along with 'platform-arch'. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. (default []) --secrets stringToString Optional. Secrets to inject into the container. Specified by key=value separated by commas. (default []) --security-groups strings Optional. The security group IDs for the task to use. Can be specified multiple times. Cannot be specified with 'app' or 'env'. --subnets strings Optional. The subnet IDs for the task to use. Can be specified multiple times. Cannot be specified with 'app', 'env' or 'default'. --tag string Optional. The container image tag in addition to \"latest\". -n, --task-group-name string Optional. The group name of the task. Tasks with the same group name share the same set of resources. --task-role string Optional. The role for the task to use. Example Run a task using your local Dockerfile and display log streams after the task is running. You will be prompted to specify an environment for the tasks to run in. $ copilot task run --follow Run a task named \"db-migrate\" in the \"test\" environment under the current workspace. $ copilot task run -n db-migrate --env test --follow Run 4 tasks with 2GB memory, an existing image, and a custom task role. $ copilot task run --count 4 --memory 2048 --image=rds-migrate --task-role migrate-role --follow Run a task with environment variables. $ copilot task run --env-vars name=myName,user=myUser Run a task using the current workspace with specific subnets and security groups. $ copilot task run --subnets subnet-123,subnet-456 --security-groups sg-123,sg-456 Run a task with a command. $ copilot task run --command \"python migrate-script.py\" Run a Windows task with the minimum cpu and memory values. $ copilot task run --platform-os WINDOWS_SERVER_2019_CORE --platform-arch X86_64 --cpu 1024 --memory 2048","title":"task run"},{"location":"docs/commands/task-run#task-run","text":"$ copilot task run","title":"task run"},{"location":"docs/commands/task-run#what-does-it-do","text":"copilot task run deploys and runs one-off tasks. Generally, the steps involved in task run are: Create an ECR repository and a log group for your task Build and push the image to ECR Create or update your ECS task definition Run and wait for the tasks to start Info Tasks with the same group name share the same set of resources, including the CloudFormation stack, ECR repository, CloudWatch log group and task definition. If the tasks are deployed to a Copilot environment (i.e. by specifying --env ), only public subnets that are created by that environment will be used. If you are using the --default flag and get an error saying there's no default cluster, run aws ecs create-cluster and then re-run the Copilot command.","title":"What does it do?"},{"location":"docs/commands/task-run#what-are-the-flags","text":"--app string Optional. Name of the application. Cannot be specified with 'default', 'subnets' or 'security-groups'. --cluster string Optional. The short name or full ARN of the cluster to run the task in. --command string Optional. The command that is passed to \"docker run\" to override the default command. --count int Optional. The number of tasks to set up. (default 1) --cpu int Optional. The number of CPU units to reserve for each task. (default 256) --default Optional. Run tasks in default cluster and default subnets. Cannot be specified with 'app', 'env' or 'subnets'. --dockerfile string Path to the Dockerfile. Mutually exclusive with -i, --image (default \"Dockerfile\"). --entrypoint string Optional. The entrypoint that is passed to \"docker run\" to override the default entrypoint. --env string Optional. Name of the environment. Cannot be specified with 'default', 'subnets' or 'security-groups'. --env-vars stringToString Optional. Environment variables specified by key=value separated by commas. (default []) --execution-role string Optional. The role that grants the container agent permission to make AWS API calls. --follow Optional. Specifies if the logs should be streamed. --generate-cmd string Optional. Generate a command with a pre-filled value for each flag. To use it for an ECS service, specify --generate-cmd <cluster name>/<service name>. Alternatively, if the service or job is created with Copilot, specify --generate-cmd <application>/<environment>/<service or job name>. Cannot be specified with any other flags. -h, --help help for run --image string The location of an existing Docker image. Mutually exclusive with -d, --dockerfile. --memory int Optional. The amount of memory to reserve in MiB for each task. (default 512) --platform-arch string Optional. Architecture of the task. Must be specified along with 'platform-os'. --platform-os string Optional. Operating system of the task. Must be specified along with 'platform-arch'. --resource-tags stringToString Optional. Labels with a key and value separated by commas. Allows you to categorize resources. (default []) --secrets stringToString Optional. Secrets to inject into the container. Specified by key=value separated by commas. (default []) --security-groups strings Optional. The security group IDs for the task to use. Can be specified multiple times. Cannot be specified with 'app' or 'env'. --subnets strings Optional. The subnet IDs for the task to use. Can be specified multiple times. Cannot be specified with 'app', 'env' or 'default'. --tag string Optional. The container image tag in addition to \"latest\". -n, --task-group-name string Optional. The group name of the task. Tasks with the same group name share the same set of resources. --task-role string Optional. The role for the task to use.","title":"What are the flags?"},{"location":"docs/commands/task-run#example","text":"Run a task using your local Dockerfile and display log streams after the task is running. You will be prompted to specify an environment for the tasks to run in. $ copilot task run --follow Run a task named \"db-migrate\" in the \"test\" environment under the current workspace. $ copilot task run -n db-migrate --env test --follow Run 4 tasks with 2GB memory, an existing image, and a custom task role. $ copilot task run --count 4 --memory 2048 --image=rds-migrate --task-role migrate-role --follow Run a task with environment variables. $ copilot task run --env-vars name=myName,user=myUser Run a task using the current workspace with specific subnets and security groups. $ copilot task run --subnets subnet-123,subnet-456 --security-groups sg-123,sg-456 Run a task with a command. $ copilot task run --command \"python migrate-script.py\" Run a Windows task with the minimum cpu and memory values. $ copilot task run --platform-os WINDOWS_SERVER_2019_CORE --platform-arch X86_64 --cpu 1024 --memory 2048","title":"Example"},{"location":"docs/commands/version","text":"version $ copilot version [flags] What does it do? copilot version prints the version of the CLI along with the target operating system it was built for. What are the flags? -h, --help help for version","title":"version"},{"location":"docs/commands/version#version","text":"$ copilot version [flags]","title":"version"},{"location":"docs/commands/version#what-does-it-do","text":"copilot version prints the version of the CLI along with the target operating system it was built for.","title":"What does it do?"},{"location":"docs/commands/version#what-are-the-flags","text":"-h, --help help for version","title":"What are the flags?"},{"location":"docs/concepts/applications","text":"An application is a group of related services, environments, and pipelines. Whether you have one service that does everything or a constellation of micro-services, Copilot organizes them and the environments they're deployed to into an \"application\". Let's walk through an example. We want to build a voting app which needs to collect votes and aggregate the results. To set up our vote app with two services, we can run copilot init twice. The first time we run copilot init , we'll be asked what we should call the application this service will belong to. Since we're trying to build a voting system, we can call our application \"vote\" and our first service \"collector\". The next time we run init , we'll be asked if we want to add our new service to the existing \u201cvote\u201d app, and we\u2019ll name the new service \"aggregator\". Your application configuration (which services and environments belong to it) is stored in your AWS account, so any other users in your account will be able to develop on the \u201cvote\" app as well. This means that you can have a teammate work on one service while you develop the other. Creating an App To set up an application, you can just run copilot init . You'll be asked if you want to set up an app or choose an use app. copilot init Once you've created an application, Copilot stores that application in SSM Parameter store in your AWS account. The account used to set up your application is known as the \"application account\". This is where your app's configuration lives, and anyone who has access to this account can use this app. All resources created within this application will be tagged with the copilot-app aws resource tag . This helps you know which app resources in your account belong to. The name of your application has to be unique within your account (even across region). Additional App Configurations You can also provide more granular configuration for your application by running copilot app init . This includes options to: Tag all application, service and environment resources with an additional set of aws resource tags Use a custom domain name for Load Balanced services $ copilot app init \\ --domain my-awesome-app.aws \\ --resource-tags department = MyDept,team = MyTeam App Infrastructure While the bulk of the infrastructure Copilot provisions is specific to an environment and service, there are some application-wide resources as well. ECR Repositories ECR Repositories are regional resources which store your service images. Each service has its own ECR Repository per region in your app. In the above diagram, the app has several environments spread across three regions. Each of those regions has its own ECR repository for every service in your app. In this case, there are three services. Every time you add a service, we create an ECR Repository in every region. We do this to maintain region isolation (if one region goes down, environments in other region won't be affected) and to reduce cross-region data transfer costs. These ECR Repositories all live within your app's account (not the environment accounts) - and have policies which allow your environment accounts to pull from them. Release Infrastructure For every region represented in your app, we create a KMS Key and an S3 bucket. These resources are used by CodePipeline to enable cross-region and cross-account deployments. All pipelines in your app share these same resources. Similar to the ECR Repositories, the S3 bucket and KMS keys have policies which allow for all of your environments, even in other accounts, to read encrypted deployment artifacts. This makes your cross-account, cross-region CodePipelines possible. Digging into your App Now that we've set up an app, we can check on it using Copilot. Below are a few common ways to check in on your app. What applications are in my account? To see all the apps in your current account and region you can run copilot app ls . $ copilot app ls vote ecs-kudos What's in my application? Running copilot app show will show you a summary of your application, including all the services and environments in your app. $ copilot app show About Name vote URI vote-app.aws Environments Name AccountID Region test 000000000000 us-east-1 Services Name Type collector Load Balanced Web Service aggregator Backend Service","title":"Applications"},{"location":"docs/concepts/applications#creating-an-app","text":"To set up an application, you can just run copilot init . You'll be asked if you want to set up an app or choose an use app. copilot init Once you've created an application, Copilot stores that application in SSM Parameter store in your AWS account. The account used to set up your application is known as the \"application account\". This is where your app's configuration lives, and anyone who has access to this account can use this app. All resources created within this application will be tagged with the copilot-app aws resource tag . This helps you know which app resources in your account belong to. The name of your application has to be unique within your account (even across region).","title":"Creating an App"},{"location":"docs/concepts/applications#additional-app-configurations","text":"You can also provide more granular configuration for your application by running copilot app init . This includes options to: Tag all application, service and environment resources with an additional set of aws resource tags Use a custom domain name for Load Balanced services $ copilot app init \\ --domain my-awesome-app.aws \\ --resource-tags department = MyDept,team = MyTeam","title":"Additional App Configurations"},{"location":"docs/concepts/applications#app-infrastructure","text":"While the bulk of the infrastructure Copilot provisions is specific to an environment and service, there are some application-wide resources as well.","title":"App Infrastructure"},{"location":"docs/concepts/applications#ecr-repositories","text":"ECR Repositories are regional resources which store your service images. Each service has its own ECR Repository per region in your app. In the above diagram, the app has several environments spread across three regions. Each of those regions has its own ECR repository for every service in your app. In this case, there are three services. Every time you add a service, we create an ECR Repository in every region. We do this to maintain region isolation (if one region goes down, environments in other region won't be affected) and to reduce cross-region data transfer costs. These ECR Repositories all live within your app's account (not the environment accounts) - and have policies which allow your environment accounts to pull from them.","title":"ECR Repositories"},{"location":"docs/concepts/applications#release-infrastructure","text":"For every region represented in your app, we create a KMS Key and an S3 bucket. These resources are used by CodePipeline to enable cross-region and cross-account deployments. All pipelines in your app share these same resources. Similar to the ECR Repositories, the S3 bucket and KMS keys have policies which allow for all of your environments, even in other accounts, to read encrypted deployment artifacts. This makes your cross-account, cross-region CodePipelines possible.","title":"Release Infrastructure"},{"location":"docs/concepts/applications#digging-into-your-app","text":"Now that we've set up an app, we can check on it using Copilot. Below are a few common ways to check in on your app.","title":"Digging into your App"},{"location":"docs/concepts/applications#what-applications-are-in-my-account","text":"To see all the apps in your current account and region you can run copilot app ls . $ copilot app ls vote ecs-kudos","title":"What applications are in my account?"},{"location":"docs/concepts/applications#whats-in-my-application","text":"Running copilot app show will show you a summary of your application, including all the services and environments in your app. $ copilot app show About Name vote URI vote-app.aws Environments Name AccountID Region test 000000000000 us-east-1 Services Name Type collector Load Balanced Web Service aggregator Backend Service","title":"What's in my application?"},{"location":"docs/concepts/environments","text":"When you first run copilot init , you're asked if you want to create a test environment. This test environment contains all the AWS resources to provision a secure network (VPC, subnets, security groups, and more), as well as other resources that are meant to be shared between multiple services like an Application Load Balancer or an ECS Cluster. When you deploy your service into your test environment, your service will use the test environment's network and resources. Your application can have multiple environments, and each will have its own networking and shared resources infrastructure. While Copilot creates a test environment for you when you get started, it's common to create a new, separate environment for production. This production environment will be completely independent from the test environment, with its own networking stack and its own copy of services. By having both a test environment and a production environment, you can deploy changes to your test environment, validate them, then promote them to the production environment. In the diagram below we have an application called MyApp with two services, API and Backend . Those two services are deployed to the two environments, test and prod . You can see that in the test environment, both services are running only one container while the prod services have more containers running. Services can use different configurations depending on the environment they're deployed in. For more, check out the using environment variables guide. Creating an Environment To create a new environment in your app, you can run copilot env init from within your workspace. Copilot will ask you what you want to name this environment and what profile you'd like to use to bootstrap the environment. These profiles are AWS named profiles which are associated with a particular account and region. When you select one of these profiles, your environment will be created in whichever account and region that profile is associated with. $ copilot env init After you run copilot env init you can watch as Copilot sets up all the environment resources, which can take a few minutes. Once all those resources are created, the environment will be linked back to the application account. This allows actors in the application account to manage the environment, even without access to the environment account. This linking process also creates and configures new regional ECR repositories, if necessary. Deploying a Service When you first create a new environment, no services are deployed to it. To deploy a service run copilot deploy from that service's directory, and you'll be prompted to select which environment to deploy to. Environment Infrastructure VPC and Networking Each environment gets its own multi-AZ VPC. Your VPC is the network boundary of your environment, allowing the traffic you expect in and out, and blocking the rest. The VPCs Copilot creates are spread across two availability zones to help balance availability and cost - with each AZ getting a public and private subnet. Your services are launched in the public subnets but can be reached only through your load balancer. Load Balancers and DNS If you set up any service using one of the Load Balanced Service types, Copilot will set up an Application Load Balancer. All Load Balanced Web Services within an environment will share a load balancer by creating app-specific listeners on it. Your load balancer is allowed to communicate with services in your VPC. Optionally, when you set up an application, you can provide a domain name that you own and is registered in Route 53. If you provide a domain name, each time you spin up an environment, Copilot will create a subdomain environment-name.app-name.your-domain.com, provision an ACM cert, and bind it to your Application Load Balancer so it can use HTTPS. Customize your Environment Optionally, you can customize your environment interactively by using flags to import your existing resources, or configure the default environment resources. Currently, only VPC resources are customizable. However, if you want to customize more types of resources, feel free to bring your use cases and cut an issue! For more, see our custom environment resources page. Digging into your Environment Now that we've spun up an environment, we can check on it using Copilot. Below are a few common ways to check in on your environment. What environments are part of my app? To see all the environments in your application you can run copilot env ls . $ copilot env ls test production What's in your environment? Running copilot env show will show you a summary of your environment. Here's an example of the output you might see for our test environment. This output includes the account and region the environment is in, the services deployed to that environment, and the tag that all resources created in this environment will have. You can also provide an optional --resources flag to see all AWS resources associated with this environment. $ copilot env show --name test About Name test Production false Region us-west-2 Account ID 693652174720 Services Name Type ---- ---- api Load Balanced Web Service backend Backend Service Tags Key Value --- ----- copilot-application my-app copilot-environment test","title":"Environments"},{"location":"docs/concepts/environments#creating-an-environment","text":"To create a new environment in your app, you can run copilot env init from within your workspace. Copilot will ask you what you want to name this environment and what profile you'd like to use to bootstrap the environment. These profiles are AWS named profiles which are associated with a particular account and region. When you select one of these profiles, your environment will be created in whichever account and region that profile is associated with. $ copilot env init After you run copilot env init you can watch as Copilot sets up all the environment resources, which can take a few minutes. Once all those resources are created, the environment will be linked back to the application account. This allows actors in the application account to manage the environment, even without access to the environment account. This linking process also creates and configures new regional ECR repositories, if necessary.","title":"Creating an Environment"},{"location":"docs/concepts/environments#deploying-a-service","text":"When you first create a new environment, no services are deployed to it. To deploy a service run copilot deploy from that service's directory, and you'll be prompted to select which environment to deploy to.","title":"Deploying a Service"},{"location":"docs/concepts/environments#environment-infrastructure","text":"","title":"Environment Infrastructure"},{"location":"docs/concepts/environments#vpc-and-networking","text":"Each environment gets its own multi-AZ VPC. Your VPC is the network boundary of your environment, allowing the traffic you expect in and out, and blocking the rest. The VPCs Copilot creates are spread across two availability zones to help balance availability and cost - with each AZ getting a public and private subnet. Your services are launched in the public subnets but can be reached only through your load balancer.","title":"VPC and Networking"},{"location":"docs/concepts/environments#load-balancers-and-dns","text":"If you set up any service using one of the Load Balanced Service types, Copilot will set up an Application Load Balancer. All Load Balanced Web Services within an environment will share a load balancer by creating app-specific listeners on it. Your load balancer is allowed to communicate with services in your VPC. Optionally, when you set up an application, you can provide a domain name that you own and is registered in Route 53. If you provide a domain name, each time you spin up an environment, Copilot will create a subdomain environment-name.app-name.your-domain.com, provision an ACM cert, and bind it to your Application Load Balancer so it can use HTTPS.","title":"Load Balancers and DNS"},{"location":"docs/concepts/environments#customize-your-environment","text":"Optionally, you can customize your environment interactively by using flags to import your existing resources, or configure the default environment resources. Currently, only VPC resources are customizable. However, if you want to customize more types of resources, feel free to bring your use cases and cut an issue! For more, see our custom environment resources page.","title":"Customize your Environment"},{"location":"docs/concepts/environments#digging-into-your-environment","text":"Now that we've spun up an environment, we can check on it using Copilot. Below are a few common ways to check in on your environment.","title":"Digging into your Environment"},{"location":"docs/concepts/environments#what-environments-are-part-of-my-app","text":"To see all the environments in your application you can run copilot env ls . $ copilot env ls test production","title":"What environments are part of my app?"},{"location":"docs/concepts/environments#whats-in-your-environment","text":"Running copilot env show will show you a summary of your environment. Here's an example of the output you might see for our test environment. This output includes the account and region the environment is in, the services deployed to that environment, and the tag that all resources created in this environment will have. You can also provide an optional --resources flag to see all AWS resources associated with this environment. $ copilot env show --name test About Name test Production false Region us-west-2 Account ID 693652174720 Services Name Type ---- ---- api Load Balanced Web Service backend Backend Service Tags Key Value --- ----- copilot-application my-app copilot-environment test","title":"What's in your environment?"},{"location":"docs/concepts/jobs","text":"Jobs are Amazon ECS tasks that are triggered by an event. Currently, Copilot supports only \"Scheduled Jobs\". These are tasks that can be triggered either on a fixed schedule or periodically by providing a rate. Creating a Job The easiest way to create a job is to run the init command from the same directory as your Dockerfile. $ copilot init Once you select which application the job should be part of, Copilot will ask you the type of job you'd like to create. Currently, Copilot only supports \"Scheduled Job\". Config and the Manifest After you've run copilot init , the CLI will create a file called manifest.yml . The scheduled job manifest is a simple declarative file that contains the most common configuration for a task that's triggered by a scheduled event. For example, you can configure when you'd like to trigger the job, the container size, the timeout for the task, as well as how many times to retry in case of failures. Deploying a Job Once you've configured your manifest file to satisfy your requirements, you can deploy the changes with the deploy command: $ copilot deploy Running this command will: Build your image locally Push to your job's ECR repository Convert the manifest file to a CloudFormation template Package any additional infrastructure into the CloudFormation template Deploy your resources What's in a Job? Since Copilot uses CloudFormation under the hood, all the resources created are visible and tagged by Copilot. Scheduled Jobs are composed of an AmazonECS Task Definition, Task Role, Task Execution Role, a Step Function State Machine for retrying on failures, and finally an Event Rule to trigger the state machine.","title":"Jobs"},{"location":"docs/concepts/jobs#creating-a-job","text":"The easiest way to create a job is to run the init command from the same directory as your Dockerfile. $ copilot init Once you select which application the job should be part of, Copilot will ask you the type of job you'd like to create. Currently, Copilot only supports \"Scheduled Job\".","title":"Creating a Job"},{"location":"docs/concepts/jobs#config-and-the-manifest","text":"After you've run copilot init , the CLI will create a file called manifest.yml . The scheduled job manifest is a simple declarative file that contains the most common configuration for a task that's triggered by a scheduled event. For example, you can configure when you'd like to trigger the job, the container size, the timeout for the task, as well as how many times to retry in case of failures.","title":"Config and the Manifest"},{"location":"docs/concepts/jobs#deploying-a-job","text":"Once you've configured your manifest file to satisfy your requirements, you can deploy the changes with the deploy command: $ copilot deploy Running this command will: Build your image locally Push to your job's ECR repository Convert the manifest file to a CloudFormation template Package any additional infrastructure into the CloudFormation template Deploy your resources","title":"Deploying a Job"},{"location":"docs/concepts/jobs#whats-in-a-job","text":"Since Copilot uses CloudFormation under the hood, all the resources created are visible and tagged by Copilot. Scheduled Jobs are composed of an AmazonECS Task Definition, Task Role, Task Execution Role, a Step Function State Machine for retrying on failures, and finally an Event Rule to trigger the state machine.","title":"What's in a Job?"},{"location":"docs/concepts/overview","text":"Concepts Copilot makes it super easy to set up and deploy your containers on AWS - but getting started is only the first step of the journey. What happens when you want to have one copy of your service running only for testing and another copy serving production traffic? What happens when you want to add another service? How do you manage deploying to all of these services? Copilot wants to help you with all of these things so let's jump into some of Copilot's core concepts to understand how they can help. Applications An Application is a collection of services and environments. When you get started with Copilot, the first thing you'll be asked to do is choose an application name. This can be a high level description of the product you're trying to build. An example might be an application named \"chat\" which has two services \"frontend\" and \"api\" . These two services could then be deployed to a \"test\" and \"production\" environment. Environments Rumor has it, there are people out there that can write perfect code on the first go without any bugs. While we tip our hats to those folks, we believe it's important to be able to test new code on a non-customer facing version of your service before promoting to production. In Copilot we do this by using environments . Each environment can have its own version of a service running allowing you to create a \"test\" and \"production\" environment. You can deploy your service to the test environment, make sure everything looks good, then deploy to your production environment. Since each environment is independent, if you deploy a bug to your test environment, customers using a service deployed to your production environment will be fine. Until now we've been talking about just one service, but what happens when you want to add another service? Perhaps you want to add a backend service to complement your frontend service. Each environment contains a set of resources shared between all the deployed services - these resources include the network (VPC, Subnets, Security Groups, etc...), the ECS Cluster, and the load balancer. If you deploy both your frontend and backend service to your test environment, both services will share the same network and cluster. Services A service is your code and all of the supporting infrastructure needed to get it up and running on AWS. When you first get started setting up a service, Copilot will ask you what type of service you want to create. The type of service determines the infrastructure that'll be created to support your code. If you want your code to serve traffic from the internet, for example, Copilot can set up an Application Load Balancer and an Amazon ECS Service running on AWS Fargate. Once you've told Copilot what type of service you're building, Copilot will take care of building your code's Dockerfile and storing the images securely in an Amazon ECR repository. Copilot will also create a simple file called the manifest which contains all the knobs and toggles for your service. This includes things like how much memory and CPU should be allocated to each copy of your service, how many copies of your service you want running, and more. Jobs Jobs are ephemeral Amazon ECS tasks that are triggered by an event. Once their work is done, the task terminates. Just like services, Copilot will ask you all the necessary information to quickly get going with a scheduled task on AWS. The manifest file can always be used to adjust the configuration and provide more advanced settings. Pipelines Now that you've got an application with a few services deployed to a couple of environments, staying on top of those deployments can become tricky. Copilot can help by setting up a release pipeline that deploys your service whenever you push to your git repository. (At this time, Copilot supports GitHub, Bitbucket, and CodeCommit repositories.) When a push is detected, your pipeline will build your service, push the image to ECR, and deploy to your environments. A common pattern is to set up a pipeline for a particular service that deploys to a test environment, runs automated testing, then deploys to the production environment.","title":"Overview"},{"location":"docs/concepts/overview#concepts","text":"Copilot makes it super easy to set up and deploy your containers on AWS - but getting started is only the first step of the journey. What happens when you want to have one copy of your service running only for testing and another copy serving production traffic? What happens when you want to add another service? How do you manage deploying to all of these services? Copilot wants to help you with all of these things so let's jump into some of Copilot's core concepts to understand how they can help.","title":"Concepts"},{"location":"docs/concepts/overview#applications","text":"An Application is a collection of services and environments. When you get started with Copilot, the first thing you'll be asked to do is choose an application name. This can be a high level description of the product you're trying to build. An example might be an application named \"chat\" which has two services \"frontend\" and \"api\" . These two services could then be deployed to a \"test\" and \"production\" environment.","title":"Applications"},{"location":"docs/concepts/overview#environments","text":"Rumor has it, there are people out there that can write perfect code on the first go without any bugs. While we tip our hats to those folks, we believe it's important to be able to test new code on a non-customer facing version of your service before promoting to production. In Copilot we do this by using environments . Each environment can have its own version of a service running allowing you to create a \"test\" and \"production\" environment. You can deploy your service to the test environment, make sure everything looks good, then deploy to your production environment. Since each environment is independent, if you deploy a bug to your test environment, customers using a service deployed to your production environment will be fine. Until now we've been talking about just one service, but what happens when you want to add another service? Perhaps you want to add a backend service to complement your frontend service. Each environment contains a set of resources shared between all the deployed services - these resources include the network (VPC, Subnets, Security Groups, etc...), the ECS Cluster, and the load balancer. If you deploy both your frontend and backend service to your test environment, both services will share the same network and cluster.","title":"Environments"},{"location":"docs/concepts/overview#services","text":"A service is your code and all of the supporting infrastructure needed to get it up and running on AWS. When you first get started setting up a service, Copilot will ask you what type of service you want to create. The type of service determines the infrastructure that'll be created to support your code. If you want your code to serve traffic from the internet, for example, Copilot can set up an Application Load Balancer and an Amazon ECS Service running on AWS Fargate. Once you've told Copilot what type of service you're building, Copilot will take care of building your code's Dockerfile and storing the images securely in an Amazon ECR repository. Copilot will also create a simple file called the manifest which contains all the knobs and toggles for your service. This includes things like how much memory and CPU should be allocated to each copy of your service, how many copies of your service you want running, and more.","title":"Services"},{"location":"docs/concepts/overview#jobs","text":"Jobs are ephemeral Amazon ECS tasks that are triggered by an event. Once their work is done, the task terminates. Just like services, Copilot will ask you all the necessary information to quickly get going with a scheduled task on AWS. The manifest file can always be used to adjust the configuration and provide more advanced settings.","title":"Jobs"},{"location":"docs/concepts/overview#pipelines","text":"Now that you've got an application with a few services deployed to a couple of environments, staying on top of those deployments can become tricky. Copilot can help by setting up a release pipeline that deploys your service whenever you push to your git repository. (At this time, Copilot supports GitHub, Bitbucket, and CodeCommit repositories.) When a push is detected, your pipeline will build your service, push the image to ECR, and deploy to your environments. A common pattern is to set up a pipeline for a particular service that deploys to a test environment, runs automated testing, then deploys to the production environment.","title":"Pipelines"},{"location":"docs/concepts/pipelines","text":"Having an automated release process is one of the most important parts of software delivery, so Copilot wants to make setting up that process as easy as possible \ud83d\ude80. In this section, we'll talk about using Copilot to set up a CodePipeline that automatically builds your service code when you push to your GitHub, Bitbucket or AWS CodeCommit repository, deploys to your environments, and runs automated testing. Attention AWS CodePipeline is not supported for services with Windows as the OS Family. CodePipeline uses Linux-based AWS CodeBuild for the 'build' stage, so for now, Copilot pipelines cannot build Windows containers. Why? We won't get too philosophical about releasing software, but what's the point of having a release pipeline? With copilot deploy you can deploy your service directly from your computer to ECS, so why add a middleman? That's a great question. For some apps, manually using deploy is enough, but as your release process gets more complicated (as you add more environments or add automated testing, for example) you want to offload the boring work of repeatedly orchestrating that process to a service. With two services, each having two environments (test and production, say), running integration tests after you deploy to your test environment becomes surprisingly cumbersome to do by hand. Using an automated release tool like CodePipeline helps make your release manageable. Even if your release isn't particularly complicated, knowing that you can just git push to deploy your change always feels a little magical \ud83c\udf08. Pipeline structure Copilot can set up a CodePipeline for you with a few commands - but before we jump into that, let's talk a little bit about the structure of the pipeline we'll be generating. Our pipeline will have the following basic structure: Source Stage - when you push to a configured GitHub, Bitbucket, or CodeCommit repository branch, a new pipeline execution is triggered. Build Stage - after your source code is pulled from your repository host, your service's container image is built and published to every environment's ECR repository. Deploy Stages - after your code is built, you can deploy to any or all of your environments, with optional post-deployment tests or manual approvals. Once you've set up a CodePipeline using Copilot, all you'll have to do is push to your GitHub, Bitbucket, or CodeCommit repository, and CodePipeline will orchestrate the deployments. Want to learn more about CodePipeline? Check out their getting started docs . Creating a Pipeline in 3 steps Creating a Pipeline requires only three steps: Preparing the pipeline structure. Committing and pushing the files generated in the copilot/ directory. Creating the actual CodePipeline. Follow the three steps below, from your workspace root: $ copilot pipeline init $ git add copilot/pipeline.yml copilot/buildspec.yml copilot/.workspace && git commit -m \"Adding pipeline artifacts\" && git push $ copilot pipeline update \u2728 And you'll have a new pipeline configured in your application account. Want to understand a little bit more what's going on? Read on! Setting up a Pipeline, step by step Step 1: Configuring your Pipeline Pipeline configurations are created at a workspace level. If your workspace has a single service, then your pipeline will be triggered only for that service. However, if you have multiple services in a workspace, then the pipeline will build all the services in the workspace. To start setting up a pipeline, cd into your service(s)'s workspace and run: copilot pipeline init This won't create your pipeline, but it will create some local files that will be used when creating your pipeline. Release order : You'll be prompted for environments you want to deploy to - select them based on the order you want them to be deployed in your pipeline (deployments happen one environment at a time). You may, for example, want to deploy to your test environment first, and then your prod environment. Tracking repository : After you've selected the environments you want to deploy to, you'll be prompted to select which repository you want your CodePipeline to track. This is the repository that, when pushed to, will trigger a pipeline execution. (If the repository you're interested in doesn't show up, you can pass it in using the --url flag.) Step 2: Updating the Pipeline manifest (optional) Just like your service has a simple manifest file, so does your pipeline. After you run pipeline init , two files are created: pipeline.yml and buildspec.yml , both in your copilot/ directory. If you poke in, you'll see that the pipeline.yml looks something like this (for a service called \"api-frontend\" with two environments, \"test\" and \"prod\"): # The manifest for the \"pipeline-ecs-kudos-kohidave-demo-api-frontend\" pipeline. # This YAML file defines your pipeline: the source repository it tracks and the order of the environments to deploy to. # For more info: https://aws.github.io/copilot-cli/docs/manifest/pipeline/ # The name of the pipeline. name : pipeline-ecs-kudos-kohidave-demo-api-frontend # The version of the schema used in this template. version : 1 # This section defines your source, changes to which trigger your pipeline. source : # The name of the provider that is used to store the source artifacts. # (i.e. GitHub, Bitbucket, CodeCommit) provider : GitHub # Additional properties that further specify the location of the artifacts. properties : branch : main repository : https://github.com/kohidave/demo-api-frontend # Optional: specify the name of an existing CodeStar Connections connection. # connection_name: a-connection # This section defines the order of the environments your pipeline will deploy to. stages : - # The name of the environment. name : test test_commands : - make test - echo \"woo! Tests passed\" - # The name of the environment. name : prod # requires_approval: true You can see every available configuration option for pipeline.yml on the pipeline manifest page. There are 3 main parts of this file: the name field, which is the name of your CodePipeline, the source section, which details the repository and branch to track, and the stages section, which lists the environments you want this pipeline to deploy to. You can update this anytime, but you must run copilot pipeline update afterwards. Typically, you'll update this file if you add new environments you want to deploy to, or want to track a different branch. If you are using CodeStar Connections to connect to your repository and would like to utilize an existing connection rather than let Copilot generate one for you, you may add the connection name here. The pipeline manifest is also where you may add a manual approval step before deployment or commands to run tests (see \"Adding Tests,\" below) after deployment. Step 3: Updating the Buildspec (optional) Along with pipeline.yml , the pipeline init command also generated a buildspec.yml file in the copilot/ directory. This contains the instructions for building and publishing your service. If you want to run any additional commands, besides docker build , such as unit tests or style checkers, feel free to add them to the buildspec's build phase. When this buildspec runs, it pulls down the version of Copilot which was used when you ran pipeline init , to ensure backwards compatibility. Step 4: Pushing New Files to your Repository Now that your pipeline.yml , buildspec.yml , and .workspace files have been created, add them to your repository. These files in your copilot/ directory are required for your pipeline's build stage to run successfully. Step 5: Creating your Pipeline Here's the fun part! Run: copilot pipeline update This parses your pipeline.yml , creates a CodePipeline in the same account and region as your application and kicks off a pipeline execution. Log into the AWS Console to watch your pipeline go, or run copilot pipeline status to check in on its execution. Info If you have selected a GitHub or Bitbucket repository, Copilot will help you connect to your source code with CodeStar Connections . You will need to install the AWS authentication app on your third-party account and update the connection status. Copilot and the AWS Management Console will guide you through these steps. Adding Tests Of course, one of the most important parts of a pipeline is the automated testing. To add tests, such as integration or end-to-end tests, that run after a deployment stage, include those commands in the test_commands section. If all the tests succeed, your change is promoted to the next stage. Adding test_commands generates a CodeBuild project with the aws/codebuild/amazonlinux2-x86_64-standard:3.0 image - so most commands from Amazon Linux 2 (including make ) are available for use. Are your tests configured to run inside a Docker container? Copilot's test commands CodeBuild project supports Docker, so docker build commands are available as well. In the example below, the pipeline will run the make test command (in your source code directory) and only promote the change to the prod stage if that command exits successfully. name : pipeline-ecs-kudos-kohidave-demo-api-frontend version : 1 source : provider : GitHub properties : branch : main repository : https://github.com/kohidave/demo-api-frontend stages : - name : test # A change will only deploy to the production stage if the # make test and echo commands exit successfully. test_commands : - make test - echo \"woo! Tests passed\" - name : prod","title":"Pipelines"},{"location":"docs/concepts/pipelines#why","text":"We won't get too philosophical about releasing software, but what's the point of having a release pipeline? With copilot deploy you can deploy your service directly from your computer to ECS, so why add a middleman? That's a great question. For some apps, manually using deploy is enough, but as your release process gets more complicated (as you add more environments or add automated testing, for example) you want to offload the boring work of repeatedly orchestrating that process to a service. With two services, each having two environments (test and production, say), running integration tests after you deploy to your test environment becomes surprisingly cumbersome to do by hand. Using an automated release tool like CodePipeline helps make your release manageable. Even if your release isn't particularly complicated, knowing that you can just git push to deploy your change always feels a little magical \ud83c\udf08.","title":"Why?"},{"location":"docs/concepts/pipelines#pipeline-structure","text":"Copilot can set up a CodePipeline for you with a few commands - but before we jump into that, let's talk a little bit about the structure of the pipeline we'll be generating. Our pipeline will have the following basic structure: Source Stage - when you push to a configured GitHub, Bitbucket, or CodeCommit repository branch, a new pipeline execution is triggered. Build Stage - after your source code is pulled from your repository host, your service's container image is built and published to every environment's ECR repository. Deploy Stages - after your code is built, you can deploy to any or all of your environments, with optional post-deployment tests or manual approvals. Once you've set up a CodePipeline using Copilot, all you'll have to do is push to your GitHub, Bitbucket, or CodeCommit repository, and CodePipeline will orchestrate the deployments. Want to learn more about CodePipeline? Check out their getting started docs .","title":"Pipeline structure"},{"location":"docs/concepts/pipelines#creating-a-pipeline-in-3-steps","text":"Creating a Pipeline requires only three steps: Preparing the pipeline structure. Committing and pushing the files generated in the copilot/ directory. Creating the actual CodePipeline. Follow the three steps below, from your workspace root: $ copilot pipeline init $ git add copilot/pipeline.yml copilot/buildspec.yml copilot/.workspace && git commit -m \"Adding pipeline artifacts\" && git push $ copilot pipeline update \u2728 And you'll have a new pipeline configured in your application account. Want to understand a little bit more what's going on? Read on!","title":"Creating a Pipeline in 3 steps"},{"location":"docs/concepts/pipelines#setting-up-a-pipeline-step-by-step","text":"","title":"Setting up a Pipeline, step by step"},{"location":"docs/concepts/pipelines#step-1-configuring-your-pipeline","text":"Pipeline configurations are created at a workspace level. If your workspace has a single service, then your pipeline will be triggered only for that service. However, if you have multiple services in a workspace, then the pipeline will build all the services in the workspace. To start setting up a pipeline, cd into your service(s)'s workspace and run: copilot pipeline init This won't create your pipeline, but it will create some local files that will be used when creating your pipeline. Release order : You'll be prompted for environments you want to deploy to - select them based on the order you want them to be deployed in your pipeline (deployments happen one environment at a time). You may, for example, want to deploy to your test environment first, and then your prod environment. Tracking repository : After you've selected the environments you want to deploy to, you'll be prompted to select which repository you want your CodePipeline to track. This is the repository that, when pushed to, will trigger a pipeline execution. (If the repository you're interested in doesn't show up, you can pass it in using the --url flag.)","title":"Step 1: Configuring your Pipeline"},{"location":"docs/concepts/pipelines#step-2-updating-the-pipeline-manifest-optional","text":"Just like your service has a simple manifest file, so does your pipeline. After you run pipeline init , two files are created: pipeline.yml and buildspec.yml , both in your copilot/ directory. If you poke in, you'll see that the pipeline.yml looks something like this (for a service called \"api-frontend\" with two environments, \"test\" and \"prod\"): # The manifest for the \"pipeline-ecs-kudos-kohidave-demo-api-frontend\" pipeline. # This YAML file defines your pipeline: the source repository it tracks and the order of the environments to deploy to. # For more info: https://aws.github.io/copilot-cli/docs/manifest/pipeline/ # The name of the pipeline. name : pipeline-ecs-kudos-kohidave-demo-api-frontend # The version of the schema used in this template. version : 1 # This section defines your source, changes to which trigger your pipeline. source : # The name of the provider that is used to store the source artifacts. # (i.e. GitHub, Bitbucket, CodeCommit) provider : GitHub # Additional properties that further specify the location of the artifacts. properties : branch : main repository : https://github.com/kohidave/demo-api-frontend # Optional: specify the name of an existing CodeStar Connections connection. # connection_name: a-connection # This section defines the order of the environments your pipeline will deploy to. stages : - # The name of the environment. name : test test_commands : - make test - echo \"woo! Tests passed\" - # The name of the environment. name : prod # requires_approval: true You can see every available configuration option for pipeline.yml on the pipeline manifest page. There are 3 main parts of this file: the name field, which is the name of your CodePipeline, the source section, which details the repository and branch to track, and the stages section, which lists the environments you want this pipeline to deploy to. You can update this anytime, but you must run copilot pipeline update afterwards. Typically, you'll update this file if you add new environments you want to deploy to, or want to track a different branch. If you are using CodeStar Connections to connect to your repository and would like to utilize an existing connection rather than let Copilot generate one for you, you may add the connection name here. The pipeline manifest is also where you may add a manual approval step before deployment or commands to run tests (see \"Adding Tests,\" below) after deployment.","title":"Step 2: Updating the Pipeline manifest (optional)"},{"location":"docs/concepts/pipelines#step-3-updating-the-buildspec-optional","text":"Along with pipeline.yml , the pipeline init command also generated a buildspec.yml file in the copilot/ directory. This contains the instructions for building and publishing your service. If you want to run any additional commands, besides docker build , such as unit tests or style checkers, feel free to add them to the buildspec's build phase. When this buildspec runs, it pulls down the version of Copilot which was used when you ran pipeline init , to ensure backwards compatibility.","title":"Step 3: Updating the Buildspec (optional)"},{"location":"docs/concepts/pipelines#step-4-pushing-new-files-to-your-repository","text":"Now that your pipeline.yml , buildspec.yml , and .workspace files have been created, add them to your repository. These files in your copilot/ directory are required for your pipeline's build stage to run successfully.","title":"Step 4: Pushing New Files to your Repository"},{"location":"docs/concepts/pipelines#step-5-creating-your-pipeline","text":"Here's the fun part! Run: copilot pipeline update This parses your pipeline.yml , creates a CodePipeline in the same account and region as your application and kicks off a pipeline execution. Log into the AWS Console to watch your pipeline go, or run copilot pipeline status to check in on its execution. Info If you have selected a GitHub or Bitbucket repository, Copilot will help you connect to your source code with CodeStar Connections . You will need to install the AWS authentication app on your third-party account and update the connection status. Copilot and the AWS Management Console will guide you through these steps.","title":"Step 5: Creating your Pipeline"},{"location":"docs/concepts/pipelines#adding-tests","text":"Of course, one of the most important parts of a pipeline is the automated testing. To add tests, such as integration or end-to-end tests, that run after a deployment stage, include those commands in the test_commands section. If all the tests succeed, your change is promoted to the next stage. Adding test_commands generates a CodeBuild project with the aws/codebuild/amazonlinux2-x86_64-standard:3.0 image - so most commands from Amazon Linux 2 (including make ) are available for use. Are your tests configured to run inside a Docker container? Copilot's test commands CodeBuild project supports Docker, so docker build commands are available as well. In the example below, the pipeline will run the make test command (in your source code directory) and only promote the change to the prod stage if that command exits successfully. name : pipeline-ecs-kudos-kohidave-demo-api-frontend version : 1 source : provider : GitHub properties : branch : main repository : https://github.com/kohidave/demo-api-frontend stages : - name : test # A change will only deploy to the production stage if the # make test and echo commands exit successfully. test_commands : - make test - echo \"woo! Tests passed\" - name : prod","title":"Adding Tests"},{"location":"docs/concepts/services","text":"One of the awesome things about containers is that once you've written your code, running it locally is as easy as typing docker run . Copilot makes running those same containers on AWS as easy as typing copilot init . Copilot will build your image, push it to Amazon ECR and set up all the infrastructure to run your service in a scalable and secure way. Creating a Service Creating a service to run your containers on AWS can be done in a few ways. The easiest way is by running the init command from the same directory as your Dockerfile. $ copilot init You'll be asked which application you want this service to be a part of (or asked to create an application if there isn't one). Copilot will then ask about the type of service you're trying to build. After selecting a service type, Copilot will detect any health checks or exposed ports from your Dockerfile and ask if you'd like to deploy. Choosing a Service Type We mentioned before that Copilot will set up all the infrastructure your service needs to run. But how does it know what kind of infrastructure to use? When you're setting up a service, Copilot will ask you about what kind of service you want to build. Internet-facing services If you want your service to serve internet traffic then you have two options: \"Request-Driven Web Service\" will provision an AWS App Runner Service to run your service. \"Load Balanced Web Service\" will provision an Application Load Balancer, security groups, an ECS service on Fargate to run your service. Request-Driven Web Service An AWS App Runner service that autoscales your instances based on incoming traffic and scales down to a baseline instance when there's no traffic. This option is more cost effective for HTTP services with sudden bursts in request volumes or low request volumes. Load Balanced Web Service An ECS Service running tasks on Fargate with an Application Load Balancer as ingress. This option is suitable for HTTP services with steady request volumes that need to access resources in a VPC or require advanced configuration. Backend Service If you want a service that can't be accessed externally, but only from other services within your application, you can create a Backend Service . Copilot will provision an ECS Service running on AWS Fargate, but won't set up any internet-facing endpoints. Worker Service Worker Services allow you to implement asynchronous service-to-service communication with pub/sub architectures . Your microservices in your application can publish events to Amazon SNS topics that can then be consumed by a \"Worker Service\". A Worker Service is composed of: One or more Amazon SQS queues to process notifications published to the topics, as well as dead-letter queues to handle failures. An Amazon ECS service on AWS Fargate that has permission to poll the SQS queues and process the messages asynchronously. Config and the Manifest After you've run copilot init you might have noticed that Copilot created a file called manifest.yml in the copilot directory. This manifest file contains common configuration options for your service. While the exact set of options depends on the type of service you're running, common ones include the resources allocated to your service (like memory and CPU), health checks, and environment variables. Let's take a look at the manifest for a Load Balanced Web Service called front-end . name : front-end type : Load Balanced Web Service image : # Path to your service's Dockerfile. build : ./Dockerfile # Port exposed through your container to route traffic to it. port : 8080 http : # Requests to this path will be forwarded to your service. # To match all requests you can use the \"/\" path. path : '/' # You can specify a custom health check path. The default is \"/\" # healthcheck: '/' # Number of CPU units for the task. cpu : 256 # Amount of memory in MiB used by the task. memory : 512 # Number of tasks that should be running in your service. count : 1 # Optional fields for more advanced use-cases. # variables : # Pass environment variables as key value pairs. LOG_LEVEL : info #secrets: # Pass secrets from AWS Systems Manager (SSM) Parameter Store. # GITHUB_TOKEN: GH_SECRET_TOKEN # The key is the name of the environment variable, # the value is the name of the SSM parameter. # You can override any of the values defined above by environment. environments : prod : count : 2 # Number of tasks to run for the \"prod\" environment. To learn about the specification of manifest files, see the manifest page. Deploying a Service Once you've set up your service, you can deploy it (and any changes to your manifest) by running the deploy command: $ copilot deploy Running this command will: Build your image locally Push to your service's ECR repository Convert your manifest file to CloudFormation Package any additional infrastructure into CloudFormation Deploy your updated service and resources to CloudFormation If you have multiple environments, you'll be prompted to select which environment you want to deploy to. Digging into your Service Now that we've got a service up and running, we can check on it using Copilot. Below are a few common ways to check in on your deployed service. What's in your service? Running copilot svc show will show you a summary of your service. Here's an example of the output you might see for a load balanced web application. This output includes the configuration of your service for each environment, all the endpoints for your service, and the environment variables passed into your service. You can also provide an optional --resources flag to see all AWS resources associated with your service. $ copilot svc show About Application my-app Name front-end Type Load Balanced Web Service Configurations Environment Tasks CPU ( vCPU ) Memory ( MiB ) Port test 1 0 .25 512 80 Routes Environment URL test http://my-ap-Publi-1RV8QEBNTEQCW-1762184596.ca-central-1.elb.amazonaws.com Service Discovery Environment Namespace test front-end.test.my-app.local:8080 Variables Name Environment Value COPILOT_APPLICATION_NAME test my-app COPILOT_ENVIRONMENT_NAME test test COPILOT_LB_DNS test my-ap-Publi-1RV8QEBNTEQCW-1762184596.ca-central-1.elb.amazonaws.com COPILOT_SERVICE_DISCOVERY_ENDPOINT test test.my-app.local COPILOT_SERVICE_NAME test front-end What's your service status? Often it's handy to be able to check on the status of your service. Are all the instances of my service healthy? Are there any alarms firing? To do that, you can run copilot svc status to get a summary of your service's status. $ copilot svc status Service Status ACTIVE 1 / 1 running tasks ( 0 pending ) Last Deployment Updated At 12 minutes ago Task Definition arn:aws:ecs:ca-central-1:693652174720:task-definition/my-app-test-front-end:1 Task Status ID Image Digest Last Status Health Status Started At Stopped At 37236ed3 da3cfcdd RUNNING HEALTHY 12 minutes ago - Alarms Name Health Last Updated Reason CPU-Utilization OK 5 minutes ago - Where are my service logs? Checking the your service logs is easy as well. Running copilot svc logs will show the most recent logs of your service. You can follow your logs live with the --follow flag. $ copilot svc logs 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok!","title":"Services"},{"location":"docs/concepts/services#creating-a-service","text":"Creating a service to run your containers on AWS can be done in a few ways. The easiest way is by running the init command from the same directory as your Dockerfile. $ copilot init You'll be asked which application you want this service to be a part of (or asked to create an application if there isn't one). Copilot will then ask about the type of service you're trying to build. After selecting a service type, Copilot will detect any health checks or exposed ports from your Dockerfile and ask if you'd like to deploy.","title":"Creating a Service"},{"location":"docs/concepts/services#choosing-a-service-type","text":"We mentioned before that Copilot will set up all the infrastructure your service needs to run. But how does it know what kind of infrastructure to use? When you're setting up a service, Copilot will ask you about what kind of service you want to build.","title":"Choosing a Service Type"},{"location":"docs/concepts/services#internet-facing-services","text":"If you want your service to serve internet traffic then you have two options: \"Request-Driven Web Service\" will provision an AWS App Runner Service to run your service. \"Load Balanced Web Service\" will provision an Application Load Balancer, security groups, an ECS service on Fargate to run your service.","title":"Internet-facing services"},{"location":"docs/concepts/services#request-driven-web-service","text":"An AWS App Runner service that autoscales your instances based on incoming traffic and scales down to a baseline instance when there's no traffic. This option is more cost effective for HTTP services with sudden bursts in request volumes or low request volumes.","title":"Request-Driven Web Service"},{"location":"docs/concepts/services#load-balanced-web-service","text":"An ECS Service running tasks on Fargate with an Application Load Balancer as ingress. This option is suitable for HTTP services with steady request volumes that need to access resources in a VPC or require advanced configuration.","title":"Load Balanced Web Service"},{"location":"docs/concepts/services#backend-service","text":"If you want a service that can't be accessed externally, but only from other services within your application, you can create a Backend Service . Copilot will provision an ECS Service running on AWS Fargate, but won't set up any internet-facing endpoints.","title":"Backend Service"},{"location":"docs/concepts/services#worker-service","text":"Worker Services allow you to implement asynchronous service-to-service communication with pub/sub architectures . Your microservices in your application can publish events to Amazon SNS topics that can then be consumed by a \"Worker Service\". A Worker Service is composed of: One or more Amazon SQS queues to process notifications published to the topics, as well as dead-letter queues to handle failures. An Amazon ECS service on AWS Fargate that has permission to poll the SQS queues and process the messages asynchronously.","title":"Worker Service"},{"location":"docs/concepts/services#config-and-the-manifest","text":"After you've run copilot init you might have noticed that Copilot created a file called manifest.yml in the copilot directory. This manifest file contains common configuration options for your service. While the exact set of options depends on the type of service you're running, common ones include the resources allocated to your service (like memory and CPU), health checks, and environment variables. Let's take a look at the manifest for a Load Balanced Web Service called front-end . name : front-end type : Load Balanced Web Service image : # Path to your service's Dockerfile. build : ./Dockerfile # Port exposed through your container to route traffic to it. port : 8080 http : # Requests to this path will be forwarded to your service. # To match all requests you can use the \"/\" path. path : '/' # You can specify a custom health check path. The default is \"/\" # healthcheck: '/' # Number of CPU units for the task. cpu : 256 # Amount of memory in MiB used by the task. memory : 512 # Number of tasks that should be running in your service. count : 1 # Optional fields for more advanced use-cases. # variables : # Pass environment variables as key value pairs. LOG_LEVEL : info #secrets: # Pass secrets from AWS Systems Manager (SSM) Parameter Store. # GITHUB_TOKEN: GH_SECRET_TOKEN # The key is the name of the environment variable, # the value is the name of the SSM parameter. # You can override any of the values defined above by environment. environments : prod : count : 2 # Number of tasks to run for the \"prod\" environment. To learn about the specification of manifest files, see the manifest page.","title":"Config and the Manifest"},{"location":"docs/concepts/services#deploying-a-service","text":"Once you've set up your service, you can deploy it (and any changes to your manifest) by running the deploy command: $ copilot deploy Running this command will: Build your image locally Push to your service's ECR repository Convert your manifest file to CloudFormation Package any additional infrastructure into CloudFormation Deploy your updated service and resources to CloudFormation If you have multiple environments, you'll be prompted to select which environment you want to deploy to.","title":"Deploying a Service"},{"location":"docs/concepts/services#digging-into-your-service","text":"Now that we've got a service up and running, we can check on it using Copilot. Below are a few common ways to check in on your deployed service.","title":"Digging into your Service"},{"location":"docs/concepts/services#whats-in-your-service","text":"Running copilot svc show will show you a summary of your service. Here's an example of the output you might see for a load balanced web application. This output includes the configuration of your service for each environment, all the endpoints for your service, and the environment variables passed into your service. You can also provide an optional --resources flag to see all AWS resources associated with your service. $ copilot svc show About Application my-app Name front-end Type Load Balanced Web Service Configurations Environment Tasks CPU ( vCPU ) Memory ( MiB ) Port test 1 0 .25 512 80 Routes Environment URL test http://my-ap-Publi-1RV8QEBNTEQCW-1762184596.ca-central-1.elb.amazonaws.com Service Discovery Environment Namespace test front-end.test.my-app.local:8080 Variables Name Environment Value COPILOT_APPLICATION_NAME test my-app COPILOT_ENVIRONMENT_NAME test test COPILOT_LB_DNS test my-ap-Publi-1RV8QEBNTEQCW-1762184596.ca-central-1.elb.amazonaws.com COPILOT_SERVICE_DISCOVERY_ENDPOINT test test.my-app.local COPILOT_SERVICE_NAME test front-end","title":"What's in your service?"},{"location":"docs/concepts/services#whats-your-service-status","text":"Often it's handy to be able to check on the status of your service. Are all the instances of my service healthy? Are there any alarms firing? To do that, you can run copilot svc status to get a summary of your service's status. $ copilot svc status Service Status ACTIVE 1 / 1 running tasks ( 0 pending ) Last Deployment Updated At 12 minutes ago Task Definition arn:aws:ecs:ca-central-1:693652174720:task-definition/my-app-test-front-end:1 Task Status ID Image Digest Last Status Health Status Started At Stopped At 37236ed3 da3cfcdd RUNNING HEALTHY 12 minutes ago - Alarms Name Health Last Updated Reason CPU-Utilization OK 5 minutes ago -","title":"What's your service status?"},{"location":"docs/concepts/services#where-are-my-service-logs","text":"Checking the your service logs is easy as well. Running copilot svc logs will show the most recent logs of your service. You can follow your logs live with the --follow flag. $ copilot svc logs 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok! 37236ed 10 .0.0.30 \ud83d\ude91 Health-check ok!","title":"Where are my service logs?"},{"location":"docs/developing/additional-aws-resources","text":"Additional AWS Resources Additional AWS resources, referred to as \"addons\" in the CLI, are any additional AWS services that a service manifest does not integrate by default. For example, an addon can be a DynamoDB table, an S3 bucket, or an RDS Aurora Serverless cluster that your service needs to read or write to. How do I add an S3 bucket, a DDB Table, or an Aurora Serverless cluster? Copilot provides the following commands to help you create certain kinds of addons: storage init will create a DynamoDB table, an S3 bucket, or an Aurora Serverless cluster. You can run copilot storage init from your workspace and be guided through some questions to help you set up these resources. How to do I add other resources? For other types of addons, you can add your own custom CloudFormation templates according to the following instructions. Let's say you have a service named webhook in your workspace: . \u2514\u2500\u2500 copilot \u2514\u2500\u2500 webhook \u2514\u2500\u2500 manifest.yml And you want to add a custom DynamoDB table to webhook . Then under the webhook/ directory, create a new addons/ directory and add a CloudFormation template for your instance. . \u2514\u2500\u2500 copilot \u2514\u2500\u2500 webhook \u251c\u2500\u2500 addons \u2502 \u2514\u2500\u2500 mytable-ddb.yaml \u2514\u2500\u2500 manifest.yaml Typically each file under the addons/ directory represents a separate addon and is represented as an AWS CloudFormation (CFN) template . For example, if we want to also add an S3 bucket addon to our service then we could either run storage init or create our own custom, separate mybucket-s3.yaml file. When your service gets deployed, Copilot merges all these files into a single AWS CloudFormation template and creates a nested stack under your service's stack. What does an addon template look like? An addon template can be any valid CloudFormation template. However, by default, Copilot will pass the App , Env , and Name Parameters ; you can customize your resource properties with Conditions or Mappings if you wish to. Connecting addon resources to your workloads Here are several possible ways to access addon Resources from your ECS task or App Runner instance: If you need to add additional policies to your ECS task role or App Runner instance role, you can define an IAM ManagedPolicy addon resource in your template that holds the additional permissions, and then output it. The permission will be injected into your task or instance role. If you need to add a security group to your ECS service, you can define a Security Group in your template, and then add it as an Output . The security group will be automatically attached to your ECS service. If you'd like to inject a secret to your ECS task, you can define a Secret in your template, and then add it as an Output . The secret will be injected into your container and can be accessed as an environment variable as capital SNAKE_CASE. If you'd like to inject any resource value as an environment variable, you can create an Output for any value that you want to be injected as an environment variable to your ECS tasks. It will be injected into your container and accessed as an environment variable as capital SNAKE_CASE. Writing a template When writing your own template, you must: Include the Parameters section with App , Env , Name . Include at least one Resource . Here is an example template for a DynamoDB table addon: # You can use any of these parameters to create conditions or mappings in your template. Parameters : App : Type : String Description : Your application's name. Env : Type : String Description : The environment name your service, job, or workflow is being deployed to. Name : Type : String Description : The name of the service, job, or workflow being deployed. Resources : # Create your resource here, such as an AWS::DynamoDB::Table: # MyTable: # Type: AWS::DynamoDB::Table # Properties: # ... # 1. In addition to your resource, if you need to access the resource from your ECS task # then you need to create an AWS::IAM::ManagedPolicy that holds the permissions for your resource. # # For example, below is a sample policy for MyTable: MyTableAccessPolicy : Type : AWS::IAM::ManagedPolicy Properties : PolicyDocument : Version : 2012-10-17 Statement : - Sid : DDBActions Effect : Allow Action : - dynamodb:BatchGet* - dynamodb:DescribeStream - dynamodb:DescribeTable - dynamodb:Get* - dynamodb:Query - dynamodb:Scan - dynamodb:BatchWrite* - dynamodb:Create* - dynamodb:Delete* - dynamodb:Update* - dynamodb:PutItem Resource : !Sub ${ MyTable.Arn} Outputs : # 1. You need to output the IAM ManagedPolicy so that Copilot can add it as a managed policy to your ECS task role. MyTableAccessPolicyArn : Description : \"The ARN of the ManagedPolicy to attach to the task role.\" Value : !Ref MyTableAccessPolicy # 2. If you want to inject a property of your resource as an environment variable to your ECS task, # then you need to define an output for it. # # For example, the output MyTableName will be injected in capital snake case, MY_TABLE_NAME, to your task. MyTableName : Description : \"The name of this DynamoDB.\" Value : !Ref MyTable On your next release, Copilot will include this template as a nested stack under your service! Info We recommend following Amazon IAM best practices while defining AWS Managed Policies for the additional resources, including: Grant least privilege to the policies defined in your addons/ directory. Use policy conditions for extra security to restrict your policies to access only the resources defined in your addons/ directory. Customizing the Parameters section AWS Copilot always requires the App , Env , and Name parameters to be defined in your template. However, if you'd like to define additional parameters that refer to resources in your service stack you can do so with a addons.parameters.yml file. . \u2514\u2500\u2500 addons/ \u251c\u2500\u2500 template.yml \u2514\u2500\u2500 addons.parameters.yml # Add this file under your addons/ directory. In your addons.parameters.yml , you can define additional parameters that can refer to values from your workload stack. For example: Parameters : ServiceName : !GetAtt Service.Name Finally, update your template file to refer to the new parameter: Parameters : # Required parameters by AWS Copilot. App : Type : String Env : Type : String Name : Type : String # Additional parameters defined in addons.parameters.yml ServiceName : Type : String","title":"Additional AWS Resources"},{"location":"docs/developing/additional-aws-resources#additional-aws-resources","text":"Additional AWS resources, referred to as \"addons\" in the CLI, are any additional AWS services that a service manifest does not integrate by default. For example, an addon can be a DynamoDB table, an S3 bucket, or an RDS Aurora Serverless cluster that your service needs to read or write to.","title":"Additional AWS Resources"},{"location":"docs/developing/additional-aws-resources#how-do-i-add-an-s3-bucket-a-ddb-table-or-an-aurora-serverless-cluster","text":"Copilot provides the following commands to help you create certain kinds of addons: storage init will create a DynamoDB table, an S3 bucket, or an Aurora Serverless cluster. You can run copilot storage init from your workspace and be guided through some questions to help you set up these resources.","title":"How do I add an S3 bucket, a DDB Table, or an Aurora Serverless cluster?"},{"location":"docs/developing/additional-aws-resources#how-to-do-i-add-other-resources","text":"For other types of addons, you can add your own custom CloudFormation templates according to the following instructions. Let's say you have a service named webhook in your workspace: . \u2514\u2500\u2500 copilot \u2514\u2500\u2500 webhook \u2514\u2500\u2500 manifest.yml And you want to add a custom DynamoDB table to webhook . Then under the webhook/ directory, create a new addons/ directory and add a CloudFormation template for your instance. . \u2514\u2500\u2500 copilot \u2514\u2500\u2500 webhook \u251c\u2500\u2500 addons \u2502 \u2514\u2500\u2500 mytable-ddb.yaml \u2514\u2500\u2500 manifest.yaml Typically each file under the addons/ directory represents a separate addon and is represented as an AWS CloudFormation (CFN) template . For example, if we want to also add an S3 bucket addon to our service then we could either run storage init or create our own custom, separate mybucket-s3.yaml file. When your service gets deployed, Copilot merges all these files into a single AWS CloudFormation template and creates a nested stack under your service's stack.","title":"How to do I add other resources?"},{"location":"docs/developing/additional-aws-resources#what-does-an-addon-template-look-like","text":"An addon template can be any valid CloudFormation template. However, by default, Copilot will pass the App , Env , and Name Parameters ; you can customize your resource properties with Conditions or Mappings if you wish to.","title":"What does an addon template look like?"},{"location":"docs/developing/additional-aws-resources#connecting-addon-resources-to-your-workloads","text":"Here are several possible ways to access addon Resources from your ECS task or App Runner instance: If you need to add additional policies to your ECS task role or App Runner instance role, you can define an IAM ManagedPolicy addon resource in your template that holds the additional permissions, and then output it. The permission will be injected into your task or instance role. If you need to add a security group to your ECS service, you can define a Security Group in your template, and then add it as an Output . The security group will be automatically attached to your ECS service. If you'd like to inject a secret to your ECS task, you can define a Secret in your template, and then add it as an Output . The secret will be injected into your container and can be accessed as an environment variable as capital SNAKE_CASE. If you'd like to inject any resource value as an environment variable, you can create an Output for any value that you want to be injected as an environment variable to your ECS tasks. It will be injected into your container and accessed as an environment variable as capital SNAKE_CASE.","title":"Connecting addon resources to your workloads"},{"location":"docs/developing/additional-aws-resources#writing-a-template","text":"When writing your own template, you must: Include the Parameters section with App , Env , Name . Include at least one Resource . Here is an example template for a DynamoDB table addon: # You can use any of these parameters to create conditions or mappings in your template. Parameters : App : Type : String Description : Your application's name. Env : Type : String Description : The environment name your service, job, or workflow is being deployed to. Name : Type : String Description : The name of the service, job, or workflow being deployed. Resources : # Create your resource here, such as an AWS::DynamoDB::Table: # MyTable: # Type: AWS::DynamoDB::Table # Properties: # ... # 1. In addition to your resource, if you need to access the resource from your ECS task # then you need to create an AWS::IAM::ManagedPolicy that holds the permissions for your resource. # # For example, below is a sample policy for MyTable: MyTableAccessPolicy : Type : AWS::IAM::ManagedPolicy Properties : PolicyDocument : Version : 2012-10-17 Statement : - Sid : DDBActions Effect : Allow Action : - dynamodb:BatchGet* - dynamodb:DescribeStream - dynamodb:DescribeTable - dynamodb:Get* - dynamodb:Query - dynamodb:Scan - dynamodb:BatchWrite* - dynamodb:Create* - dynamodb:Delete* - dynamodb:Update* - dynamodb:PutItem Resource : !Sub ${ MyTable.Arn} Outputs : # 1. You need to output the IAM ManagedPolicy so that Copilot can add it as a managed policy to your ECS task role. MyTableAccessPolicyArn : Description : \"The ARN of the ManagedPolicy to attach to the task role.\" Value : !Ref MyTableAccessPolicy # 2. If you want to inject a property of your resource as an environment variable to your ECS task, # then you need to define an output for it. # # For example, the output MyTableName will be injected in capital snake case, MY_TABLE_NAME, to your task. MyTableName : Description : \"The name of this DynamoDB.\" Value : !Ref MyTable On your next release, Copilot will include this template as a nested stack under your service! Info We recommend following Amazon IAM best practices while defining AWS Managed Policies for the additional resources, including: Grant least privilege to the policies defined in your addons/ directory. Use policy conditions for extra security to restrict your policies to access only the resources defined in your addons/ directory.","title":"Writing a template"},{"location":"docs/developing/additional-aws-resources#customizing-the-parameters-section","text":"AWS Copilot always requires the App , Env , and Name parameters to be defined in your template. However, if you'd like to define additional parameters that refer to resources in your service stack you can do so with a addons.parameters.yml file. . \u2514\u2500\u2500 addons/ \u251c\u2500\u2500 template.yml \u2514\u2500\u2500 addons.parameters.yml # Add this file under your addons/ directory. In your addons.parameters.yml , you can define additional parameters that can refer to values from your workload stack. For example: Parameters : ServiceName : !GetAtt Service.Name Finally, update your template file to refer to the new parameter: Parameters : # Required parameters by AWS Copilot. App : Type : String Env : Type : String Name : Type : String # Additional parameters defined in addons.parameters.yml ServiceName : Type : String","title":"Customizing the Parameters section"},{"location":"docs/developing/custom-environment-resources","text":"Custom Environment Resources When creating a new environment with Copilot, you are given the option to import existing VPC resources. (Use flags with env init or the guided experience, shown below.) % copilot env init What is your environment 's name? env-name Which credentials would you like to use to create name? [profile default] Would you like to use the default configuration for a new environment? - A new VPC with 2 AZs, 2 public subnets and 2 private subnets - A new ECS Cluster - New IAM Roles to manage services and jobs in your environment [Use arrows to move, type to filter] Yes, use default. Yes, but I' d like configure the default resources ( CIDR ranges ) . > No, I ' d like to import existing resources ( VPC, subnets ) . When you select the default configuration, Copilot follows AWS best practices and creates a VPC with two public and two private subnets, with one of each type in one of two Availability Zones. While this is a good configuration for most cases, Copilot allows some flexibility when you import your own resources. For example, you may bring a VPC with only two private subnets and no public subnets for your workloads that are not internet-facing. (For more details on the resources you'll need for isolated networks, go here .) Considerations If you are importing an existing VPC, we recommend following Security best practices for your VPC and the Security & Filtering section from the Amazon VPC FAQs . If you are using a private hosted zone, you must set enableDnsHostname and enableDnsSupport to true. To deploy internet-facing workloads in private subnets , your VPC will need a NAT gateway .","title":"Custom Environment Resources"},{"location":"docs/developing/custom-environment-resources#custom-environment-resources","text":"When creating a new environment with Copilot, you are given the option to import existing VPC resources. (Use flags with env init or the guided experience, shown below.) % copilot env init What is your environment 's name? env-name Which credentials would you like to use to create name? [profile default] Would you like to use the default configuration for a new environment? - A new VPC with 2 AZs, 2 public subnets and 2 private subnets - A new ECS Cluster - New IAM Roles to manage services and jobs in your environment [Use arrows to move, type to filter] Yes, use default. Yes, but I' d like configure the default resources ( CIDR ranges ) . > No, I ' d like to import existing resources ( VPC, subnets ) . When you select the default configuration, Copilot follows AWS best practices and creates a VPC with two public and two private subnets, with one of each type in one of two Availability Zones. While this is a good configuration for most cases, Copilot allows some flexibility when you import your own resources. For example, you may bring a VPC with only two private subnets and no public subnets for your workloads that are not internet-facing. (For more details on the resources you'll need for isolated networks, go here .)","title":"Custom Environment Resources"},{"location":"docs/developing/custom-environment-resources#considerations","text":"If you are importing an existing VPC, we recommend following Security best practices for your VPC and the Security & Filtering section from the Amazon VPC FAQs . If you are using a private hosted zone, you must set enableDnsHostname and enableDnsSupport to true. To deploy internet-facing workloads in private subnets , your VPC will need a NAT gateway .","title":"Considerations"},{"location":"docs/developing/domain","text":"Domain Load Balanced Web Service As mentioned in the Application Guide , you can configure the domain name of your app when running copilot app init . After deploying your Load Balanced Web Services , you should be able to access them publicly via ${SvcName}.${EnvName}.${AppName}.${DomainName} For example: https://kudo.test.coolapp.example.aws Currently, you can only use aliases under the domain you specified when creating the application. Since we delegate responsibility for the subdomain to Route 53 , the alias you specify must be in either one of these three hosted zone: root: ${DomainName} app: ${AppName}.${DomainName} env: ${EnvName}.${AppName}.${DomainName} We'll make this feature more powerful in the future by allowing you to import certificates and use any aliases! Info Both root and app hosted zone are in your app account, while the env hosted zones are in your env accounts. How do I configure an alias for my service? If you don't like the default domain name Copilot assigns to your service, setting an alias for your service is also very easy. You can add it directly to your manifest's alias section. The following snippet will set an alias to your service. # in copilot/{service name}/manifest.yml http : path : '/' alias : example.aws Info Using this feature requires your app version to be at least v1.0.0 . You will be prompted to run app upgrade first if your app version does not meet the requirement. What happens under the hood? Under the hood, Copilot creates a hosted zone in your app account for the new app subdomain ${AppName}.${DomainName} creates another hosted zone in your env account for the new env subdomain ${EnvName}.${AppName}.${DomainName} creates and validates an ACM certificate for the env subdomain associates the certificate with your HTTPS listener and redirects HTTP traffic to HTTPS creates an optional A record for your alias What does it look like? Request-Driven Web Service You can also add a custom domain for your request-driven web service. Similar to Load Balanced Web Service, you can do so by modifying the alias field in your manifest: # in copilot/{service name}/manifest.yml http : path : '/' alias : web.example.aws Likewise, your application should have been associated with the domain (e.g. example.aws ) in order for your Request-Driven Web Service to use it. Info For now, we support only 1-level subdomain such as web.example.aws . Environment-level domains (e.g. web.${envName}.${appName}.example.aws ), application-level domains (e.g. web.${appName}.example.aws ), or root domains (i.e. example.aws ) are not supported yet. This also means that your subdomain shouldn't collide with your application name. Under the hood, Copilot: associates the domain with your app runner service creates the domain record as well as the validation records in your root domain's hosted zone","title":"Domain"},{"location":"docs/developing/domain#domain","text":"","title":"Domain"},{"location":"docs/developing/domain#load-balanced-web-service","text":"As mentioned in the Application Guide , you can configure the domain name of your app when running copilot app init . After deploying your Load Balanced Web Services , you should be able to access them publicly via ${SvcName}.${EnvName}.${AppName}.${DomainName} For example: https://kudo.test.coolapp.example.aws Currently, you can only use aliases under the domain you specified when creating the application. Since we delegate responsibility for the subdomain to Route 53 , the alias you specify must be in either one of these three hosted zone: root: ${DomainName} app: ${AppName}.${DomainName} env: ${EnvName}.${AppName}.${DomainName} We'll make this feature more powerful in the future by allowing you to import certificates and use any aliases! Info Both root and app hosted zone are in your app account, while the env hosted zones are in your env accounts.","title":"Load Balanced Web Service"},{"location":"docs/developing/domain#how-do-i-configure-an-alias-for-my-service","text":"If you don't like the default domain name Copilot assigns to your service, setting an alias for your service is also very easy. You can add it directly to your manifest's alias section. The following snippet will set an alias to your service. # in copilot/{service name}/manifest.yml http : path : '/' alias : example.aws Info Using this feature requires your app version to be at least v1.0.0 . You will be prompted to run app upgrade first if your app version does not meet the requirement.","title":"How do I configure an alias for my service?"},{"location":"docs/developing/domain#what-happens-under-the-hood","text":"Under the hood, Copilot creates a hosted zone in your app account for the new app subdomain ${AppName}.${DomainName} creates another hosted zone in your env account for the new env subdomain ${EnvName}.${AppName}.${DomainName} creates and validates an ACM certificate for the env subdomain associates the certificate with your HTTPS listener and redirects HTTP traffic to HTTPS creates an optional A record for your alias","title":"What happens under the hood?"},{"location":"docs/developing/domain#what-does-it-look-like","text":"","title":"What does it look like?"},{"location":"docs/developing/domain#request-driven-web-service","text":"You can also add a custom domain for your request-driven web service. Similar to Load Balanced Web Service, you can do so by modifying the alias field in your manifest: # in copilot/{service name}/manifest.yml http : path : '/' alias : web.example.aws Likewise, your application should have been associated with the domain (e.g. example.aws ) in order for your Request-Driven Web Service to use it. Info For now, we support only 1-level subdomain such as web.example.aws . Environment-level domains (e.g. web.${envName}.${appName}.example.aws ), application-level domains (e.g. web.${appName}.example.aws ), or root domains (i.e. example.aws ) are not supported yet. This also means that your subdomain shouldn't collide with your application name. Under the hood, Copilot: associates the domain with your app runner service creates the domain record as well as the validation records in your root domain's hosted zone","title":"Request-Driven Web Service"},{"location":"docs/developing/environment-variables","text":"Environment Variables Environment variables are variables that are available to your service, based on the environment they're running in. Your service can reference them without having to define them. Environment variables are useful for when you want to pass in data to your service that's specific to a particular environment. For example, your test database name versus your production database name. Accessing environment variables is usually simply based on the language you're using. Here are some examples of getting an environment variable called DATABASE_NAME in a few different languages. Go dbName := os . Getenv ( \"DATABASE_NAME\" ) Javascript var dbName = process . env . DATABASE_NAME ; Python database_name = os . getenv ( 'DATABASE_NAME' ) What are the Default Environment Variables? By default, the AWS Copilot CLI passes in some default environment variables for your service to use. COPILOT_APPLICATION_NAME - this is the name of the application this service is running in. COPILOT_ENVIRONMENT_NAME - this is the name of the environment the service is running in (test vs prod, for example) COPILOT_SERVICE_NAME - this is the name of the current service. COPILOT_LB_DNS - this is the DNS name of the Load Balancer (if it exists) such as kudos-Publi-MC2WNHAIOAVS-588300247.us-west-2.elb.amazonaws.com . Note: if you're using a custom domain name, this value will still be the Load Balancer's DNS name. COPILOT_SERVICE_DISCOVERY_ENDPOINT - this is the endpoint to add after a service name to talk to another service in your environment via service discovery. The value is {env name}.{app name}.local . For more information about service discovery, check out our Service Discovery guide . How do I add my own Environment Variables? Adding your own environment variable is easy. You can add them directly to your manifest in the variables section. The following snippet will pass a environment variable called LOG_LEVEL to your service, with the value set to debug . # in copilot/{service name}/manifest.yml variables : LOG_LEVEL : debug You can also pass in a specific value for an environment variable based on the environment. We'll follow the same example as above, by setting the log level, but overriding the value to be info in our production environment. Changes to your manifest take effect when you deploy them, so changing them locally is safe. # in copilot/{service name}/manifest.yml variables : LOG_LEVEL : debug environments : production : variables : LOG_LEVEL : info Here's a quick guide showing you how to add environment variables to your app by editing the manifest \ud83d\udc47 How do I know the name of my DynamoDB table, S3 bucket, RDS database, etc? When using the Copilot CLI to provision additional AWS resources such as DynamoDB tables, S3 buckets, databases, etc., any output values will be passed in as environment variables to your app. For more information, check out the additional resources guide .","title":"Container Environment Variables"},{"location":"docs/developing/environment-variables#environment-variables","text":"Environment variables are variables that are available to your service, based on the environment they're running in. Your service can reference them without having to define them. Environment variables are useful for when you want to pass in data to your service that's specific to a particular environment. For example, your test database name versus your production database name. Accessing environment variables is usually simply based on the language you're using. Here are some examples of getting an environment variable called DATABASE_NAME in a few different languages. Go dbName := os . Getenv ( \"DATABASE_NAME\" ) Javascript var dbName = process . env . DATABASE_NAME ; Python database_name = os . getenv ( 'DATABASE_NAME' )","title":"Environment Variables"},{"location":"docs/developing/environment-variables#what-are-the-default-environment-variables","text":"By default, the AWS Copilot CLI passes in some default environment variables for your service to use. COPILOT_APPLICATION_NAME - this is the name of the application this service is running in. COPILOT_ENVIRONMENT_NAME - this is the name of the environment the service is running in (test vs prod, for example) COPILOT_SERVICE_NAME - this is the name of the current service. COPILOT_LB_DNS - this is the DNS name of the Load Balancer (if it exists) such as kudos-Publi-MC2WNHAIOAVS-588300247.us-west-2.elb.amazonaws.com . Note: if you're using a custom domain name, this value will still be the Load Balancer's DNS name. COPILOT_SERVICE_DISCOVERY_ENDPOINT - this is the endpoint to add after a service name to talk to another service in your environment via service discovery. The value is {env name}.{app name}.local . For more information about service discovery, check out our Service Discovery guide .","title":"What are the Default Environment Variables?"},{"location":"docs/developing/environment-variables#how-do-i-add-my-own-environment-variables","text":"Adding your own environment variable is easy. You can add them directly to your manifest in the variables section. The following snippet will pass a environment variable called LOG_LEVEL to your service, with the value set to debug . # in copilot/{service name}/manifest.yml variables : LOG_LEVEL : debug You can also pass in a specific value for an environment variable based on the environment. We'll follow the same example as above, by setting the log level, but overriding the value to be info in our production environment. Changes to your manifest take effect when you deploy them, so changing them locally is safe. # in copilot/{service name}/manifest.yml variables : LOG_LEVEL : debug environments : production : variables : LOG_LEVEL : info Here's a quick guide showing you how to add environment variables to your app by editing the manifest \ud83d\udc47","title":"How do I add my own Environment Variables?"},{"location":"docs/developing/environment-variables#how-do-i-know-the-name-of-my-dynamodb-table-s3-bucket-rds-database-etc","text":"When using the Copilot CLI to provision additional AWS resources such as DynamoDB tables, S3 buckets, databases, etc., any output values will be passed in as environment variables to your app. For more information, check out the additional resources guide .","title":"How do I know the name of my DynamoDB table, S3 bucket, RDS database, etc?"},{"location":"docs/developing/manifest-env-var","text":"Environment Variables in the Manifest Shell environment variables It\u2019s possible to use environment variables in your shell to populate values in your manifest files: image : location : id.dkr.ecr.zone.amazonaws.com/project-name:${TAG} Suppose the shell has TAG=version01 , the manifest example will be resolved as image : location : id.dkr.ecr.zone.amazonaws.com/project-name:version01 When Copilot defines the container, it will use the image located at id.dkr.ecr.zone.amazonaws.com/project-name and with tag version01 . Info At this moment, you can only substitute shell environment variables for fields that accept strings, including String (e.g., image.location ), Array of Strings (e.g., entrypoint ), or Map where the value type is String (e.g., secrets ). Predefined variables Predefined variables are reserved variables that will be resolved by Copilot when interpreting the manifest. Currently, available predefined environment variables include: COPILOT_APPLICATION_NAME COPILOT_ENVIRONMENT_NAME secrets : DB_PASSWORD : /copilot/${COPILOT_APPLICATION_NAME}/${COPILOT_ENVIRONMENT_NAME}/secrets/db_password Copilot will substitute ${COPILOT_APPLICATION_NAME} and ${COPILOT_ENVIRONMENT_NAME} with the names of the application and the environment where the workload is deployed. For example, when you run $ copilot svc deploy --app my-app --env test to deploy the service to the test environment in your my-app application, Copilot will resolve /copilot/${COPILOT_APPLICATION_NAME}/${COPILOT_ENVIRONMENT_NAME}/secrets/db_password to /copilot/my-app/test/secrets/db_password . (For more information of secret injection, see here ).","title":"Manifest Environment Variables"},{"location":"docs/developing/manifest-env-var#environment-variables-in-the-manifest","text":"","title":"Environment Variables in the Manifest"},{"location":"docs/developing/manifest-env-var#shell-environment-variables","text":"It\u2019s possible to use environment variables in your shell to populate values in your manifest files: image : location : id.dkr.ecr.zone.amazonaws.com/project-name:${TAG} Suppose the shell has TAG=version01 , the manifest example will be resolved as image : location : id.dkr.ecr.zone.amazonaws.com/project-name:version01 When Copilot defines the container, it will use the image located at id.dkr.ecr.zone.amazonaws.com/project-name and with tag version01 . Info At this moment, you can only substitute shell environment variables for fields that accept strings, including String (e.g., image.location ), Array of Strings (e.g., entrypoint ), or Map where the value type is String (e.g., secrets ).","title":"Shell environment variables"},{"location":"docs/developing/manifest-env-var#predefined-variables","text":"Predefined variables are reserved variables that will be resolved by Copilot when interpreting the manifest. Currently, available predefined environment variables include: COPILOT_APPLICATION_NAME COPILOT_ENVIRONMENT_NAME secrets : DB_PASSWORD : /copilot/${COPILOT_APPLICATION_NAME}/${COPILOT_ENVIRONMENT_NAME}/secrets/db_password Copilot will substitute ${COPILOT_APPLICATION_NAME} and ${COPILOT_ENVIRONMENT_NAME} with the names of the application and the environment where the workload is deployed. For example, when you run $ copilot svc deploy --app my-app --env test to deploy the service to the test environment in your my-app application, Copilot will resolve /copilot/${COPILOT_APPLICATION_NAME}/${COPILOT_ENVIRONMENT_NAME}/secrets/db_password to /copilot/my-app/test/secrets/db_password . (For more information of secret injection, see here ).","title":"Predefined variables"},{"location":"docs/developing/publish-subscribe","text":"Publish/Subscribe Architectures Copilot Worker Services take advantage of the publish field common to all service and job types to allow customers to easily create publish/subscribe logic for passing messages between services. A common pattern in AWS is the combination of SNS and SQS to deliver and process messages. SNS is a robust message delivery system which can send messages to a variety of subscribed endpoints with guarantees about message delivery. SQS is a message queue to allow asynchronous processing of messages. Queues can be populated by one or more SNS topics or AWS EventBridge event filters. The combination of these two services effectively decouples the sending and receipt of messages, meaning publishers don't have to care what queues are actually subscribed to their topics, and worker service code doesn't have to care where the messages come from. Sending Messages from a Publisher To allow an existing service to publish messages to SNS, simply set the publish field in its manifest. You'll need a name for the topic which describes its function. # manifest.yml for api service name : api type : Backend Service publish : topics : - name : ordersTopic This will create an SNS topic and set a resource policy on the topic to allow SQS queues in your AWS account to create subscriptions. Copilot also injects the ARNs of any SNS topics into your container under the environment variable COPILOT_SNS_TOPIC_ARNS . Javascript Example Once the publishing service has been deployed, you can send messages to SNS via the AWS SDK for SNS. const { SNSClient , PublishCommand } = require ( \"@aws-sdk/client-sns\" ); const client = new SNSClient ({ region : \"us-west-2\" }); const { ordersTopic } = JSON . parse ( process . env . COPILOT_SNS_TOPIC_ARNS ); const out = await client . send ( new PublishCommand ({ Message : \"hello\" , TopicArn : ordersTopic , })); Subscribing to a topic with a Worker Service To subscribe to an existing SNS topic with a worker service, you'll need to edit the worker service's manifest. Using the subscribe field in the manifest, you can define subscriptions to existing SNS topics exposed by other services in your environment. In this example, we'll use the ordersTopic topic which the api service from the last section exposed. We'll also customize the worker service's queue to enable a dead-letter queue. The tries field tells SQS how many times to try redelivering a failed message before sending it to the DLQ for further inspection. name : orders-worker type : Worker Service subscribe : topics : - name : ordersTopic service : api queue : dead_letter : tries : 5 Copilot will create a subscription between this worker service's queue and the ordersTopic topic from the api service. It will also inject the queue URI into the service container under the environment variable COPILOT_QUEUE_URI . Javascript Example The central business logic of a worker service's container involves pulling messages from the queue. To do this with the AWS SDK, you can use the SQS Clients for your language of choice. In Javascript, the logic for pulling, processing, and deleting messages from the queue would look like the following code snipped. const { SQSClient , ReceiveMessageCommand , DeleteMessageCommand } = require ( \"@aws-sdk/client-sqs\" ); const client = new SQSClient ({ region : \"us-west-2\" }); const out = await client . send ( new ReceiveMessageCommand ({ QueueUrl : process . env . COPILOT_QUEUE_URI , WaitTimeSeconds : 10 , })); console . log ( `results: ${ JSON . stringify ( out ) } ` ); if ( out . Messages === undefined || out . Messages . length === 0 ) { return ; } // Process the message here. await client . send ( new DeleteMessageCommand ({ QueueUrl : process . env . COPILOT_QUEUE_URI , ReceiptHandle : out . Messages [ 0 ]. ReceiptHandle , }));","title":"Publish/Subscribe"},{"location":"docs/developing/publish-subscribe#publishsubscribe-architectures","text":"Copilot Worker Services take advantage of the publish field common to all service and job types to allow customers to easily create publish/subscribe logic for passing messages between services. A common pattern in AWS is the combination of SNS and SQS to deliver and process messages. SNS is a robust message delivery system which can send messages to a variety of subscribed endpoints with guarantees about message delivery. SQS is a message queue to allow asynchronous processing of messages. Queues can be populated by one or more SNS topics or AWS EventBridge event filters. The combination of these two services effectively decouples the sending and receipt of messages, meaning publishers don't have to care what queues are actually subscribed to their topics, and worker service code doesn't have to care where the messages come from.","title":"Publish/Subscribe Architectures"},{"location":"docs/developing/publish-subscribe#sending-messages-from-a-publisher","text":"To allow an existing service to publish messages to SNS, simply set the publish field in its manifest. You'll need a name for the topic which describes its function. # manifest.yml for api service name : api type : Backend Service publish : topics : - name : ordersTopic This will create an SNS topic and set a resource policy on the topic to allow SQS queues in your AWS account to create subscriptions. Copilot also injects the ARNs of any SNS topics into your container under the environment variable COPILOT_SNS_TOPIC_ARNS .","title":"Sending Messages from a Publisher"},{"location":"docs/developing/publish-subscribe#javascript-example","text":"Once the publishing service has been deployed, you can send messages to SNS via the AWS SDK for SNS. const { SNSClient , PublishCommand } = require ( \"@aws-sdk/client-sns\" ); const client = new SNSClient ({ region : \"us-west-2\" }); const { ordersTopic } = JSON . parse ( process . env . COPILOT_SNS_TOPIC_ARNS ); const out = await client . send ( new PublishCommand ({ Message : \"hello\" , TopicArn : ordersTopic , }));","title":"Javascript Example"},{"location":"docs/developing/publish-subscribe#subscribing-to-a-topic-with-a-worker-service","text":"To subscribe to an existing SNS topic with a worker service, you'll need to edit the worker service's manifest. Using the subscribe field in the manifest, you can define subscriptions to existing SNS topics exposed by other services in your environment. In this example, we'll use the ordersTopic topic which the api service from the last section exposed. We'll also customize the worker service's queue to enable a dead-letter queue. The tries field tells SQS how many times to try redelivering a failed message before sending it to the DLQ for further inspection. name : orders-worker type : Worker Service subscribe : topics : - name : ordersTopic service : api queue : dead_letter : tries : 5 Copilot will create a subscription between this worker service's queue and the ordersTopic topic from the api service. It will also inject the queue URI into the service container under the environment variable COPILOT_QUEUE_URI .","title":"Subscribing to a topic with a Worker Service"},{"location":"docs/developing/publish-subscribe#javascript-example_1","text":"The central business logic of a worker service's container involves pulling messages from the queue. To do this with the AWS SDK, you can use the SQS Clients for your language of choice. In Javascript, the logic for pulling, processing, and deleting messages from the queue would look like the following code snipped. const { SQSClient , ReceiveMessageCommand , DeleteMessageCommand } = require ( \"@aws-sdk/client-sqs\" ); const client = new SQSClient ({ region : \"us-west-2\" }); const out = await client . send ( new ReceiveMessageCommand ({ QueueUrl : process . env . COPILOT_QUEUE_URI , WaitTimeSeconds : 10 , })); console . log ( `results: ${ JSON . stringify ( out ) } ` ); if ( out . Messages === undefined || out . Messages . length === 0 ) { return ; } // Process the message here. await client . send ( new DeleteMessageCommand ({ QueueUrl : process . env . COPILOT_QUEUE_URI , ReceiptHandle : out . Messages [ 0 ]. ReceiptHandle , }));","title":"Javascript Example"},{"location":"docs/developing/secrets","text":"Secrets Secrets are sensitive bits of information like OAuth tokens, secret keys or API keys - information that you need in your application code, but shouldn't commit to your source code. In the AWS Copilot CLI, secrets are passed in as environment variables (read more about developing with environment variables ), but they're treated differently due to their sensitive nature. How do I add Secrets? Adding secrets requires you to store your secret as a SecureString in AWS Systems Manager Parameter Store (SSM), then add a reference to the SSM parameter to your manifest . You can easily create secrets using copilot secret init ! After creating the secrets, Copilot will tell you what your secrets' names are. You can then use the name to add the reference in your manifest. Alternatively... If you want to bring your own secrets, be sure to add two tags to your secrets - copilot-application: <application from which you want to access the secret> and copilot-environment: <environment from which you want to access the secret>. Copilot requires the copilot-application and copilot-environment tags to limit access to this secret. Suppose you have a (properly tagged!) SSM parameter named GH_WEBHOOK_SECRET with value secretvalue1234 . You can modify your manifest file to pass in this value: secrets : GITHUB_WEBHOOK_SECRET : GH_WEBHOOK_SECRET Once you deploy this updated manifest, your service or job will be able to access the environment variable GITHUB_WEBHOOK_SECRET , which will have the value of the SSM parameter GH_WEBHOOK_SECRET , secretvalue1234 . This works because ECS Agent will resolve the SSM parameter when it starts up your task, and set the environment variable for you. Attention Secrets are not supported for Request-Driven Web Services.","title":"Secrets"},{"location":"docs/developing/secrets#secrets","text":"Secrets are sensitive bits of information like OAuth tokens, secret keys or API keys - information that you need in your application code, but shouldn't commit to your source code. In the AWS Copilot CLI, secrets are passed in as environment variables (read more about developing with environment variables ), but they're treated differently due to their sensitive nature.","title":"Secrets"},{"location":"docs/developing/secrets#how-do-i-add-secrets","text":"Adding secrets requires you to store your secret as a SecureString in AWS Systems Manager Parameter Store (SSM), then add a reference to the SSM parameter to your manifest . You can easily create secrets using copilot secret init ! After creating the secrets, Copilot will tell you what your secrets' names are. You can then use the name to add the reference in your manifest.","title":"How do I add Secrets?"},{"location":"docs/developing/secrets#alternatively","text":"If you want to bring your own secrets, be sure to add two tags to your secrets - copilot-application: <application from which you want to access the secret> and copilot-environment: <environment from which you want to access the secret>. Copilot requires the copilot-application and copilot-environment tags to limit access to this secret. Suppose you have a (properly tagged!) SSM parameter named GH_WEBHOOK_SECRET with value secretvalue1234 . You can modify your manifest file to pass in this value: secrets : GITHUB_WEBHOOK_SECRET : GH_WEBHOOK_SECRET Once you deploy this updated manifest, your service or job will be able to access the environment variable GITHUB_WEBHOOK_SECRET , which will have the value of the SSM parameter GH_WEBHOOK_SECRET , secretvalue1234 . This works because ECS Agent will resolve the SSM parameter when it starts up your task, and set the environment variable for you. Attention Secrets are not supported for Request-Driven Web Services.","title":"Alternatively..."},{"location":"docs/developing/service-discovery","text":"Service Discovery Service Discovery is a way of letting services discover and connect with each other. Typically, services can only talk to each other if they expose a public endpoint - and even then, requests will have to go over the internet. With ECS Service Discovery , each service you create is given a private address and DNS name - meaning each service can talk to another without ever leaving the local network (VPC) and without exposing a public endpoint. How do I use Service Discovery? Service Discovery is enabled for all services set up using the Copilot CLI. We'll show you how to use it by using an example. Imagine we have an app called kudos and two services: api and front-end . Attention Service Discovery is not supported for Request-Driven Web Services. In this example we'll imagine our front-end service is deployed in the test environment, has a public endpoint and wants to call our api service using its service discovery endpoint. // Calling our api service from the front-end service using Service Discovery func ServiceDiscoveryGet ( w http . ResponseWriter , req * http . Request , ps httprouter . Params ) { endpoint := fmt . Sprintf ( \"http://api.%s/some-request\" , os . Getenv ( \"COPILOT_SERVICE_DISCOVERY_ENDPOINT\" )) resp , err := http . Get ( endpoint /* http://api.test.kudos.local/some-request */ ) if err != nil { http . Error ( w , err . Error (), http . StatusInternalServerError ) return } defer resp . Body . Close () body , _ := ioutil . ReadAll ( resp . Body ) w . WriteHeader ( http . StatusOK ) w . Write ( body ) } The important part is that our front-end service is making a request to our api service through a special endpoint: endpoint := fmt . Sprintf ( \"http://api.%s/some-request\" , os . Getenv ( \"COPILOT_SERVICE_DISCOVERY_ENDPOINT\" )) COPILOT_SERVICE_DISCOVERY_ENDPOINT is a special environment variable that the Copilot CLI sets for you when it creates your service. It's of the format {env name}.{app name}.local - so in this case in our kudos app, when deployed in the test environment, the request would be to http://api.test.kudos.local/some-request . Since our api service is running on port 80, we're not specifying the port in the URL. However, if it was running on another port, say 8080, we'd need to include the port in the request, as well http://api.kudos.local:8080/some-request . When our front-end makes this request, the endpoint api.test.kudos.local resolves to a private IP address and is routed privately within your VPC. Legacy Environments and Service Discovery Prior to Copilot v1.9.0, the service discovery namespace used the format {app name}.local , without including the environment. This limitation made it impossible to deploy multiple environments in the same VPC. Any environments created with Copilot v1.9.0 and newer can share a VPC with any other environment. When your environments are upgraded, Copilot will honor the service discovery namespace that the environment was created with. That means that the endpoint your services are reachable at will not change. Any new environments created with Copilot v1.9.0 and above will use the {env name}.{app name}.local format for service discovery, and can share VPCs with older environments.","title":"Service Discovery"},{"location":"docs/developing/service-discovery#service-discovery","text":"Service Discovery is a way of letting services discover and connect with each other. Typically, services can only talk to each other if they expose a public endpoint - and even then, requests will have to go over the internet. With ECS Service Discovery , each service you create is given a private address and DNS name - meaning each service can talk to another without ever leaving the local network (VPC) and without exposing a public endpoint.","title":"Service Discovery"},{"location":"docs/developing/service-discovery#how-do-i-use-service-discovery","text":"Service Discovery is enabled for all services set up using the Copilot CLI. We'll show you how to use it by using an example. Imagine we have an app called kudos and two services: api and front-end . Attention Service Discovery is not supported for Request-Driven Web Services. In this example we'll imagine our front-end service is deployed in the test environment, has a public endpoint and wants to call our api service using its service discovery endpoint. // Calling our api service from the front-end service using Service Discovery func ServiceDiscoveryGet ( w http . ResponseWriter , req * http . Request , ps httprouter . Params ) { endpoint := fmt . Sprintf ( \"http://api.%s/some-request\" , os . Getenv ( \"COPILOT_SERVICE_DISCOVERY_ENDPOINT\" )) resp , err := http . Get ( endpoint /* http://api.test.kudos.local/some-request */ ) if err != nil { http . Error ( w , err . Error (), http . StatusInternalServerError ) return } defer resp . Body . Close () body , _ := ioutil . ReadAll ( resp . Body ) w . WriteHeader ( http . StatusOK ) w . Write ( body ) } The important part is that our front-end service is making a request to our api service through a special endpoint: endpoint := fmt . Sprintf ( \"http://api.%s/some-request\" , os . Getenv ( \"COPILOT_SERVICE_DISCOVERY_ENDPOINT\" )) COPILOT_SERVICE_DISCOVERY_ENDPOINT is a special environment variable that the Copilot CLI sets for you when it creates your service. It's of the format {env name}.{app name}.local - so in this case in our kudos app, when deployed in the test environment, the request would be to http://api.test.kudos.local/some-request . Since our api service is running on port 80, we're not specifying the port in the URL. However, if it was running on another port, say 8080, we'd need to include the port in the request, as well http://api.kudos.local:8080/some-request . When our front-end makes this request, the endpoint api.test.kudos.local resolves to a private IP address and is routed privately within your VPC.","title":"How do I use Service Discovery?"},{"location":"docs/developing/service-discovery#legacy-environments-and-service-discovery","text":"Prior to Copilot v1.9.0, the service discovery namespace used the format {app name}.local , without including the environment. This limitation made it impossible to deploy multiple environments in the same VPC. Any environments created with Copilot v1.9.0 and newer can share a VPC with any other environment. When your environments are upgraded, Copilot will honor the service discovery namespace that the environment was created with. That means that the endpoint your services are reachable at will not change. Any new environments created with Copilot v1.9.0 and above will use the {env name}.{app name}.local format for service discovery, and can share VPCs with older environments.","title":"Legacy Environments and Service Discovery"},{"location":"docs/developing/sidecars","text":"Sidecars Sidecars are additional containers that run along side the main container. They are usually used to perform peripheral tasks such as logging, configuration, or proxying requests. Attention Sidecars are not supported for Request-Driven Web Services. Attention If your main container is using a Windows image, FireLens , AWS X-Ray , and AWS App Mesh are not supported. Please check if your sidecar container supports Windows. AWS also provides some plugin options that can be seamlessly incorporated with your ECS service, including but not limited to FireLens , AWS X-Ray , and AWS App Mesh . If you have defined an EFS volume for your main container through the storage field in the manifest, you can also mount that volume in any sidecar containers you have defined. How to add sidecars with Copilot? There are two ways of adding sidecars using the Copilot manifest: by specifying general sidecars or by using sidecar patterns . General sidecars You'll need to provide the URL for the sidecar image. Optionally, you can specify the port you'd like to expose and the credential parameter for private registry . port Integer Port of the container to expose (optional). image String Image URL for the sidecar container (required). essential Bool Whether the sidecar container is an essential container (optional, default true). credentialsParameter String ARN of the secret containing the private repository credentials (optional). variables Map Environment variables for the sidecar container (optional) secrets Map Secrets to expose to the sidecar container (optional) mount_points Array of Maps Mount paths for EFS volumes specified at the service level (optional). mount_points. source_volume String Source volume to mount in this sidecar (required). mount_points. path String The path inside the sidecar container at which to mount the volume (required). mount_points. read_only Boolean Whether to allow the sidecar read-only access to the volume (default true). labels Map Docker labels to apply to this container (optional). depends_on Map Container dependencies to apply to this container (optional). entrypoint String or Array of Strings Override the default entrypoint in the sidecar. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the sidecar. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] healthcheck Map Optional configuration for sidecar container health checks. healthcheck. command Array of Strings The command to run to determine if the sidecar container is healthy. The string array can start with CMD to execute the command arguments directly, or CMD-SHELL to run the command with the container's default shell. healthcheck. interval Duration Time period between health checks, in seconds. Default is 10s. healthcheck. retries Integer Number of times to retry before container is deemed unhealthy. Default is 2. healthcheck. timeout Duration How long to wait before considering the health check failed, in seconds. Default is 5s. healthcheck. start_period Duration Length of grace period for containers to bootstrap before failed health checks count towards the maximum number of retries. Default is 0s. Example Below is an example of specifying the nginx sidecar container in a load balanced web service manifest. name : api type : Load Balanced Web Service image : build : api/Dockerfile port : 3000 http : path : 'api' healthcheck : '/api/health-check' # Target container for Load Balancer is our sidecar 'nginx', instead of the service container. targetContainer : 'nginx' cpu : 256 memory : 512 count : 1 sidecars : nginx : port : 80 image : 1234567890.dkr.ecr.us-west-2.amazonaws.com/reverse-proxy:revision_1 variables : NGINX_PORT : 80 Below is a fragment of a manifest including an EFS volume in both the service and sidecar container. storage : volumes : myEFSVolume : path : '/etc/mount1' read_only : false efs : id : fs-1234567 sidecars : nginx : port : 80 image : 1234567890.dkr.ecr.us-west-2.amazonaws.com/reverse-proxy:revision_1 variables : NGINX_PORT : 80 mount_points : - source_volume : myEFSVolume path : '/etc/mount1' Sidecar patterns Sidecar patterns are predefined Copilot sidecar configurations. For now, the only supported pattern is FireLens, but we'll add more in the future! # In the manifest. logging : # The Fluent Bit image. (Optional, we'll use \"amazon/aws-for-fluent-bit:latest\" by default) image : <image URL> # The configuration options to send to the FireLens log driver. (Optional) destination : <config key> : <config value> # Whether to include ECS metadata in logs. (Optional, default to true) enableMetadata : <true|false> # Secret to pass to the log configuration. (Optional) secretOptions : <key> : <value> # The full config file path in your custom Fluent Bit image. (Optional) configFilePath : <config file path> # Environment variables for the sidecar container. (Optional) variables : <key> : <value> # Secrets to expose to the sidecar container. (Optional) secrets : <key> : <value> For example: logging : destination : Name : cloudwatch region : us-west-2 log_group_name : /copilot/sidecar-test-hello log_stream_prefix : copilot/ You might need to add necessary permissions to the task role so that FireLens can forward your data. You can add permissions by specifying them in your addons . For example: Resources : FireLensPolicy : Type : AWS::IAM::ManagedPolicy Properties : PolicyDocument : Version : 2012-10-17 Statement : - Effect : Allow Action : - logs:CreateLogStream - logs:CreateLogGroup - logs:DescribeLogStreams - logs:PutLogEvents Resource : \"<resource ARN>\" Outputs : FireLensPolicyArn : Description : An addon ManagedPolicy gets used by the ECS task role Value : !Ref FireLensPolicy Info Since the FireLens log driver can route your main container's logs to various destinations, the svc logs command can track them only when they are sent to the log group we create for your Copilot service in CloudWatch. Info We're going to make this easier and more powerful! Currently, we only support using remote images for sidecars, which means users need to build and push their local sidecar images. But we are planning to support using local images or Dockerfiles. Additionally, FireLens will be able to route logs for the other sidecars (not just the main container).","title":"Sidecars"},{"location":"docs/developing/sidecars#sidecars","text":"Sidecars are additional containers that run along side the main container. They are usually used to perform peripheral tasks such as logging, configuration, or proxying requests. Attention Sidecars are not supported for Request-Driven Web Services. Attention If your main container is using a Windows image, FireLens , AWS X-Ray , and AWS App Mesh are not supported. Please check if your sidecar container supports Windows. AWS also provides some plugin options that can be seamlessly incorporated with your ECS service, including but not limited to FireLens , AWS X-Ray , and AWS App Mesh . If you have defined an EFS volume for your main container through the storage field in the manifest, you can also mount that volume in any sidecar containers you have defined.","title":"Sidecars"},{"location":"docs/developing/sidecars#how-to-add-sidecars-with-copilot","text":"There are two ways of adding sidecars using the Copilot manifest: by specifying general sidecars or by using sidecar patterns .","title":"How to add sidecars with Copilot?"},{"location":"docs/developing/sidecars#general-sidecars","text":"You'll need to provide the URL for the sidecar image. Optionally, you can specify the port you'd like to expose and the credential parameter for private registry . port Integer Port of the container to expose (optional). image String Image URL for the sidecar container (required). essential Bool Whether the sidecar container is an essential container (optional, default true). credentialsParameter String ARN of the secret containing the private repository credentials (optional). variables Map Environment variables for the sidecar container (optional) secrets Map Secrets to expose to the sidecar container (optional) mount_points Array of Maps Mount paths for EFS volumes specified at the service level (optional). mount_points. source_volume String Source volume to mount in this sidecar (required). mount_points. path String The path inside the sidecar container at which to mount the volume (required). mount_points. read_only Boolean Whether to allow the sidecar read-only access to the volume (default true). labels Map Docker labels to apply to this container (optional). depends_on Map Container dependencies to apply to this container (optional). entrypoint String or Array of Strings Override the default entrypoint in the sidecar. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the sidecar. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] healthcheck Map Optional configuration for sidecar container health checks. healthcheck. command Array of Strings The command to run to determine if the sidecar container is healthy. The string array can start with CMD to execute the command arguments directly, or CMD-SHELL to run the command with the container's default shell. healthcheck. interval Duration Time period between health checks, in seconds. Default is 10s. healthcheck. retries Integer Number of times to retry before container is deemed unhealthy. Default is 2. healthcheck. timeout Duration How long to wait before considering the health check failed, in seconds. Default is 5s. healthcheck. start_period Duration Length of grace period for containers to bootstrap before failed health checks count towards the maximum number of retries. Default is 0s.","title":"General sidecars"},{"location":"docs/developing/sidecars#example","text":"Below is an example of specifying the nginx sidecar container in a load balanced web service manifest. name : api type : Load Balanced Web Service image : build : api/Dockerfile port : 3000 http : path : 'api' healthcheck : '/api/health-check' # Target container for Load Balancer is our sidecar 'nginx', instead of the service container. targetContainer : 'nginx' cpu : 256 memory : 512 count : 1 sidecars : nginx : port : 80 image : 1234567890.dkr.ecr.us-west-2.amazonaws.com/reverse-proxy:revision_1 variables : NGINX_PORT : 80 Below is a fragment of a manifest including an EFS volume in both the service and sidecar container. storage : volumes : myEFSVolume : path : '/etc/mount1' read_only : false efs : id : fs-1234567 sidecars : nginx : port : 80 image : 1234567890.dkr.ecr.us-west-2.amazonaws.com/reverse-proxy:revision_1 variables : NGINX_PORT : 80 mount_points : - source_volume : myEFSVolume path : '/etc/mount1'","title":"Example"},{"location":"docs/developing/sidecars#sidecar-patterns","text":"Sidecar patterns are predefined Copilot sidecar configurations. For now, the only supported pattern is FireLens, but we'll add more in the future! # In the manifest. logging : # The Fluent Bit image. (Optional, we'll use \"amazon/aws-for-fluent-bit:latest\" by default) image : <image URL> # The configuration options to send to the FireLens log driver. (Optional) destination : <config key> : <config value> # Whether to include ECS metadata in logs. (Optional, default to true) enableMetadata : <true|false> # Secret to pass to the log configuration. (Optional) secretOptions : <key> : <value> # The full config file path in your custom Fluent Bit image. (Optional) configFilePath : <config file path> # Environment variables for the sidecar container. (Optional) variables : <key> : <value> # Secrets to expose to the sidecar container. (Optional) secrets : <key> : <value> For example: logging : destination : Name : cloudwatch region : us-west-2 log_group_name : /copilot/sidecar-test-hello log_stream_prefix : copilot/ You might need to add necessary permissions to the task role so that FireLens can forward your data. You can add permissions by specifying them in your addons . For example: Resources : FireLensPolicy : Type : AWS::IAM::ManagedPolicy Properties : PolicyDocument : Version : 2012-10-17 Statement : - Effect : Allow Action : - logs:CreateLogStream - logs:CreateLogGroup - logs:DescribeLogStreams - logs:PutLogEvents Resource : \"<resource ARN>\" Outputs : FireLensPolicyArn : Description : An addon ManagedPolicy gets used by the ECS task role Value : !Ref FireLensPolicy Info Since the FireLens log driver can route your main container's logs to various destinations, the svc logs command can track them only when they are sent to the log group we create for your Copilot service in CloudWatch. Info We're going to make this easier and more powerful! Currently, we only support using remote images for sidecars, which means users need to build and push their local sidecar images. But we are planning to support using local images or Dockerfiles. Additionally, FireLens will be able to route logs for the other sidecars (not just the main container).","title":"Sidecar patterns"},{"location":"docs/developing/storage","text":"Storage There are two ways to add persistence to Copilot workloads: using copilot storage init to create databases and S3 buckets; and attaching an existing EFS filesystem using the storage field in the manifest. Database and Artifacts To add a database or S3 bucket to your job or service, simply run copilot storage init . # For a guided experience. $ copilot storage init -t S3 # To create a bucket named \"my-bucket\" accessible by the \"api\" service. $ copilot storage init -n my-bucket -t S3 -w api The above command will create the Cloudformation template for an S3 bucket in the addons directory for the \"api\" service. The next time you run copilot deploy -n api , the bucket will be created, permission to access it will be added to the api task role, and the name of the bucket will be injected into the api container under the environment variable MY_BUCKET_NAME . Info All names are converted into SCREAMING_SNAKE_CASE based on their use of hyphens or underscores. You can view the environment variables for a given service by running copilot svc show . You can also create a DynamoDB table using copilot storage init . For example, to create the Cloudformation template for a table with a sort key and a local secondary index, you could run the following command. # For a guided experience. $ copilot storage init -t DynamoDB # Or skip the prompts by providing flags. $ copilot storage init -n users -t DynamoDB -w api --partition-key id:N --sort-key email:S --lsi post-count:N This will create a DynamoDB table called ${app}-${env}-${svc}-users . Its partition key will be id , a Number attribute; its sort key will be email , a String attribute; and it will have a local secondary index (essentially an alternate sort key) on the Number attribute post-count . It is also possible to create an RDS Aurora Serverless cluster using copilot storage init . # For a guided experience. $ copilot storage init -t Aurora # Or skip the prompts by providing flags. $ copilot storage init -n my-cluster -t Aurora -w api --engine PostgreSQL --initial-db my_db This will create an RDS Aurora Serverless cluster that uses PostgreSQL engine with a database named my_db . An environment variable named MYCLUSTER_SECRET is injected into your workload as a JSON string. The fields are 'host' , 'port' , 'dbname' , 'username' , 'password' , 'dbClusterIdentifier' and 'engine' . File Systems There are two ways to use an EFS file system with Copilot: using managed EFS, and importing your own filesystem. Attention EFS is not supported for Windows-based services. Managed EFS The easiest way to get started using EFS for service- or job-level storage is via Copilot's built-in managed EFS capability. To get started, simply enable the efs key in the manifest under your volume's name. name : frontend storage : volumes : myManagedEFSVolume : efs : true path : /var/efs read_only : false This manifest will result in an EFS volume being created at the environment level, with an Access Point and dedicated directory at the path /frontend in the EFS filesystem created specifically for your service. Your container will be able to access this directory and all its subdirectories at the /var/efs path in its own filesystem. The /frontend directory and EFS filesystem will persist until you delete your environment. The use of an access point for each service ensures that no two services can access each other's data unless you specifically intend for them to do so by specifying the full advanced configuration. You can read more in Advanced Use Cases . You can also customize the UID and GID used for the access point by specifying the uid and gid fields in advanced EFS configuration. If you do not specify a UID or GID, Copilot picks a pseudorandom UID and GID for the access point based on the CRC32 checksum of the service's name. storage : volumes : myManagedEFSVolume : efs : uid : 1000 gid : 10000 path : /var/efs read_only : false uid and gid may not be specified with any other advanced EFS configuration. Under the Hood When you enable managed EFS, Copilot creates the following resources at the environment level: An EFS file system . Mount targets in each of your environment's private subnets Security group rules allowing the Environment Security Group to access the mount targets. At the service level, Copilot creates: An EFS Access Point . The Access Point refers to a directory created by CFN named after the service or job you wish to use EFS with. You can see the environment-level resources created by calling copilot env show --json --resources and parsing the output with your favorite command line JSON processor. For example: > copilot env show -n test --json --resources | jq '.resources[] | select( .type | contains(\"EFS\") )' Advanced Use Cases Hydrating a Managed EFS Volume Sometimes, you may wish to populate the created EFS volume with data before your service begins accepting traffic. There are several ways you can do this, depending on your main container's requirements and whether it requires this data for startup. Using a Sidecar You can mount the created EFS volume in a sidecar using the mount_points field, and use your sidecar's COMMAND or ENTRYPOINT directives to copy data from the sidecar's filesystem or pull data down from S3 or another cloud service. If you mark the sidecar as nonessential with essential:false , it will start, do its work, and exit as the service containers come up and stabilize. This may not be suitable for workloads which depend on the correct data being present in the EFS volume. Using copilot svc exec For workloads where data must be present prior to your task containers coming up, we recommend using a placeholder container first. For example, deploy your service with the following values in the manifest: image : location : amazon/amazon-ecs-sample exec : true storage : volumes : myVolume : efs : true path : /var/efs read_only : false Then, when your service is stable, run: $ copilot svc exec This will open an interactive shell from which you can add packages like curl or wget , download data from the internet, create a directory structure, etc. Info This method of configuring containers is not recommended for production environments; containers are ephemeral and if you wish for a piece of software to be present in your service containers, be sure to add it using the RUN directive in a Dockerfile. When you have populated the directory, modify your manifest to remove the exec directive and update the build field to your desired Docker build config or image location. image : build : ./Dockerfile storage : volumes : myVolume : efs : true path : /var/efs read_only : false External EFS Mounting an externally-managed EFS volume in Copilot tasks requires two things: That you create an EFS file system in the desired environment's region. That you create an EFS Mount Target using the Copilot environment security group in each subnet of your environment. When those prerequisites are satisfied, you can enable EFS storage using simple syntax in your manifest. You'll need the filesystem ID and, if using, the access point configuration for the filesystem. Info You can only use a given EFS file system in a single environment at a time. Mount targets are limited to one per availability zone; therefore, you must delete any existing mount targets before bringing the file system to Copilot if you have used it in another VPC. Manifest Syntax The simplest possible EFS volume can be specified with the following syntax: storage : volumes : myEFSVolume : # This is a variable key and can be set to an arbitrary string. path : '/etc/mount1' efs : id : fs-1234567 This will create a read-only mounted volume in your service's or job's container using the filesystem fs-1234567 . If mount targets are not created in the subnets of the environment, the task will fail to launch. Full syntax for external EFS volumes follows. storage : volumes : <volume name> : path : <mount path> # Required. The path inside the container. read_only : <boolean> # Default: true efs : id : <filesystem ID> # Required. root_dir : <filesystem root> # Optional. Defaults to \"/\". Must not be # specified if using access points. auth : iam : <boolean> # Optional. Whether to use IAM authorization when # mounting this filesystem. access_point_id : <access point ID> # Optional. The ID of the EFS Access Point # to use when mounting this filesystem. uid : <uint32> # Optional. UID for managed EFS access point. gid : <uint32> # Optional. GID for managed EFS access point. Cannot be specified # with `id`, `root_dir`, or `auth`. Creating Mount Targets There are several ways to create mount targets for an existing EFS filesystem: using the AWS CLI and using CloudFormation . With the AWS CLI To create mount targets for an existing filesystem, you'll need the ID of that filesystem. a Copilot environment deployed in the same account and region. To retrieve the filesystem ID, you can use the AWS CLI: $ EFS_FILESYSTEMS = $( aws efs describe-file-systems | \\ jq '.FileSystems[] | {ID: .FileSystemId, CreationTime: .CreationTime, Size: .SizeInBytes.Value}' ) If you echo this variable you should be able to find which filesystem you need. Assign it to the variable $EFS_ID and continue. You'll also need the public subnets of the Copilot environment and the Environment Security Group. This jq command will filter the output of the describe-stacks call down to simply the desired output value. Info The filesystem you use MUST be in the same region as your Copilot environment! $ SUBNETS = $( aws cloudformation describe-stacks --stack-name ${ YOUR_APP } - ${ YOUR_ENV } \\ | jq '.Stacks[] | .Outputs[] | select(.OutputKey == \"PublicSubnets\") | .OutputValue' ) $ SUBNET1 = $( echo $SUBNETS | jq -r 'split(\",\") | .[0]' ) $ SUBNET2 = $( echo $SUBNETS | jq -r 'split(\",\") | .[1]' ) $ ENV_SG = $( aws cloudformation describe-stacks --stack-name ${ YOUR_APP } - ${ YOUR_ENV } \\ | jq -r '.Stacks[] | .Outputs[] | select(.OutputKey == \"EnvironmentSecurityGroup\") | .OutputValue' ) Once you have these, create the mount targets. $ MOUNT_TARGET_1_ID = $( aws efs create-mount-target \\ --subnet-id $SUBNET_1 \\ --security-groups $ENV_SG \\ --file-system-id $EFS_ID | jq -r .MountTargetID ) $ MOUNT_TARGET_2_ID = $( aws efs create-mount-target \\ --subnet-id $SUBNET_2 \\ --security-groups $ENV_SG \\ --file-system-id $EFS_ID | jq -r .MountTargetID ) Once you've done this, you can specify the storage configuration in the manifest as above. Cleanup Delete the mount targets using the AWS CLI. $ aws efs delete-mount-target --mount-target-id $MOUNT_TARGET_1 $ aws efs delete-mount-target --mount-target-id $MOUNT_TARGET_2 CloudFormation Here's an example of how you might create the appropriate EFS infrastructure for an external file system using a CloudFormation stack. After creating an environment, deploy the following CloudFormation template into the same account and region as the environment. Place the following CloudFormation template into a file called efs.yml . Parameters : App : Type : String Description : Your application's name. Env : Type : String Description : The environment name your service, job, or workflow is being deployed to. Resources : EFSFileSystem : Metadata : 'aws:copilot:description' : 'An EFS File System for persistent backing storage for tasks and services' Type : AWS::EFS::FileSystem Properties : PerformanceMode : generalPurpose ThroughputMode : bursting Encrypted : true MountTargetPublicSubnet1 : Type : AWS::EFS::MountTarget Properties : FileSystemId : !Ref EFSFileSystem SecurityGroups : - Fn::ImportValue : !Sub \"${App}-${Env}-EnvironmentSecurityGroup\" SubnetId : !Select - 0 - !Split - \",\" - Fn::ImportValue : !Sub \"${App}-${Env}-PublicSubnets\" MountTargetPublicSubnet2 : Type : AWS::EFS::MountTarget Properties : FileSystemId : !Ref EFSFileSystem SecurityGroups : - Fn::ImportValue : !Sub \"${App}-${Env}-EnvironmentSecurityGroup\" SubnetId : !Select - 1 - !Split - \",\" - Fn::ImportValue : !Sub \"${App}-${Env}-PublicSubnets\" Outputs : EFSVolumeID : Value : !Ref EFSFileSystem Export : Name : !Sub ${App}-${Env}-FilesystemID Then run: $ aws cloudformation deploy --stack-name efs-cfn \\ --template-file ecs.yml --parameter-overrides App = ${ YOUR_APP } Env = ${ YOUR_ENV } This will create an EFS file system and the mount targets your tasks need using outputs from the Copilot environment stack. To get the EFS filesystem ID, you can run a describe-stacks call: $ aws cloudformation describe-stacks --stack-name efs-cfn | \\ jq -r '.Stacks[] | .Outputs[] | .OutputValue' Then, in the manifest of the service which you would like to have access to the EFS filesystem, add the following configuration. storage : volumes : copilotVolume : # This is a variable key and can be set to arbitrary strings. path : '/etc/mount1' read_only : true # Set to false if your service needs write access. efs : id : <your filesystem ID> Finally, run copilot svc deploy to reconfigure your service to mount the filesystem at /etc/mount1 . Cleanup To clean this up, remove the storage configuration from the manifest and redeploy the service: $ copilot svc deploy Then, delete the stack. $ aws cloudformation delete-stack --stack-name efs-cfn","title":"Storage"},{"location":"docs/developing/storage#storage","text":"There are two ways to add persistence to Copilot workloads: using copilot storage init to create databases and S3 buckets; and attaching an existing EFS filesystem using the storage field in the manifest.","title":"Storage"},{"location":"docs/developing/storage#database-and-artifacts","text":"To add a database or S3 bucket to your job or service, simply run copilot storage init . # For a guided experience. $ copilot storage init -t S3 # To create a bucket named \"my-bucket\" accessible by the \"api\" service. $ copilot storage init -n my-bucket -t S3 -w api The above command will create the Cloudformation template for an S3 bucket in the addons directory for the \"api\" service. The next time you run copilot deploy -n api , the bucket will be created, permission to access it will be added to the api task role, and the name of the bucket will be injected into the api container under the environment variable MY_BUCKET_NAME . Info All names are converted into SCREAMING_SNAKE_CASE based on their use of hyphens or underscores. You can view the environment variables for a given service by running copilot svc show . You can also create a DynamoDB table using copilot storage init . For example, to create the Cloudformation template for a table with a sort key and a local secondary index, you could run the following command. # For a guided experience. $ copilot storage init -t DynamoDB # Or skip the prompts by providing flags. $ copilot storage init -n users -t DynamoDB -w api --partition-key id:N --sort-key email:S --lsi post-count:N This will create a DynamoDB table called ${app}-${env}-${svc}-users . Its partition key will be id , a Number attribute; its sort key will be email , a String attribute; and it will have a local secondary index (essentially an alternate sort key) on the Number attribute post-count . It is also possible to create an RDS Aurora Serverless cluster using copilot storage init . # For a guided experience. $ copilot storage init -t Aurora # Or skip the prompts by providing flags. $ copilot storage init -n my-cluster -t Aurora -w api --engine PostgreSQL --initial-db my_db This will create an RDS Aurora Serverless cluster that uses PostgreSQL engine with a database named my_db . An environment variable named MYCLUSTER_SECRET is injected into your workload as a JSON string. The fields are 'host' , 'port' , 'dbname' , 'username' , 'password' , 'dbClusterIdentifier' and 'engine' .","title":"Database and Artifacts"},{"location":"docs/developing/storage#file-systems","text":"There are two ways to use an EFS file system with Copilot: using managed EFS, and importing your own filesystem. Attention EFS is not supported for Windows-based services.","title":"File Systems"},{"location":"docs/developing/storage#managed-efs","text":"The easiest way to get started using EFS for service- or job-level storage is via Copilot's built-in managed EFS capability. To get started, simply enable the efs key in the manifest under your volume's name. name : frontend storage : volumes : myManagedEFSVolume : efs : true path : /var/efs read_only : false This manifest will result in an EFS volume being created at the environment level, with an Access Point and dedicated directory at the path /frontend in the EFS filesystem created specifically for your service. Your container will be able to access this directory and all its subdirectories at the /var/efs path in its own filesystem. The /frontend directory and EFS filesystem will persist until you delete your environment. The use of an access point for each service ensures that no two services can access each other's data unless you specifically intend for them to do so by specifying the full advanced configuration. You can read more in Advanced Use Cases . You can also customize the UID and GID used for the access point by specifying the uid and gid fields in advanced EFS configuration. If you do not specify a UID or GID, Copilot picks a pseudorandom UID and GID for the access point based on the CRC32 checksum of the service's name. storage : volumes : myManagedEFSVolume : efs : uid : 1000 gid : 10000 path : /var/efs read_only : false uid and gid may not be specified with any other advanced EFS configuration.","title":"Managed EFS"},{"location":"docs/developing/storage#under-the-hood","text":"When you enable managed EFS, Copilot creates the following resources at the environment level: An EFS file system . Mount targets in each of your environment's private subnets Security group rules allowing the Environment Security Group to access the mount targets. At the service level, Copilot creates: An EFS Access Point . The Access Point refers to a directory created by CFN named after the service or job you wish to use EFS with. You can see the environment-level resources created by calling copilot env show --json --resources and parsing the output with your favorite command line JSON processor. For example: > copilot env show -n test --json --resources | jq '.resources[] | select( .type | contains(\"EFS\") )'","title":"Under the Hood"},{"location":"docs/developing/storage#advanced-use-cases","text":"","title":"Advanced Use Cases"},{"location":"docs/developing/storage#hydrating-a-managed-efs-volume","text":"Sometimes, you may wish to populate the created EFS volume with data before your service begins accepting traffic. There are several ways you can do this, depending on your main container's requirements and whether it requires this data for startup.","title":"Hydrating a Managed EFS Volume"},{"location":"docs/developing/storage#using-a-sidecar","text":"You can mount the created EFS volume in a sidecar using the mount_points field, and use your sidecar's COMMAND or ENTRYPOINT directives to copy data from the sidecar's filesystem or pull data down from S3 or another cloud service. If you mark the sidecar as nonessential with essential:false , it will start, do its work, and exit as the service containers come up and stabilize. This may not be suitable for workloads which depend on the correct data being present in the EFS volume.","title":"Using a Sidecar"},{"location":"docs/developing/storage#using-copilot-svc-exec","text":"For workloads where data must be present prior to your task containers coming up, we recommend using a placeholder container first. For example, deploy your service with the following values in the manifest: image : location : amazon/amazon-ecs-sample exec : true storage : volumes : myVolume : efs : true path : /var/efs read_only : false Then, when your service is stable, run: $ copilot svc exec This will open an interactive shell from which you can add packages like curl or wget , download data from the internet, create a directory structure, etc. Info This method of configuring containers is not recommended for production environments; containers are ephemeral and if you wish for a piece of software to be present in your service containers, be sure to add it using the RUN directive in a Dockerfile. When you have populated the directory, modify your manifest to remove the exec directive and update the build field to your desired Docker build config or image location. image : build : ./Dockerfile storage : volumes : myVolume : efs : true path : /var/efs read_only : false","title":"Using copilot svc exec"},{"location":"docs/developing/storage#external-efs","text":"Mounting an externally-managed EFS volume in Copilot tasks requires two things: That you create an EFS file system in the desired environment's region. That you create an EFS Mount Target using the Copilot environment security group in each subnet of your environment. When those prerequisites are satisfied, you can enable EFS storage using simple syntax in your manifest. You'll need the filesystem ID and, if using, the access point configuration for the filesystem. Info You can only use a given EFS file system in a single environment at a time. Mount targets are limited to one per availability zone; therefore, you must delete any existing mount targets before bringing the file system to Copilot if you have used it in another VPC.","title":"External EFS"},{"location":"docs/developing/storage#manifest-syntax","text":"The simplest possible EFS volume can be specified with the following syntax: storage : volumes : myEFSVolume : # This is a variable key and can be set to an arbitrary string. path : '/etc/mount1' efs : id : fs-1234567 This will create a read-only mounted volume in your service's or job's container using the filesystem fs-1234567 . If mount targets are not created in the subnets of the environment, the task will fail to launch. Full syntax for external EFS volumes follows. storage : volumes : <volume name> : path : <mount path> # Required. The path inside the container. read_only : <boolean> # Default: true efs : id : <filesystem ID> # Required. root_dir : <filesystem root> # Optional. Defaults to \"/\". Must not be # specified if using access points. auth : iam : <boolean> # Optional. Whether to use IAM authorization when # mounting this filesystem. access_point_id : <access point ID> # Optional. The ID of the EFS Access Point # to use when mounting this filesystem. uid : <uint32> # Optional. UID for managed EFS access point. gid : <uint32> # Optional. GID for managed EFS access point. Cannot be specified # with `id`, `root_dir`, or `auth`.","title":"Manifest Syntax"},{"location":"docs/developing/storage#creating-mount-targets","text":"There are several ways to create mount targets for an existing EFS filesystem: using the AWS CLI and using CloudFormation .","title":"Creating Mount Targets"},{"location":"docs/developing/storage#with-the-aws-cli","text":"To create mount targets for an existing filesystem, you'll need the ID of that filesystem. a Copilot environment deployed in the same account and region. To retrieve the filesystem ID, you can use the AWS CLI: $ EFS_FILESYSTEMS = $( aws efs describe-file-systems | \\ jq '.FileSystems[] | {ID: .FileSystemId, CreationTime: .CreationTime, Size: .SizeInBytes.Value}' ) If you echo this variable you should be able to find which filesystem you need. Assign it to the variable $EFS_ID and continue. You'll also need the public subnets of the Copilot environment and the Environment Security Group. This jq command will filter the output of the describe-stacks call down to simply the desired output value. Info The filesystem you use MUST be in the same region as your Copilot environment! $ SUBNETS = $( aws cloudformation describe-stacks --stack-name ${ YOUR_APP } - ${ YOUR_ENV } \\ | jq '.Stacks[] | .Outputs[] | select(.OutputKey == \"PublicSubnets\") | .OutputValue' ) $ SUBNET1 = $( echo $SUBNETS | jq -r 'split(\",\") | .[0]' ) $ SUBNET2 = $( echo $SUBNETS | jq -r 'split(\",\") | .[1]' ) $ ENV_SG = $( aws cloudformation describe-stacks --stack-name ${ YOUR_APP } - ${ YOUR_ENV } \\ | jq -r '.Stacks[] | .Outputs[] | select(.OutputKey == \"EnvironmentSecurityGroup\") | .OutputValue' ) Once you have these, create the mount targets. $ MOUNT_TARGET_1_ID = $( aws efs create-mount-target \\ --subnet-id $SUBNET_1 \\ --security-groups $ENV_SG \\ --file-system-id $EFS_ID | jq -r .MountTargetID ) $ MOUNT_TARGET_2_ID = $( aws efs create-mount-target \\ --subnet-id $SUBNET_2 \\ --security-groups $ENV_SG \\ --file-system-id $EFS_ID | jq -r .MountTargetID ) Once you've done this, you can specify the storage configuration in the manifest as above.","title":"With the AWS CLI"},{"location":"docs/developing/storage#cleanup","text":"Delete the mount targets using the AWS CLI. $ aws efs delete-mount-target --mount-target-id $MOUNT_TARGET_1 $ aws efs delete-mount-target --mount-target-id $MOUNT_TARGET_2","title":"Cleanup"},{"location":"docs/developing/storage#cloudformation","text":"Here's an example of how you might create the appropriate EFS infrastructure for an external file system using a CloudFormation stack. After creating an environment, deploy the following CloudFormation template into the same account and region as the environment. Place the following CloudFormation template into a file called efs.yml . Parameters : App : Type : String Description : Your application's name. Env : Type : String Description : The environment name your service, job, or workflow is being deployed to. Resources : EFSFileSystem : Metadata : 'aws:copilot:description' : 'An EFS File System for persistent backing storage for tasks and services' Type : AWS::EFS::FileSystem Properties : PerformanceMode : generalPurpose ThroughputMode : bursting Encrypted : true MountTargetPublicSubnet1 : Type : AWS::EFS::MountTarget Properties : FileSystemId : !Ref EFSFileSystem SecurityGroups : - Fn::ImportValue : !Sub \"${App}-${Env}-EnvironmentSecurityGroup\" SubnetId : !Select - 0 - !Split - \",\" - Fn::ImportValue : !Sub \"${App}-${Env}-PublicSubnets\" MountTargetPublicSubnet2 : Type : AWS::EFS::MountTarget Properties : FileSystemId : !Ref EFSFileSystem SecurityGroups : - Fn::ImportValue : !Sub \"${App}-${Env}-EnvironmentSecurityGroup\" SubnetId : !Select - 1 - !Split - \",\" - Fn::ImportValue : !Sub \"${App}-${Env}-PublicSubnets\" Outputs : EFSVolumeID : Value : !Ref EFSFileSystem Export : Name : !Sub ${App}-${Env}-FilesystemID Then run: $ aws cloudformation deploy --stack-name efs-cfn \\ --template-file ecs.yml --parameter-overrides App = ${ YOUR_APP } Env = ${ YOUR_ENV } This will create an EFS file system and the mount targets your tasks need using outputs from the Copilot environment stack. To get the EFS filesystem ID, you can run a describe-stacks call: $ aws cloudformation describe-stacks --stack-name efs-cfn | \\ jq -r '.Stacks[] | .Outputs[] | .OutputValue' Then, in the manifest of the service which you would like to have access to the EFS filesystem, add the following configuration. storage : volumes : copilotVolume : # This is a variable key and can be set to arbitrary strings. path : '/etc/mount1' read_only : true # Set to false if your service needs write access. efs : id : <your filesystem ID> Finally, run copilot svc deploy to reconfigure your service to mount the filesystem at /etc/mount1 .","title":"CloudFormation"},{"location":"docs/developing/storage#cleanup_1","text":"To clean this up, remove the storage configuration from the manifest and redeploy the service: $ copilot svc deploy Then, delete the stack. $ aws cloudformation delete-stack --stack-name efs-cfn","title":"Cleanup"},{"location":"docs/developing/taskdef-overrides","text":"Task Definition Overrides Attention Task Definition overrides is an advanced use case. Overriding a field might cause the task not able to launch. Please use with caution! Copilot generates CloudFormation templates using configuration specified in the manifest . However, there are fields that are not configurable in the manifest. For example, You might want to configure the Ulimits for your workload container, but it is not exposed in our manifest. You can configure additional ECS Task Definition settings by specifying taskdef_overrides rules, which will be applied to the CloudFormation template that Copilot generates out of the manifest. How to specify override rules? For each override rule, you need to specify a path of the CloudFormation resource field you want to override, and a value of that field. The following is an example valid taskdef_overrides field that can be applied to a manifest file: taskdef_overrides : - path : ContainerDefinitions[0].Cpu value : 512 - path : ContainerDefinitions[0].Memory value : 1024 Each rule is applied sequentially to the CloudFormation template. The resulting CloudFormation template becomes the target of the next rule. Evaluation continues until all rules are successfully applied or an error is encountered. Path Evaluation The path field is a '.' character separated path to a target Task Definition field under Properties in CloudFormation . Copilot recursively inserts fields if they don't exist in the CloudFormation template. For example: if a rule has the path A.B[-].C ( B and C don't exist), Copilot will insert the field B and C . A concrete example can be found below . If the target path specifies a member that already exists, that member's value is replaced. To append a new member to a list field such as Ulimits you can use the special character - : Ulimits[-] . Attention The following fields in the task definition are not allowed to be modified. Family ContainerDefinitions[ ].Name Testing In order to ensure that your override rules behave as expected, we recommend running copilot svc package or copilot job package to preview the generated CloudFormation template. Examples Add Ulimits to the main container taskdef_overrides : - path : ContainerDefinitions[0].Ulimits[-] value : Name : \"cpu\" SoftLimit : 1024 HardLimit : 2048 Expose an extra UDP port taskdef_overrides : - path : \"ContainerDefinitions[0].PortMappings[-].ContainerPort\" value : 2056 // PortMappings[1] gets the port mapping added by the previous rule, since by default Copilot creates a port mapping. - path : \"ContainerDefinitions[0].PortMappings[1].Protocol\" value : \"udp\" Give read-only access to the root file system taskdef_overrides : - path : \"ContainerDefinitions[0].ReadonlyRootFilesystem\" value : true","title":"Task Definition Overrides"},{"location":"docs/developing/taskdef-overrides#task-definition-overrides","text":"Attention Task Definition overrides is an advanced use case. Overriding a field might cause the task not able to launch. Please use with caution! Copilot generates CloudFormation templates using configuration specified in the manifest . However, there are fields that are not configurable in the manifest. For example, You might want to configure the Ulimits for your workload container, but it is not exposed in our manifest. You can configure additional ECS Task Definition settings by specifying taskdef_overrides rules, which will be applied to the CloudFormation template that Copilot generates out of the manifest.","title":"Task Definition Overrides"},{"location":"docs/developing/taskdef-overrides#how-to-specify-override-rules","text":"For each override rule, you need to specify a path of the CloudFormation resource field you want to override, and a value of that field. The following is an example valid taskdef_overrides field that can be applied to a manifest file: taskdef_overrides : - path : ContainerDefinitions[0].Cpu value : 512 - path : ContainerDefinitions[0].Memory value : 1024 Each rule is applied sequentially to the CloudFormation template. The resulting CloudFormation template becomes the target of the next rule. Evaluation continues until all rules are successfully applied or an error is encountered.","title":"How to specify override rules?"},{"location":"docs/developing/taskdef-overrides#path-evaluation","text":"The path field is a '.' character separated path to a target Task Definition field under Properties in CloudFormation . Copilot recursively inserts fields if they don't exist in the CloudFormation template. For example: if a rule has the path A.B[-].C ( B and C don't exist), Copilot will insert the field B and C . A concrete example can be found below . If the target path specifies a member that already exists, that member's value is replaced. To append a new member to a list field such as Ulimits you can use the special character - : Ulimits[-] . Attention The following fields in the task definition are not allowed to be modified. Family ContainerDefinitions[ ].Name","title":"Path Evaluation"},{"location":"docs/developing/taskdef-overrides#testing","text":"In order to ensure that your override rules behave as expected, we recommend running copilot svc package or copilot job package to preview the generated CloudFormation template.","title":"Testing"},{"location":"docs/developing/taskdef-overrides#examples","text":"","title":"Examples"},{"location":"docs/developing/taskdef-overrides#add-ulimits-to-the-main-container","text":"taskdef_overrides : - path : ContainerDefinitions[0].Ulimits[-] value : Name : \"cpu\" SoftLimit : 1024 HardLimit : 2048","title":"Add Ulimits to the main container"},{"location":"docs/developing/taskdef-overrides#expose-an-extra-udp-port","text":"taskdef_overrides : - path : \"ContainerDefinitions[0].PortMappings[-].ContainerPort\" value : 2056 // PortMappings[1] gets the port mapping added by the previous rule, since by default Copilot creates a port mapping. - path : \"ContainerDefinitions[0].PortMappings[1].Protocol\" value : \"udp\"","title":"Expose an extra UDP port"},{"location":"docs/developing/taskdef-overrides#give-read-only-access-to-the-root-file-system","text":"taskdef_overrides : - path : \"ContainerDefinitions[0].ReadonlyRootFilesystem\" value : true","title":"Give read-only access to the root file system"},{"location":"docs/getting-started/first-app-tutorial","text":"AWS Copilot makes it easy to deploy your containers to AWS in just a few steps. In this tutorial we\u2019re going to do just that - we\u2019re going to deploy a sample front end service that you can visit in your browser. While we\u2019ll be using a sample static website in this example, you can use AWS Copilot to build and deploy any container app with a Dockerfile. After we get your service all set up, we\u2019ll show you how to delete the resources Copilot created to avoid charges. Sound fun? Let\u2019s do it! Step 1: Download & Configure AWS Copilot You\u2019ll need a few things to use AWS Copilot - the AWS Copilot binary, AWS CLI, Docker Desktop and AWS credentials. Follow our instructions here on how to set up and configure all these tools. Make sure that you have a default profile! Run aws configure to set one up! Step 2: Download some code to deploy In this example, we\u2019ll be using a sample app that\u2019s just a simple static website - but if you already have something you\u2019d like to deploy, just open your terminal and cd into your Dockerfile\u2019s directory. Otherwise you can just clone our sample repository. In your terminal, copy and paste this code. This will clone our sample app and change directories to it. $ git clone https://github.com/aws-samples/aws-copilot-sample-service example $ cd example Step 3: Set up our app Now this is where the fun starts! We have our service code and our Dockerfile and we want to get it deployed to AWS. Let\u2019s have AWS Copilot help us do just that! From within your code directory run: $ copilot init Step 4: Answer a few questions The next thing we\u2019re going to do is answer a few questions from Copilot. Copilot will use these questions to help us choose the best AWS infrastructure for your service. There\u2019s only a few so let\u2019s go through them: \u201cWhat would you like to name your application\u201d - an application is a collection of services. In this example we\u2019ll only have one service in our app, but if you wanted to have a multi-service app, Copilot makes that easy. Let\u2019s call this app example-app . \u201cWhich service type best represents your service's architecture?\u201d - Copilot is asking us what we want our service to do - do we want it to service traffic? Do we want it to be a private backend service? For us, we want our app to be accessible from the web, so let's hit enter and select Load Balanced Web Service . \u201cWhat do you want to name this Load Balanced Web Service?\u201d - now what should we call our service in our app? Be as creative as you want - but I recommend naming this service front-end . \u201cWhich Dockerfile would you like to use for front-end?\u201d - go ahead and choose the default Dockerfile here. This is the service that Copilot will build and deploy for you. Once you choose your Dockerfile, Copilot will start setting up the AWS infrastructure to manage your service. Step 5: Deploy your service Once Copilot finishes setting up the infrastructure to manage your app, you\u2019ll be asked if you want to deploy your service to a test environment type yes. Now we can wait a few minutes \u23f3 while Copilot sets up all the resources needed to run your service. After all the infrastructure for your service is set up, Copilot will build your image and push it to Amazon ECR, and start deploying to Amazon ECS. After your deployment completes your service will be up and running on AWS Fargate and Copilot will print a link to the URL \ud83c\udf89! Step 6: Clean up Now that you've deployed your service, let's go ahead and run copilot app delete - this will delete all the resources Copilot set up for your application, including your ECS Service and the ECR Repository. To delete everything run: $ copilot app delete Congratulations! Congratulations! You have learned how to set up, deploy, and delete your container application to Amazon ECS using AWS Copilot. AWS Copilot is a command line tool that helps you develop, release and operate your container apps on AWS. We hope you had fun deploying your app. Ready to dive deeper into AWS Copilot and learn how to build and manage production ready container apps on AWS? Check out the Developing section in the sidebar.","title":"Deploy your first application"},{"location":"docs/getting-started/first-app-tutorial#step-1-download-configure-aws-copilot","text":"You\u2019ll need a few things to use AWS Copilot - the AWS Copilot binary, AWS CLI, Docker Desktop and AWS credentials. Follow our instructions here on how to set up and configure all these tools. Make sure that you have a default profile! Run aws configure to set one up!","title":"Step 1: Download &amp; Configure AWS Copilot"},{"location":"docs/getting-started/first-app-tutorial#step-2-download-some-code-to-deploy","text":"In this example, we\u2019ll be using a sample app that\u2019s just a simple static website - but if you already have something you\u2019d like to deploy, just open your terminal and cd into your Dockerfile\u2019s directory. Otherwise you can just clone our sample repository. In your terminal, copy and paste this code. This will clone our sample app and change directories to it. $ git clone https://github.com/aws-samples/aws-copilot-sample-service example $ cd example","title":"Step 2: Download some code to deploy"},{"location":"docs/getting-started/first-app-tutorial#step-3-set-up-our-app","text":"Now this is where the fun starts! We have our service code and our Dockerfile and we want to get it deployed to AWS. Let\u2019s have AWS Copilot help us do just that! From within your code directory run: $ copilot init","title":"Step 3: Set up our app"},{"location":"docs/getting-started/first-app-tutorial#step-4-answer-a-few-questions","text":"The next thing we\u2019re going to do is answer a few questions from Copilot. Copilot will use these questions to help us choose the best AWS infrastructure for your service. There\u2019s only a few so let\u2019s go through them: \u201cWhat would you like to name your application\u201d - an application is a collection of services. In this example we\u2019ll only have one service in our app, but if you wanted to have a multi-service app, Copilot makes that easy. Let\u2019s call this app example-app . \u201cWhich service type best represents your service's architecture?\u201d - Copilot is asking us what we want our service to do - do we want it to service traffic? Do we want it to be a private backend service? For us, we want our app to be accessible from the web, so let's hit enter and select Load Balanced Web Service . \u201cWhat do you want to name this Load Balanced Web Service?\u201d - now what should we call our service in our app? Be as creative as you want - but I recommend naming this service front-end . \u201cWhich Dockerfile would you like to use for front-end?\u201d - go ahead and choose the default Dockerfile here. This is the service that Copilot will build and deploy for you. Once you choose your Dockerfile, Copilot will start setting up the AWS infrastructure to manage your service.","title":"Step 4: Answer a few questions"},{"location":"docs/getting-started/first-app-tutorial#step-5-deploy-your-service","text":"Once Copilot finishes setting up the infrastructure to manage your app, you\u2019ll be asked if you want to deploy your service to a test environment type yes. Now we can wait a few minutes \u23f3 while Copilot sets up all the resources needed to run your service. After all the infrastructure for your service is set up, Copilot will build your image and push it to Amazon ECR, and start deploying to Amazon ECS. After your deployment completes your service will be up and running on AWS Fargate and Copilot will print a link to the URL \ud83c\udf89!","title":"Step 5: Deploy your service"},{"location":"docs/getting-started/first-app-tutorial#step-6-clean-up","text":"Now that you've deployed your service, let's go ahead and run copilot app delete - this will delete all the resources Copilot set up for your application, including your ECS Service and the ECR Repository. To delete everything run: $ copilot app delete","title":"Step 6: Clean up"},{"location":"docs/getting-started/first-app-tutorial#congratulations","text":"Congratulations! You have learned how to set up, deploy, and delete your container application to Amazon ECS using AWS Copilot. AWS Copilot is a command line tool that helps you develop, release and operate your container apps on AWS. We hope you had fun deploying your app. Ready to dive deeper into AWS Copilot and learn how to build and manage production ready container apps on AWS? Check out the Developing section in the sidebar.","title":"Congratulations!"},{"location":"docs/getting-started/install","text":"You can install AWS Copilot through Homebrew or by downloading the binaries directly. Homebrew \ud83c\udf7b brew install aws/tap/copilot-cli Manually Copy and paste the command into your terminal. macOS Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-darwin && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Linux x86 (64-bit) Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Linux (ARM) Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux-arm64 && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Windows Command to install Invoke-WebRequest -OutFile 'C:\\Program Files\\copilot.exe' https://github.com/aws/copilot-cli/releases/latest/download/copilot-windows.exe Tip Please use the Windows Terminal to have the best user experience. If you encounter permissions issues, ensure that you are running your terminal as an administrator. Info To download a specific version, replace \"latest\" with the specific version. For example, to download v0.6.0 on macOS, type: curl -Lo copilot https://github.com/aws/copilot-cli/releases/download/v0.6.0/copilot-darwin && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help","title":"Install Copilot"},{"location":"docs/getting-started/install#homebrew","text":"brew install aws/tap/copilot-cli","title":"Homebrew \ud83c\udf7b"},{"location":"docs/getting-started/install#manually","text":"Copy and paste the command into your terminal. macOS Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-darwin && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Linux x86 (64-bit) Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Linux (ARM) Command to install curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux-arm64 && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help Windows Command to install Invoke-WebRequest -OutFile 'C:\\Program Files\\copilot.exe' https://github.com/aws/copilot-cli/releases/latest/download/copilot-windows.exe Tip Please use the Windows Terminal to have the best user experience. If you encounter permissions issues, ensure that you are running your terminal as an administrator. Info To download a specific version, replace \"latest\" with the specific version. For example, to download v0.6.0 on macOS, type: curl -Lo copilot https://github.com/aws/copilot-cli/releases/download/v0.6.0/copilot-darwin && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help","title":"Manually"},{"location":"docs/getting-started/verify","text":"The AWS Copilot CLI executables are cryptographically signed using PGP signatures. The PGP signatures can be used to verify the validity of the AWS Copilot CLI executable. Use the following steps to verify the signatures using the GnuPG tool. Download and install GnuPG. For more information, see the GnuPG website . For macOS, we recommend using Homebrew. Install Homebrew using the instructions from their website. For more information, see Homebrew . After Homebrew is installed, use the following command from your macOS terminal. brew install gnupg For Linux systems, install gpg using the package manager on your flavor of Linux. For Windows systems, download and use the Windows simple installer from the GnuPG website. For more information, see GnuPG Download . Retrieve the Amazon ECS PGP public key. You can use a command to do this or manually create the key and then import it. Option 1: Retrieve the key with the following command. gpg --keyserver hkp://keys.gnupg.net --recv BCE9D9A42D51784F Option 2: Create a file with the following contents of the Amazon ECS PGP public key and then import it. -----BEGIN PGP PUBLIC KEY BLOCK----- Version: GnuPG v2 mQINBFq1SasBEADliGcT1NVJ1ydfN8DqebYYe9ne3dt6jqKFmKowLmm6LLGJe7HU jGtqhCWRDkN+qPpHqdArRgDZAtn2pXY5fEipHgar4CP8QgRnRMO2fl74lmavr4Vg 7K/KH8VHlq2uRw32/B94XLEgRbGTMdWFdKuxoPCttBQaMj3LGn6Pe+6xVWRkChQu BoQAhjBQ+bEm0kNy0LjNgjNlnL3UMAG56t8E3LANIgGgEnpNsB1UwfWluPoGZoTx N+6pHBJrKIL/1v/ETU4FXpYw2zvhWNahxeNRnoYj3uycHkeliCrw4kj0+skizBgO 2K7oVX8Oc3j5+ZilhL/qDLXmUCb2az5cMM1mOoF8EKX5HaNuq1KfwJxqXE6NNIcO lFTrT7QwD5fMNld3FanLgv/ZnIrsSaqJOL6zRSq8O4LN1OWBVbndExk2Kr+5kFxn 5lBPgfPgRj5hQ+KTHMa9Y8Z7yUc64BJiN6F9Nl7FJuSsfqbdkvRLsQRbcBG9qxX3 rJAEhieJzVMEUNl+EgeCkxj5xuSkNU7zw2c3hQZqEcrADLV+hvFJktOz9Gm6xzbq lTnWWCz4xrIWtuEBA2qE+MlDheVd78a3gIsEaSTfQq0osYXaQbvlnSWOoc1y/5Zb zizHTJIhLtUyls9WisP2s0emeHZicVMfW61EgPrJAiupgc7kyZvFt4YwfwARAQAB tCRBbWF6b24gRUNTIDxlY3Mtc2VjdXJpdHlAYW1hem9uLmNvbT6JAhwEEAECAAYF AlrjL0YACgkQHivRXs0TaQrg1g/+JppwPqHnlVPmv7lessB8I5UqZeD6p6uVpHd7 Bs3pcPp8BV7BdRbs3sPLt5bV1+rkqOlw+0gZ4Q/ue/YbWtOAt4qY0OcEo0HgcnaX lsB827QIfZIVtGWMhuh94xzm/SJkvngml6KB3YJNnWP61A9qJ37/VbVVLzvcmazA McWB4HUMNrhd0JgBCo0gIpqCbpJEvUc02Bjn23eEJsS9kC7OUAHyQkVnx4d9UzXF 4OoISF6hmQKIBoLnRrAlj5Qvs3GhvHQ0ThYq0Grk/KMJJX2CSqt7tWJ8gk1n3H3Y SReRXJRnv7DsDDBwFgT6r5Q2HW1TBUvaoZy5hF6maD09nHcNnvBjqADzeT8Tr/Qu bBCLzkNSYqqkpgtwv7seoD2P4n1giRvDAOEfMZpVkUr+C252IaH1HZFEz+TvBVQM Y8OWWxmIJW+J6evjo3N1eO19UHv71jvoF8zljbI4bsL2c+QTJmOv7nRqzDQgCWyp Id/v2dUVVTk1j9omuLBBwNJzQCB+72LcIzJhYmaP1HC4LcKQG+/f41exuItenatK lEJQhYtyVXcBlh6Yn/wzNg2NWOwb3vqY/F7m6u9ixAwgtIMgPCDE4aJ86zrrXYFz N2HqkTSQh77Z8KPKmyGopsmN/reMuilPdINb249nA0dzoN+nj+tTFOYCIaLaFyjs Z0r1QAOJAjkEEwECACMFAlq1SasCGwMHCwkIBwMCAQYVCAIJCgsEFgIDAQIeAQIX gAAKCRC86dmkLVF4T9iFEACEnkm1dNXsWUx34R3c0vamHrPxvfkyI1FlEUen8D1h uX9xy6jCEROHWEp0rjGK4QDPgM93sWJ+s1UAKg214QRVzft0y9/DdR+twApA0fzy uavIthGd6+03jAAo6udYDE+cZC3P7XBbDiYEWk4XAF9I1JjB8hTZUgvXBL046JhG eM17+crgUyQeetkiOQemLbsbXQ40Bd9V7zf7XJraFd8VrwNUwNb+9KFtgAsc9rk+ YIT/PEf+YOPysgcxI4sTWghtyCulVnuGoskgDv4v73PALU0ieUrvvQVqWMRvhVx1 0X90J7cC1KOyhlEQQ1aFTgmQjmXexVTwIBm8LvysFK6YXM41KjOrlz3+6xBIm/qe bFyLUnf4WoiuOplAaJhK9pRY+XEnGNxdtN4D26Kd0F+PLkm3Tr3Hy3b1Ok34FlGr KVHUq1TZD7cvMnnNKEELTUcKX+1mV3an16nmAg/my1JSUt6BNK2rJpY1s/kkSGSE XQ4zuF2IGCpvBFhYAlt5Un5zwqkwwQR3/n2kwAoDzonJcehDw/C/cGos5D0aIU7I K2X2aTD3+pA7Mx3IMe2hqmYqRt9X42yF1PIEVRneBRJ3HDezAgJrNh0GQWRQkhIx gz6/cTR+ekr5TptVszS9few2GpI5bCgBKBisZIssT89aw7mAKWut0Gcm4qM9/yK6 1bkCDQRatUmrARAAxNPvVwreJ2yAiFcUpdRlVhsuOgnxvs1QgsIw3H7+Pacr9Hpe 8uftYZqdC82KeSKhpHq7c8gMTMucIINtH25x9BCc73E33EjCL9Lqov1TL7+QkgHe T+JIhZwdD8Mx2K+LVVVu/aWkNrfMuNwyDUciSI4D5QHa8T+F8fgN4OTpwYjirzel 5yoICMr9hVcbzDNv/ozKCxjx+XKgnFc3wrnDfJfntfDAT7ecwbUTL+viQKJ646s+ psiqXRYtVvYInEhLVrJ0aV6zHFoigE/Bils6/g7ru1Q6CEHqEw++APs5CcE8VzJu WAGSVHZgun5Y9N4quR/M9Vm+IPMhTxrAg7rOvyRN9cAXfeSMf77I+XTifigNna8x t/MOdjXr1fjF4pThEi5u6WsuRdFwjY2azEv3vevodTi4HoJReH6dFRa6y8c+UDgl 2iHiOKIpQqLbHEfQmHcDd2fix+AaJKMnPGNku9qCFEMbgSRJpXz6BfwnY1QuKE+I R6jA0frUNt2jhiGG/F8RceXzohaaC/Cx7LUCUFWc0n7z32C9/Dtj7I1PMOacdZzz bjJzRKO/ZDv+UN/c9dwAkllzAyPMwGBkUaY68EBstnIliW34aWm6IiHhxioVPKSp VJfyiXPO0EXqujtHLAeChfjcns3I12YshT1dv2PafG53fp33ZdzeUgsBo+EAEQEA AYkCHwQYAQIACQUCWrVJqwIbDAAKCRC86dmkLVF4T+ZdD/9x/8APzgNJF3o3STrF jvnV1ycyhWYGAeBJiu7wjsNWwzMFOv15tLjB7AqeVxZn+WKDD/mIOQ45OZvnYZuy X7DR0JszaH9wrYTxZLVruAu+t6UL0y/XQ4L1GZ9QR6+r+7t1Mvbfy7BlHbvX/gYt Rwe/uwdibI0CagEzyX+2D3kTOlHO5XThbXaNf8AN8zha91Jt2Q2UR2X5T6JcwtMz FBvZnl3LSmZyE0EQehS2iUurU4uWOpGppuqVnbi0jbCvCHKgDGrqZ0smKNAQng54 F365W3g8AfY48s8XQwzmcliowYX9bT8PZiEi0J4QmQh0aXkpqZyFefuWeOL2R94S XKzr+gRh3BAULoqF+qK+IUMxTip9KTPNvYDpiC66yBiT6gFDji5Ca9pGpJXrC3xe TXiKQ8DBWDhBPVPrruLIaenTtZEOsPc4I85yt5U9RoPTStcOr34s3w5yEaJagt6S Gc5r9ysjkfH6+6rbi1ujxMgROSqtqr+RyB+V9A5/OgtNZc8llK6u4UoOCde8jUUW vqWKvjJB/Kz3u4zaeNu2ZyyHaOqOuH+TETcW+jsY9IhbEzqN5yQYGi4pVmDkY5vu lXbJnbqPKpRXgM9BecV9AMbPgbDq/5LnHJJXg+G8YQOgp4lR/hC1TEFdIp5wM8AK CWsENyt2o1rjgMXiZOMF8A5oBLkCDQRatUuSARAAr77kj7j2QR2SZeOSlFBvV7oS mFeSNnz9xZssqrsm6bTwSHM6YLDwc7Sdf2esDdyzONETwqrVCg+FxgL8hmo9hS4c rR6tmrP0mOmptr+xLLsKcaP7ogIXsyZnrEAEsvW8PnfayoiPCdc3cMCR/lTnHFGA 7EuR/XLBmi7Qg9tByVYQ5Yj5wB9V4B2yeCt3XtzPqeLKvaxl7PNelaHGJQY/xo+m V0bndxf9IY+4oFJ4blD32WqvyxESo7vW6WBh7oqv3Zbm0yQrr8a6mDBpqLkvWwNI 3kpJR974tg5o5LfDu1BeeyHWPSGm4U/G4JB+JIG1ADy+RmoWEt4BqTCZ/knnoGvw D5sTCxbKdmuOmhGyTssoG+3OOcGYHV7pWYPhazKHMPm201xKCjH1RfzRULzGKjD+ yMLT1I3AXFmLmZJXikAOlvE3/wgMqCXscbycbLjLD/bXIuFWo3rzoezeXjgi/DJx jKBAyBTYO5nMcth1O9oaFd9d0HbsOUDkIMnsgGBE766Piro6MHo0T0rXl07Tp4pI rwuSOsc6XzCzdImj0Wc6axS/HeUKRXWdXJwno5awTwXKRJMXGfhCvSvbcbc2Wx+L IKvmB7EB4K3fmjFFE67yolmiw2qRcUBfygtH3eL5XZU28MiCpue8Y8GKJoBAUyvf KeM1rO8Jm3iRAc5a/D0AEQEAAYkEPgQYAQIACQUCWrVLkgIbAgIpCRC86dmkLVF4 T8FdIAQZAQIABgUCWrVLkgAKCRDePL1hra+LjtHYD/9MucxdFe6bXO1dQR4tKhhQ P0LRqy6zlBY9ILCLowNdGZdqorogUiUymgn3VhEhVtxTOoHcN7qOuM01PNsRnOeS EYjf8Xrb1clzkD6xULwmOclTb9bBxnBc/4PFvHAbZW3QzusaZniNgkuxt6BTfloS Of4inq71kjmGK+TlzQ6mUMQUg228NUQC+a84EPqYyAeY1sgvgB7hJBhYL0QAxhcW 6m20Rd8iEc6HyzJ3yCOCsKip/nRWAbf0OvfHfRBp0+m0ZwnJM8cPRFjOqqzFpKH9 HpDmTrC4wKP1+TL52LyEqNh4yZitXmZNV7giSRIkk0eDSko+bFy6VbMzKUMkUJK3 D3eHFAMkujmbfJmSMTJOPGn5SB1HyjCZNx6bhIIbQyEUB9gKCmUFaqXKwKpF6rj0 iQXAJxLR/shZ5Rk96VxzOphUl7T90m/PnUEEPwq8KsBhnMRgxa0RFidDP+n9fgtv HLmrOqX9zBCVXh0mdWYLrWvmzQFWzG7AoE55fkf8nAEPsalrCdtaNUBHRXA0OQxG AHMOdJQQvBsmqMvuAdjkDWpFu5y0My5ddU+hiUzUyQLjL5Hhd5LOUDdewlZgIw1j xrEAUzDKetnemM8GkHxDgg8koev5frmShJuce7vSjKpCNg3EIJSgqMOPFjJuLWtZ vjHeDNbJy6uNL65ckJy6WhGjEADS2WAW1D6Tfekkc21SsIXk/LqEpLMR/0g5OUif wcEN1rS9IJXBwIy8MelN9qr5KcKQLmfdfBNEyyceBhyVl0MDyHOKC+7PofMtkGBq 13QieRHv5GJ8LB3fclqHV8pwTTo3Bc8z2g0TjmUYAN/ixETdReDoKavWJYSE9yoM aaJu279ioVTrwpECse0XkiRyKToTjwOb73CGkBZZpJyqux/rmCV/fp4ALdSW8zbz FJVORaivhoWwzjpfQKhwcU9lABXi2UvVm14v0AfeI7oiJPSU1zM4fEny4oiIBXlR zhFNih1UjIu82X16mTm3BwbIga/s1fnQRGzyhqUIMii+mWra23EwjChaxpvjjcUH 5ilLc5Zq781aCYRygYQw+hu5nFkOH1R+Z50Ubxjd/aqUfnGIAX7kPMD3Lof4KldD Q8ppQriUvxVo+4nPV6rpTy/PyqCLWDjkguHpJsEFsMkwajrAz0QNSAU5CJ0G2Zu4 yxvYlumHCEl7nbFrm0vIiA75Sa8KnywTDsyZsu3XcOcf3g+g1xWTpjJqy2bYXlqz 9uDOWtArWHOis6bq8l9RE6xr1RBVXS6uqgQIZFBGyq66b0dIq4D2JdsUvgEMaHbc e7tBfeB1CMBdA64e9Rq7bFR7Tvt8gasCZYlNr3lydh+dFHIEkH53HzQe6l88HEic +0jVnLkCDQRa55wJARAAyLya2Lx6gyoWoJN1a6740q3o8e9d4KggQOfGMTCflmeq ivuzgN+3DZHN+9ty2KxXMtn0mhHBerZdbNJyjMNT1gAgrhPNB4HtXBXum2wS57WK DNmade914L7FWTPAWBG2Wn448OEHTqsClICXXWy9IICgclAEyIq0Yq5mAdTEgRJS Z8t4GpwtDL9gNQyFXaWQmDmkAsCygQMvhAlmu9xOIzQG5CxSnZFk7zcuL60k14Z3 Cmt49k4T/7ZU8goWi8tt+rU78/IL3J/fF9+1civ1OwuUidgfPCSvOUW1JojsdCQA L+RZJcoXq7lfOFj/eNjeOSstCTDPfTCL+kThE6E5neDtbQHBYkEX1BRiTedsV4+M ucgiTrdQFWKf89G72xdv8ut9AYYQ2BbEYU+JAYhUH8rYYui2dHKJIgjNvJscuUWb +QEqJIRleJRhrO+/CHgMs4fZAkWF1VFhKBkcKmEjLn1f7EJJUUW84ZhKXjO/AUPX 1CHsNjziRceuJCJYox1cwsoq6jTE50GiNzcIxTn9xUc0UMKFeggNAFys1K+TDTm3 Bzo8H5ucjCUEmUm9lhkGwqTZgOlRX5eqPX+JBoSaObqhgqCa5IPinKRa6MgoFPHK 6sYKqroYwBGgZm6Js5chpNchvJMs/3WXNOEVg0J3z3vP0DMhxqWm+r+n9zlW8qsA EQEAAYkEPgQYAQgACQUCWuecCQIbAgIpCRC86dmkLVF4T8FdIAQZAQgABgUCWuec CQAKCRBQ3szEcQ5hr+ykD/4tOLRHFHXuKUcxgGaubUcVtsFrwBKma1cYjqaPms8u 6Sk0wfGRI32G/GhOrp0Ts/MOkbObq6VLTh8N5Yc/53MEl8zQFw9Y5AmRoW4PZXER ujs5s7p4oR7xHMihMjCCBn1bvrR+34YPfgzTcgLiOEFHYT8UTxwnGmXOvNkMM7md xD3CV5q6VAte8WKBo/220II3fcQlc9r/oWX4kXXkb0v9hoGwKbDJ1tzqTPrp/xFt yohqnvImpnlz+Q9zXmbrWYL9/g8VCmW/NN2gju2G3Lu/TlFUWIT4v/5OPK6TdeNb VKJO4+S8bTayqSG9CML1S57KSgCo5HUhQWeSNHI+fpe5oX6FALPT9JLDce8OZz1i cZZ0MELP37mOOQun0AlmHm/hVzf0f311PtbzcqWaE51tJvgUR/nZFo6Ta3O5Ezhs 3VlEJNQ1Ijf/6DH87SxvAoRIARCuZd0qxBcDK0avpFzUtbJd24lRA3WJpkEiMqKv RDVZkE4b6TW61f0o+LaVfK6E8oLpixegS4fiqC16mFrOdyRk+RJJfIUyz0WTDVmt g0U1CO1ezokMSqkJ7724pyjr2xf/r9/sC6aOJwB/lKgZkJfC6NqL7TlxVA31dUga LEOvEJTTE4gl+tYtfsCDvALCtqL0jduSkUo+RXcBItmXhA+tShW0pbS2Rtx/ixua KohVD/0R4QxiSwQmICNtm9mw9ydIl1yjYXX5a9x4wMJracNY/LBybJPFnZnT4dYR z4XjqysDwvvYZByaWoIe3QxjX84V6MlI2IdAT/xImu8gbaCI8tmyfpIrLnPKiR9D VFYfGBXuAX7+HgPPSFtrHQONCALxxzlbNpS+zxt9r0MiLgcLyspWxSdmoYGZ6nQP RO5Nm/ZVS+u2imPCRzNUZEMa+dlE6kHx0rS0dPiuJ4O7NtPeYDKkoQtNagspsDvh cK7CSqAiKMq06UBTxqlTSRkm62eOCtcs3p3OeHu5GRZF1uzTET0ZxYkaPgdrQknx ozjP5mC7X+45lcCfmcVt94TFNL5HwEUVJpmOgmzILCI8yoDTWzloo+i+fPFsXX4f kynhE83mSEcr5VHFYrTY3mQXGmNJ3bCLuc/jq7ysGq69xiKmTlUeXFm+aojcRO5i zyShIRJZ0GZfuzDYFDbMV9amA/YQGygLw//zP5ju5SW26dNxlf3MdFQE5JJ86rn9 MgZ4gcpazHEVUsbZsgkLizRp9imUiH8ymLqAXnfRGlU/LpNSefnvDFTtEIRcpOHc bhayG0bk51Bd4mioOXnIsKy4j63nJXA27x5EVVHQ1sYRN8Ny4Fdr2tMAmj2O+X+J qX2yy/UX5nSPU492e2CdZ1UhoU0SRFY3bxKHKB7SDbVeav+K5g== =Gi5D -----END PGP PUBLIC KEY BLOCK----- The details of the Amazon ECS PGP public key for reference: Key ID: BCE9D9A42D51784F Type: RSA Size: 4096/4096 Expires: Never User ID: Amazon ECS Key fingerprint: F34C 3DDA E729 26B0 79BE AEC6 BCE9 D9A4 2D51 784F Import the Amazon ECS PGP public key with the following command. gpg --import <public_key_filename> Download the AWS Copilot CLI signatures. The signatures are ASCII detached PGP signatures stored in files with the extension .asc . The signatures file has the same name as its corresponding executable, with .asc appended. macOS For macOS systems, run the following command. sudo curl -Lo copilot.asc https://github.com/aws/copilot-cli/releases/latest/download/copilot-darwin.asc Linux For Linux systems, run the following command. sudo curl -Lo copilot.asc https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux.asc Windows For Windows systems, run the following command. Invoke-WebRequest -OutFile ecs-cli.asc https://github.com/aws/copilot-cli/releases/latest/download/copilot-windows.exe Verify the signature with the following command. For macOS and Linux systems: gpg --verify copilot.asc /usr/local/bin/copilot For Windows systems: gpg --verify ecs-cli.asc 'C:\\Program Files\\copilot.exe' Expected output: gpg: Signature made Tue Apr 3 13:29:30 2018 PDT gpg: using RSA key DE3CBD61ADAF8B8E gpg: Good signature from \"Amazon ECS <ecs-security@amazon.com>\" [unknown] gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: F34C 3DDA E729 26B0 79BE AEC6 BCE9 D9A4 2D51 784F Subkey fingerprint: EB3D F841 E2C9 212A 2BD4 2232 DE3C BD61 ADAF 8B8E Warning The warning in the output is expected and is not problematic. It occurs because there is not a chain of trust between your personal PGP key (if you have one) and the Amazon ECS PGP key. For more information, see Web of trust .","title":"(Optional) Verify the installation"},{"location":"docs/include/command","text":"command String or Array of Strings Override the default command in the image. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ]","title":"Command"},{"location":"docs/include/common-svc-fields","text":"entrypoint String or Array of Strings Override the default entrypoint in the image. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the image. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] cpu Integer Number of CPU units for the task. See the Amazon ECS docs for valid CPU values. memory Integer Amount of memory in MiB used by the task. See the Amazon ECS docs for valid memory values. platform String or Map Operating system and architecture (formatted as [os]/[arch] ) to pass with docker build --platform . For example, linux/arm64 or windows/x86_64 . The default is linux/x86_64 . Override the generated string to build with a different valid osfamily or architecture . For example, Windows users might change the string platform : windows/x86_64 which defaults to WINDOWS_SERVER_2019_CORE , using a map: platform : osfamily : windows_server_2019_full architecture : x86_64 count Integer or Map If you specify a number: count : 5 The service will set the desired count to 5 and maintain 5 tasks in your service. count. spot Integer If you want to use Fargate Spot capacity to run your services, you can specify a number under the spot subfield: count : spot : 5 Info Fargate Spot is not supported for containers running on ARM architecture. Alternatively, you can specify a map for setting up autoscaling: count : range : 1-10 cpu_percentage : 70 memory_percentage : 80 requests : 10000 response_time : 2s count. range String or Map You can specify a minimum and maximum bound for the number of tasks your service should maintain, based on the values you specify for the metrics. count : range : n-m This will set up an Application Autoscaling Target with the MinCapacity of n and MaxCapacity of m . Alternatively, if you wish to scale your service onto Fargate Spot instances, specify min and max under range and then specify spot_from with the desired count you wish to start placing your services onto Spot capacity. For example: count : range : min : 1 max : 10 spot_from : 3 This will set your range as 1-10 as above, but will place the first two copies of your service on dedicated Fargate capacity. If your service scales to 3 or higher, the third and any additional copies will be placed on Spot until the maximum is reached. range. min Integer The minimum desired count for your service using autoscaling. range. max Integer The maximum desired count for your service using autoscaling. range. spot_from Integer The desired count at which you wish to start placing your service using Fargate Spot capacity providers. count. cpu_percentage Integer Scale up or down based on the average CPU your service should maintain. count. memory_percentage Integer Scale up or down based on the average memory your service should maintain. count. requests Integer Scale up or down based on the request count handled per tasks. count. response_time Duration Scale up or down based on the service average response time. exec Boolean Enable running commands in your container. The default is false . Required for $ copilot svc exec . network Map The network section contains parameters for connecting to AWS resources in a VPC. network. vpc Map Subnets and security groups attached to your tasks. network.vpc. placement String Must be one of 'public' or 'private' . Defaults to launching your tasks in public subnets. Info If you launch tasks in 'private' subnets and use a Copilot-generated VPC, Copilot will automatically add NAT Gateways to your environment for internet connectivity. (See pricing .) Alternatively, when running copilot env init , you can import an existing VPC with NAT Gateways, or one with VPC endpoints for isolated workloads. See our custom environment resources page for more. network.vpc. security_groups Array of Strings Additional security group IDs associated with your tasks. Copilot always includes a security group so containers within your environment can communicate with each other. variables Map Key-value pairs that represent environment variables that will be passed to your service. Copilot will include a number of environment variables by default for you. secrets Map Key-value pairs that represent secret values from AWS Systems Manager Parameter Store that will be securely passed to your service as environment variables. storage Map The Storage section lets you specify external EFS volumes for your containers and sidecars to mount. This allows you to access persistent storage across availability zones in a region for data processing or CMS workloads. For more detail, see the storage page. You can also specify extensible ephemeral storage at the task level. storage. ephemeral Int Specify how much ephemeral task storage to provision in GiB. The default value and minimum is 20 GiB. The maximum size is 200 GiB. Sizes above 20 GiB incur additional charges. To create a shared filesystem context between an essential container and a sidecar, you can use an empty volume: storage : ephemeral : 100 volumes : scratch : path : /var/data read_only : false sidecars : mySidecar : image : public.ecr.aws/my-image:latest mount_points : - source_volume : scratch path : /var/data read_only : false This example will provision 100 GiB of storage to be shared between the sidecar and the task container. This can be useful for large datasets, or for using a sidecar to transfer data from EFS into task storage for workloads with high disk I/O requirements. storage. volumes Map Specify the name and configuration of any EFS volumes you would like to attach. The volumes field is specified as a map of the form: volumes : <volume name> : path : \"/etc/mountpath\" efs : ... storage.volumes. volume Map Specify the configuration of a volume. volume. path String Required. Specify the location in the container where you would like your volume to be mounted. Must be fewer than 242 characters and must consist only of the characters a-zA-Z0-9.-_/ . volume. read_only Boolean Optional. Defaults to true . Defines whether the volume is read-only or not. If false, the container is granted elasticfilesystem:ClientWrite permissions to the filesystem and the volume is writable. volume. efs Boolean or Map Specify more detailed EFS configuration. If specified as a boolean, or using only the uid and gid subfields, creates a managed EFS filesystem and dedicated Access Point for this workload. // Simple managed EFS efs : true // Managed EFS with custom POSIX info efs : uid : 10000 gid : 110000 volume.efs. id String Required. The ID of the filesystem you would like to mount. volume.efs. root_dir String Optional. Defaults to / . Specify the location in the EFS filesystem you would like to use as the root of your volume. Must be fewer than 255 characters and must consist only of the characters a-zA-Z0-9.-_/ . If using an access point, root_dir must be either empty or / and auth.iam must be true . volume.efs. uid Uint32 Optional. Must be specified with gid . Mutually exclusive with root_dir , auth , and id . The POSIX UID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. gid Uint32 Optional. Must be specified with uid . Mutually exclusive with root_dir , auth , and id . The POSIX GID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. auth Map Specify advanced authorization configuration for EFS. volume.efs.auth. iam Boolean Optional. Defaults to true . Whether or not to use IAM authorization to determine whether the volume is allowed to connect to EFS. volume.efs.auth. access_point_id String Optional. Defaults to \"\" . The ID of the EFS access point to connect to. If using an access point, root_dir must be either empty or / and auth.iam must be true . logging Map The logging section contains log configuration. You can also configure parameters for your container's FireLens log driver in this section (see examples here ). logging. retention Integer Optional. The number of days to retain the log events. See this page for all accepted values. If omitted, the default is 30. logging. image String Optional. The Fluent Bit image to use. Defaults to amazon/aws-for-fluent-bit:latest . logging. destination Map Optional. The configuration options to send to the FireLens log driver. logging. enableMetadata Boolean Optional. Whether to include ECS metadata in logs. Defaults to true . logging. secretOptions Map Optional. The secrets to pass to the log configuration. logging. configFilePath String Optional. The full config file path in your custom Fluent Bit image. taskdef_overrides Array of Rules The taskdef_overrides section allows users to apply overriding rules to their ECS Task Definitions (see examples here ). taskdef_overrides. path String Required. Path to the Task Definition field to override. taskdef_overrides. value Any Required. Value of the Task Definition field to override. environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the count parameter so that we can run 2 copies of our service in our 'prod' environment, and 2 copies using Fargate Spot capacity in our 'staging' environment.","title":"Common svc fields"},{"location":"docs/include/entrypoint","text":"entrypoint String or Array of Strings Override the default entrypoint in the image. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ]","title":"Entrypoint"},{"location":"docs/include/environments","text":"environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the count parameter so that we can run 2 copies of our service in our 'prod' environment, and 2 copies using Fargate Spot capacity in our 'staging' environment.","title":"Environments"},{"location":"docs/include/envvars","text":"variables Map Key-value pairs that represent environment variables that will be passed to your service. Copilot will include a number of environment variables by default for you.","title":"Envvars"},{"location":"docs/include/exec","text":"exec Boolean Enable running commands in your container. The default is false . Required for $ copilot svc exec . Info Exec is not supported for containers running on Windows OS.","title":"Exec"},{"location":"docs/include/http-config","text":"http Map The http section contains parameters related to integrating your service with an Application Load Balancer. http. path String Requests to this path will be forwarded to your service. Each Load Balanced Web Service should listen on a unique path. http. healthcheck String or Map If you specify a string, Copilot interprets it as the path exposed in your container to handle target group health check requests. The default is \"/\". http : healthcheck : '/' You can also specify healthcheck as a map: http : healthcheck : path : '/' success_codes : '200' healthy_threshold : 3 unhealthy_threshold : 2 interval : 15s timeout : 10s grace_period : 60s http.healthcheck. path String The destination that the health check requests are sent to. http.healthcheck. success_codes String The HTTP status codes that healthy targets must use when responding to an HTTP health check. You can specify values between 200 and 499. You can specify multiple values (for example, \"200,202\") or a range of values (for example, \"200-299\"). The default is 200. http.healthcheck. healthy_threshold Integer The number of consecutive health check successes required before considering an unhealthy target healthy. The default is 5. Range: 2-10. http.healthcheck. unhealthy_threshold Integer The number of consecutive health check failures required before considering a target unhealthy. The default is 2. Range: 2-10. http.healthcheck. interval Duration The approximate amount of time, in seconds, between health checks of an individual target. The default is 30s. Range: 5s\u2013300s. http.healthcheck. timeout Duration The amount of time, in seconds, during which no response from a target means a failed health check. The default is 5s. Range 5s-300s. http.healthcheck. grace_period Duration The amount of time to ignore failing target group healthchecks on container start. The default is 60s. This can be useful to fix deployment issues for containers which take a while to become healthy and begin listening for incoming connections, or to speed up deployment of containers guaranteed to start quickly. http. deregistration_delay Duration The amount of time to wait for targets to drain connections during deregistration. The default is 60s. Setting this to a larger value gives targets more time to gracefully drain connections, but increases the time required for new deployments. Range 0s-3600s. http. target_container String A sidecar container that takes the place of a service container. http. stickiness Boolean Indicates whether sticky sessions are enabled. http. allowed_source_ips Array of Strings CIDR IP addresses permitted to access your service. http : allowed_source_ips : [ \"192.0.2.0/24\" , \"198.51.100.10/32\" ] http. alias String or Array of Strings HTTPS domain alias of your service. # String version. http : alias : example.com # Alteratively, as an array of strings. http : alias : [ \"example.com\" , \"v1.example.com\" ] http. version String The HTTP(S) protocol version. Must be one of 'grpc' , 'http1' , or 'http2' . If omitted, then 'http1' is assumed. If using gRPC, please note that a domain must be associated with your application.","title":"Http config"},{"location":"docs/include/image-config-with-port","text":"image Map The image section contains parameters relating to the Docker build configuration and exposed port. image. build String or Map Build a container from a Dockerfile with optional arguments. Mutually exclusive with image.location . If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . The location field follows the same definition as the image parameter in the Amazon ECS task definition. Warning If you are passing in a Windows image, you must add platform: windows/x86_64 to your manifest. If you are passing in an ARM architecture-based image, you must add platform: linux/arm64 to your manifest. image. credentials String An optional credentials ARN for a private repository. The credentials field follows the same definition as the credentialsParameter in the Amazon ECS task definition. image. labels Map An optional key/value map of Docker labels to add to the container. image. depends_on Map An optional key/value map of Container Dependencies to add to the container. The key of the map is a container name and the value is the condition to depend on. Valid conditions are: start , healthy , complete , and success . You cannot specify a complete or success dependency on an essential container. For example: image : build : ./Dockerfile depends_on : nginx : start startup : success In the above example, the task's main container will only start after the nginx sidecar has started and the startup container has completed successfully. image. port Integer The port exposed in your Dockerfile. Copilot should parse this value for you from your EXPOSE instruction.","title":"Image config with port"},{"location":"docs/include/image-config","text":"image Map The image section contains parameters relating to the Docker build configuration and exposed port. image. build String or Map Build a container from a Dockerfile with optional arguments. Mutually exclusive with image.location . If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . The location field follows the same definition as the image parameter in the Amazon ECS task definition. Warning If you are passing in a Windows image, you must add platform: windows/x86_64 to your manifest. If you are passing in an ARM architecture-based image, you must add platform: linux/arm64 to your manifest. image. credentials String An optional credentials ARN for a private repository. The credentials field follows the same definition as the credentialsParameter in the Amazon ECS task definition. image. labels Map An optional key/value map of Docker labels to add to the container. image. depends_on Map An optional key/value map of Container Dependencies to add to the container. The key of the map is a container name and the value is the condition to depend on. Valid conditions are: start , healthy , complete , and success . You cannot specify a complete or success dependency on an essential container. For example: image : build : ./Dockerfile depends_on : nginx : start startup : success In the above example, the task's main container will only start after the nginx sidecar has started and the startup container has completed successfully.","title":"Image config"},{"location":"docs/include/image-healthcheck","text":"image. healthcheck Map Optional configuration for container health checks. image.healthcheck. command Array of Strings The command to run to determine if the container is healthy. The string array can start with CMD to execute the command arguments directly, or CMD-SHELL to run the command with the container's default shell. image.healthcheck. interval Duration Time period between health checks, in seconds. Default is 10s. image.healthcheck. retries Integer Number of times to retry before container is deemed unhealthy. Default is 2. image.healthcheck. timeout Duration How long to wait before considering the health check failed, in seconds. Default is 5s. image.healthcheck. start_period Duration Length of grace period for containers to bootstrap before failed health checks count towards the maximum number of retries. Default is 0s.","title":"Image healthcheck"},{"location":"docs/include/logging","text":"logging Map The logging section contains log configuration. You can also configure parameters for your container's FireLens log driver in this section (see examples here ). logging. retention Integer Optional. The number of days to retain the log events. See this page for all accepted values. If omitted, the default is 30. logging. image Map Optional. The Fluent Bit image to use. Defaults to amazon/aws-for-fluent-bit:latest . logging. destination Map Optional. The configuration options to send to the FireLens log driver. logging. enableMetadata Map Optional. Whether to include ECS metadata in logs. Defaults to true . logging. secretOptions Map Optional. The secrets to pass to the log configuration. logging. configFilePath Map Optional. The full config file path in your custom Fluent Bit image.","title":"Logging"},{"location":"docs/include/network","text":"network Map The network section contains parameters for connecting to AWS resources in a VPC. network. vpc Map Subnets and security groups attached to your tasks. network.vpc. placement String Must be one of 'public' or 'private' . Defaults to launching your tasks in public subnets. Info If you launch tasks in 'private' subnets and use a Copilot-generated VPC, Copilot will automatically add NAT Gateways to your environment for internet connectivity. (See pricing .) Alternatively, when running copilot env init , you can import an existing VPC with NAT Gateways, or one with VPC endpoints for isolated workloads. See our custom environment resources page for more. network.vpc. security_groups Array of Strings Additional security group IDs associated with your tasks. Copilot always includes a security group so containers within your environment can communicate with each other.","title":"Network"},{"location":"docs/include/platform","text":"platform String Operating system and architecture (formatted as [os]/[arch] ) to pass with docker build --platform . For example, linux/arm64 or windows/x86_64 . The default is linux/x86_64 . Override the generated string to build with a different valid osfamily or architecture . For example, Windows users might change the string platform : windows/x86_64 which defaults to WINDOWS_SERVER_2019_CORE , using a map: platform : osfamily : windows_server_2019_full architecture : x86_64","title":"Platform"},{"location":"docs/include/publish","text":"publish Map The publish section allows services to publish messages to one or more SNS topics. publish : topics : - name : order-events In the example above, this manifest declares an SNS topic named order-events that other worker services which are deployed to the Copilot environment can subscribe to. publish. topics Array of topics List of topic objects. publish.topics. topic Map Holds configuration for a single SNS topic. topic. name String Required. The name of the SNS topic. Must contain only upper and lowercase letters, numbers, hyphens, and underscores.","title":"Publish"},{"location":"docs/include/secrets","text":"secrets Map Key-value pairs that represent secret values from AWS Systems Manager Parameter Store that will be securely passed to your service as environment variables.","title":"Secrets"},{"location":"docs/include/sidecar-config","text":"port Integer Port of the container to expose (optional). image String Image URL for the sidecar container (required). essential Bool Whether the sidecar container is an essential container (optional, default true). credentialsParameter String ARN of the secret containing the private repository credentials (optional). variables Map Environment variables for the sidecar container (optional) secrets Map Secrets to expose to the sidecar container (optional) mount_points Array of Maps Mount paths for EFS volumes specified at the service level (optional). mount_points. source_volume String Source volume to mount in this sidecar (required). mount_points. path String The path inside the sidecar container at which to mount the volume (required). mount_points. read_only Boolean Whether to allow the sidecar read-only access to the volume (default true). labels Map Docker labels to apply to this container (optional). depends_on Map Container dependencies to apply to this container (optional). entrypoint String or Array of Strings Override the default entrypoint in the sidecar. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the sidecar. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] healthcheck Map Optional configuration for sidecar container health checks. healthcheck. command Array of Strings The command to run to determine if the sidecar container is healthy. The string array can start with CMD to execute the command arguments directly, or CMD-SHELL to run the command with the container's default shell. healthcheck. interval Duration Time period between health checks, in seconds. Default is 10s. healthcheck. retries Integer Number of times to retry before container is deemed unhealthy. Default is 2. healthcheck. timeout Duration How long to wait before considering the health check failed, in seconds. Default is 5s. healthcheck. start_period Duration Length of grace period for containers to bootstrap before failed health checks count towards the maximum number of retries. Default is 0s.","title":"Sidecar config"},{"location":"docs/include/storage","text":"storage Map The Storage section lets you specify external EFS volumes for your containers and sidecars to mount. This allows you to access persistent storage across availability zones in a region for data processing or CMS workloads. For more detail, see the storage page. You can also specify extensible ephemeral storage at the task level. storage. ephemeral Int Specify how much ephemeral task storage to provision in GiB. The default value and minimum is 20 GiB. The maximum size is 200 GiB. Sizes above 20 GiB incur additional charges. To create a shared filesystem context between an essential container and a sidecar, you can use an empty volume: storage : ephemeral : 100 volumes : scratch : path : /var/data read_only : false sidecars : mySidecar : image : public.ecr.aws/my-image:latest mount_points : - source_volume : scratch path : /var/data read_only : false This example will provision 100 GiB of storage to be shared between the sidecar and the task container. This can be useful for large datasets, or for using a sidecar to transfer data from EFS into task storage for workloads with high disk I/O requirements. storage. volumes Map Specify the name and configuration of any EFS volumes you would like to attach. The volumes field is specified as a map of the form: volumes : <volume name> : path : \"/etc/mountpath\" efs : ... storage.volumes. volume Map Specify the configuration of a volume. volume. path String Required. Specify the location in the container where you would like your volume to be mounted. Must be fewer than 242 characters and must consist only of the characters a-zA-Z0-9.-_/ . volume. read_only Boolean Optional. Defaults to true . Defines whether the volume is read-only or not. If false, the container is granted elasticfilesystem:ClientWrite permissions to the filesystem and the volume is writable. volume. efs Boolean or Map Specify more detailed EFS configuration. If specified as a boolean, or using only the uid and gid subfields, creates a managed EFS filesystem and dedicated Access Point for this workload. // Simple managed EFS efs : true // Managed EFS with custom POSIX info efs : uid : 10000 gid : 110000 volume.efs. id String Required. The ID of the filesystem you would like to mount. volume.efs. root_dir String Optional. Defaults to / . Specify the location in the EFS filesystem you would like to use as the root of your volume. Must be fewer than 255 characters and must consist only of the characters a-zA-Z0-9.-_/ . If using an access point, root_dir must be either empty or / and auth.iam must be true . volume.efs. uid Uint32 Optional. Must be specified with gid . Mutually exclusive with root_dir , auth , and id . The POSIX UID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. gid Uint32 Optional. Must be specified with uid . Mutually exclusive with root_dir , auth , and id . The POSIX GID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. auth Map Specify advanced authorization configuration for EFS. volume.efs.auth. iam Boolean Optional. Defaults to true . Whether or not to use IAM authorization to determine whether the volume is allowed to connect to EFS. volume.efs.auth. access_point_id String Optional. Defaults to \"\" . The ID of the EFS access point to connect to. If using an access point, root_dir must be either empty or / and auth.iam must be true .","title":"Storage"},{"location":"docs/include/task-size","text":"cpu Integer Number of CPU units for the task. See the Amazon ECS docs for valid CPU values. memory Integer Amount of memory in MiB used by the task. See the Amazon ECS docs for valid memory values.","title":"Task size"},{"location":"docs/include/taskdef-overrides","text":"taskdef_overrides Array of Rules The taskdef_overrides section allows users to apply overriding rules to their ECS Task Definitions (see examples here ). taskdef_overrides. path String Required. Path to the Task Definition field to override. taskdef_overrides. value Any Required. Value of the Task Definition field to override.","title":"Taskdef overrides"},{"location":"docs/manifest/backend-service","text":"List of all available properties for a 'Backend Service' manifest. To learn about Copilot services, see the Services concept page. Sample manifest for an api service # Your service name will be used in naming your resources like log groups, ECS services, etc. name : api type : Backend Service # Your service is reachable at \"http://api.${COPILOT_SERVICE_DISCOVERY_ENDPOINT}:8080\" but is not public. # Configuration for your containers and service. image : build : ./api/Dockerfile port : 8080 healthcheck : command : [ \"CMD-SHELL\" , \"curl -f http://localhost:8080 || exit 1\" ] interval : 10s retries : 2 timeout : 5s start_period : 0s cpu : 256 memory : 512 count : 1 exec : true storage : volumes : myEFSVolume : path : '/etc/mount1' read_only : true efs : id : fs-12345678 root_dir : '/' auth : iam : true access_point_id : fsap-12345678 network : vpc : placement : 'private' security_groups : [ 'sg-05d7cd12cceeb9a6e' ] variables : LOG_LEVEL : info secrets : GITHUB_TOKEN : GITHUB_TOKEN # You can override any of the values defined above by environment. environments : test : count : spot : 2 production : count : 2 name String The name of your service. type String The architecture type for your service. Backend Services are not reachable from the internet, but can be reached with service discovery from your other services. image Map The image section contains parameters relating to the Docker build configuration and exposed port. image. build String or Map Build a container from a Dockerfile with optional arguments. Mutually exclusive with image.location . If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . The location field follows the same definition as the image parameter in the Amazon ECS task definition. Warning If you are passing in a Windows image, you must add platform: windows/x86_64 to your manifest. If you are passing in an ARM architecture-based image, you must add platform: linux/arm64 to your manifest. image. credentials String An optional credentials ARN for a private repository. The credentials field follows the same definition as the credentialsParameter in the Amazon ECS task definition. image. labels Map An optional key/value map of Docker labels to add to the container. image. depends_on Map An optional key/value map of Container Dependencies to add to the container. The key of the map is a container name and the value is the condition to depend on. Valid conditions are: start , healthy , complete , and success . You cannot specify a complete or success dependency on an essential container. For example: image : build : ./Dockerfile depends_on : nginx : start startup : success In the above example, the task's main container will only start after the nginx sidecar has started and the startup container has completed successfully. image. port Integer The port exposed in your Dockerfile. Copilot should parse this value for you from your EXPOSE instruction. image. healthcheck Map Optional configuration for container health checks. image.healthcheck. command Array of Strings The command to run to determine if the container is healthy. The string array can start with CMD to execute the command arguments directly, or CMD-SHELL to run the command with the container's default shell. image.healthcheck. interval Duration Time period between health checks, in seconds. Default is 10s. image.healthcheck. retries Integer Number of times to retry before container is deemed unhealthy. Default is 2. image.healthcheck. timeout Duration How long to wait before considering the health check failed, in seconds. Default is 5s. image.healthcheck. start_period Duration Length of grace period for containers to bootstrap before failed health checks count towards the maximum number of retries. Default is 0s. cpu Integer Number of CPU units for the task. See the Amazon ECS docs for valid CPU values. memory Integer Amount of memory in MiB used by the task. See the Amazon ECS docs for valid memory values. platform String Operating system and architecture (formatted as [os]/[arch] ) to pass with docker build --platform . For example, linux/arm64 or windows/x86_64 . The default is linux/x86_64 . Override the generated string to build with a different valid osfamily or architecture . For example, Windows users might change the string platform : windows/x86_64 which defaults to WINDOWS_SERVER_2019_CORE , using a map: platform : osfamily : windows_server_2019_full architecture : x86_64 count Integer or Map If you specify a number: count : 5 The service will set the desired count to 5 and maintain 5 tasks in your service. count. spot Integer If you want to use Fargate Spot capacity to run your services, you can specify a number under the spot subfield: count : spot : 5 Info Fargate Spot is not supported for containers running on ARM architecture. Alternatively, you can specify a map for setting up autoscaling: count : range : 1-10 cpu_percentage : 70 memory_percentage : 80 count. range String or Map You can specify a minimum and maximum bound for the number of tasks your service should maintain, based on the values you specify for the metrics. count : range : n-m This will set up an Application Autoscaling Target with the MinCapacity of n and MaxCapacity of m . Alternatively, if you wish to scale your service onto Fargate Spot instances, specify min and max under range and then specify spot_from with the desired count you wish to start placing your services onto Spot capacity. For example: count : range : min : 1 max : 10 spot_from : 3 This will set your range as 1-10 as above, but will place the first two copies of your service on dedicated Fargate capacity. If your service scales to 3 or higher, the third and any additional copies will be placed on Spot until the maximum is reached. range. min Integer The minimum desired count for your service using autoscaling. range. max Integer The maximum desired count for your service using autoscaling. range. spot_from Integer The desired count at which you wish to start placing your service using Fargate Spot capacity providers. count. cpu_percentage Integer Scale up or down based on the average CPU your service should maintain. count. memory_percentage Integer Scale up or down based on the average memory your service should maintain. exec Boolean Enable running commands in your container. The default is false . Required for $ copilot svc exec . Info Exec is not supported for containers running on Windows OS. entrypoint String or Array of Strings Override the default entrypoint in the image. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the image. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] network Map The network section contains parameters for connecting to AWS resources in a VPC. network. vpc Map Subnets and security groups attached to your tasks. network.vpc. placement String Must be one of 'public' or 'private' . Defaults to launching your tasks in public subnets. Info If you launch tasks in 'private' subnets and use a Copilot-generated VPC, Copilot will automatically add NAT Gateways to your environment for internet connectivity. (See pricing .) Alternatively, when running copilot env init , you can import an existing VPC with NAT Gateways, or one with VPC endpoints for isolated workloads. See our custom environment resources page for more. network.vpc. security_groups Array of Strings Additional security group IDs associated with your tasks. Copilot always includes a security group so containers within your environment can communicate with each other. variables Map Key-value pairs that represent environment variables that will be passed to your service. Copilot will include a number of environment variables by default for you. secrets Map Key-value pairs that represent secret values from AWS Systems Manager Parameter Store that will be securely passed to your service as environment variables. storage Map The Storage section lets you specify external EFS volumes for your containers and sidecars to mount. This allows you to access persistent storage across availability zones in a region for data processing or CMS workloads. For more detail, see the storage page. You can also specify extensible ephemeral storage at the task level. storage. ephemeral Int Specify how much ephemeral task storage to provision in GiB. The default value and minimum is 20 GiB. The maximum size is 200 GiB. Sizes above 20 GiB incur additional charges. To create a shared filesystem context between an essential container and a sidecar, you can use an empty volume: storage : ephemeral : 100 volumes : scratch : path : /var/data read_only : false sidecars : mySidecar : image : public.ecr.aws/my-image:latest mount_points : - source_volume : scratch path : /var/data read_only : false This example will provision 100 GiB of storage to be shared between the sidecar and the task container. This can be useful for large datasets, or for using a sidecar to transfer data from EFS into task storage for workloads with high disk I/O requirements. storage. volumes Map Specify the name and configuration of any EFS volumes you would like to attach. The volumes field is specified as a map of the form: volumes : <volume name> : path : \"/etc/mountpath\" efs : ... storage.volumes. volume Map Specify the configuration of a volume. volume. path String Required. Specify the location in the container where you would like your volume to be mounted. Must be fewer than 242 characters and must consist only of the characters a-zA-Z0-9.-_/ . volume. read_only Boolean Optional. Defaults to true . Defines whether the volume is read-only or not. If false, the container is granted elasticfilesystem:ClientWrite permissions to the filesystem and the volume is writable. volume. efs Boolean or Map Specify more detailed EFS configuration. If specified as a boolean, or using only the uid and gid subfields, creates a managed EFS filesystem and dedicated Access Point for this workload. // Simple managed EFS efs : true // Managed EFS with custom POSIX info efs : uid : 10000 gid : 110000 volume.efs. id String Required. The ID of the filesystem you would like to mount. volume.efs. root_dir String Optional. Defaults to / . Specify the location in the EFS filesystem you would like to use as the root of your volume. Must be fewer than 255 characters and must consist only of the characters a-zA-Z0-9.-_/ . If using an access point, root_dir must be either empty or / and auth.iam must be true . volume.efs. uid Uint32 Optional. Must be specified with gid . Mutually exclusive with root_dir , auth , and id . The POSIX UID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. gid Uint32 Optional. Must be specified with uid . Mutually exclusive with root_dir , auth , and id . The POSIX GID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. auth Map Specify advanced authorization configuration for EFS. volume.efs.auth. iam Boolean Optional. Defaults to true . Whether or not to use IAM authorization to determine whether the volume is allowed to connect to EFS. volume.efs.auth. access_point_id String Optional. Defaults to \"\" . The ID of the EFS access point to connect to. If using an access point, root_dir must be either empty or / and auth.iam must be true . publish Map The publish section allows services to publish messages to one or more SNS topics. publish : topics : - name : order-events In the example above, this manifest declares an SNS topic named order-events that other worker services which are deployed to the Copilot environment can subscribe to. publish. topics Array of topics List of topic objects. publish.topics. topic Map Holds configuration for a single SNS topic. topic. name String Required. The name of the SNS topic. Must contain only upper and lowercase letters, numbers, hyphens, and underscores. logging Map The logging section contains log configuration. You can also configure parameters for your container's FireLens log driver in this section (see examples here ). logging. retention Integer Optional. The number of days to retain the log events. See this page for all accepted values. If omitted, the default is 30. logging. image Map Optional. The Fluent Bit image to use. Defaults to amazon/aws-for-fluent-bit:latest . logging. destination Map Optional. The configuration options to send to the FireLens log driver. logging. enableMetadata Map Optional. Whether to include ECS metadata in logs. Defaults to true . logging. secretOptions Map Optional. The secrets to pass to the log configuration. logging. configFilePath Map Optional. The full config file path in your custom Fluent Bit image. taskdef_overrides Array of Rules The taskdef_overrides section allows users to apply overriding rules to their ECS Task Definitions (see examples here ). taskdef_overrides. path String Required. Path to the Task Definition field to override. taskdef_overrides. value Any Required. Value of the Task Definition field to override. environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the count parameter so that we can run 2 copies of our service in our 'prod' environment, and 2 copies using Fargate Spot capacity in our 'staging' environment.","title":"Backend Service"},{"location":"docs/manifest/lb-web-service","text":"List of all available properties for a 'Load Balanced Web Service' manifest. To learn about Copilot services, see the Services concept page. Sample manifest for a frontend service # Your service name will be used in naming your resources like log groups, ECS services, etc. name : frontend type : Load Balanced Web Service # Distribute traffic to your service. http : path : '/' healthcheck : path : '/_healthcheck' success_codes : '200,301' healthy_threshold : 3 unhealthy_threshold : 2 interval : 15s timeout : 10s grace_period : 45s deregistration_delay : 5s stickiness : false allowed_source_ips : [ \"10.24.34.0/23\" ] alias : example.com # Configuration for your containers and service. image : build : dockerfile : ./frontend/Dockerfile context : ./frontend port : 80 cpu : 256 memory : 512 count : range : 1-10 cpu_percentage : 70 memory_percentage : 80 requests : 10000 response_time : 2s exec : true variables : LOG_LEVEL : info secrets : GITHUB_TOKEN : GITHUB_TOKEN # You can override any of the values defined above by environment. environments : test : count : range : min : 1 max : 10 spot_from : 2 staging : count : spot : 2 production : count : 2 name String The name of your service. type String The architecture type for your service. A Load Balanced Web Service is an internet-facing service that's behind a load balancer, orchestrated by Amazon ECS on AWS Fargate. http Map The http section contains parameters related to integrating your service with an Application Load Balancer. http. path String Requests to this path will be forwarded to your service. Each Load Balanced Web Service should listen on a unique path. http. healthcheck String or Map If you specify a string, Copilot interprets it as the path exposed in your container to handle target group health check requests. The default is \"/\". http : healthcheck : '/' You can also specify healthcheck as a map: http : healthcheck : path : '/' success_codes : '200' healthy_threshold : 3 unhealthy_threshold : 2 interval : 15s timeout : 10s grace_period : 60s http.healthcheck. path String The destination that the health check requests are sent to. http.healthcheck. success_codes String The HTTP status codes that healthy targets must use when responding to an HTTP health check. You can specify values between 200 and 499. You can specify multiple values (for example, \"200,202\") or a range of values (for example, \"200-299\"). The default is 200. http.healthcheck. healthy_threshold Integer The number of consecutive health check successes required before considering an unhealthy target healthy. The default is 5. Range: 2-10. http.healthcheck. unhealthy_threshold Integer The number of consecutive health check failures required before considering a target unhealthy. The default is 2. Range: 2-10. http.healthcheck. interval Duration The approximate amount of time, in seconds, between health checks of an individual target. The default is 30s. Range: 5s\u2013300s. http.healthcheck. timeout Duration The amount of time, in seconds, during which no response from a target means a failed health check. The default is 5s. Range 5s-300s. http.healthcheck. grace_period Duration The amount of time to ignore failing target group healthchecks on container start. The default is 60s. This can be useful to fix deployment issues for containers which take a while to become healthy and begin listening for incoming connections, or to speed up deployment of containers guaranteed to start quickly. http. deregistration_delay Duration The amount of time to wait for targets to drain connections during deregistration. The default is 60s. Setting this to a larger value gives targets more time to gracefully drain connections, but increases the time required for new deployments. Range 0s-3600s. http. target_container String A sidecar container that takes the place of a service container. http. stickiness Boolean Indicates whether sticky sessions are enabled. http. allowed_source_ips Array of Strings CIDR IP addresses permitted to access your service. http : allowed_source_ips : [ \"192.0.2.0/24\" , \"198.51.100.10/32\" ] http. alias String or Array of Strings HTTPS domain alias of your service. # String version. http : alias : example.com # Alteratively, as an array of strings. http : alias : [ \"example.com\" , \"v1.example.com\" ] http. version String The HTTP(S) protocol version. Must be one of 'grpc' , 'http1' , or 'http2' . If omitted, then 'http1' is assumed. If using gRPC, please note that a domain must be associated with your application. image Map The image section contains parameters relating to the Docker build configuration and exposed port. image. build String or Map Build a container from a Dockerfile with optional arguments. Mutually exclusive with image.location . If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . The location field follows the same definition as the image parameter in the Amazon ECS task definition. Warning If you are passing in a Windows image, you must add platform: windows/x86_64 to your manifest. If you are passing in an ARM architecture-based image, you must add platform: linux/arm64 to your manifest. image. credentials String An optional credentials ARN for a private repository. The credentials field follows the same definition as the credentialsParameter in the Amazon ECS task definition. image. labels Map An optional key/value map of Docker labels to add to the container. image. depends_on Map An optional key/value map of Container Dependencies to add to the container. The key of the map is a container name and the value is the condition to depend on. Valid conditions are: start , healthy , complete , and success . You cannot specify a complete or success dependency on an essential container. For example: image : build : ./Dockerfile depends_on : nginx : start startup : success In the above example, the task's main container will only start after the nginx sidecar has started and the startup container has completed successfully. image. port Integer The port exposed in your Dockerfile. Copilot should parse this value for you from your EXPOSE instruction. image. healthcheck Map Optional configuration for container health checks. image.healthcheck. command Array of Strings The command to run to determine if the container is healthy. The string array can start with CMD to execute the command arguments directly, or CMD-SHELL to run the command with the container's default shell. image.healthcheck. interval Duration Time period between health checks, in seconds. Default is 10s. image.healthcheck. retries Integer Number of times to retry before container is deemed unhealthy. Default is 2. image.healthcheck. timeout Duration How long to wait before considering the health check failed, in seconds. Default is 5s. image.healthcheck. start_period Duration Length of grace period for containers to bootstrap before failed health checks count towards the maximum number of retries. Default is 0s. cpu Integer Number of CPU units for the task. See the Amazon ECS docs for valid CPU values. memory Integer Amount of memory in MiB used by the task. See the Amazon ECS docs for valid memory values. platform String Operating system and architecture (formatted as [os]/[arch] ) to pass with docker build --platform . For example, linux/arm64 or windows/x86_64 . The default is linux/x86_64 . Override the generated string to build with a different valid osfamily or architecture . For example, Windows users might change the string platform : windows/x86_64 which defaults to WINDOWS_SERVER_2019_CORE , using a map: platform : osfamily : windows_server_2019_full architecture : x86_64 count Integer or Map If you specify a number: count : 5 The service will set the desired count to 5 and maintain 5 tasks in your service. count. spot Integer If you want to use Fargate Spot capacity to run your services, you can specify a number under the spot subfield: count : spot : 5 Info Fargate Spot is not supported for containers running on ARM architecture. Alternatively, you can specify a map for setting up autoscaling: count : range : 1-10 cpu_percentage : 70 memory_percentage : 80 requests : 10000 response_time : 2s count. range String or Map You can specify a minimum and maximum bound for the number of tasks your service should maintain, based on the values you specify for the metrics. count : range : n-m This will set up an Application Autoscaling Target with the MinCapacity of n and MaxCapacity of m . Alternatively, if you wish to scale your service onto Fargate Spot instances, specify min and max under range and then specify spot_from with the desired count you wish to start placing your services onto Spot capacity. For example: count : range : min : 1 max : 10 spot_from : 3 This will set your range as 1-10 as above, but will place the first two copies of your service on dedicated Fargate capacity. If your service scales to 3 or higher, the third and any additional copies will be placed on Spot until the maximum is reached. range. min Integer The minimum desired count for your service using autoscaling. range. max Integer The maximum desired count for your service using autoscaling. range. spot_from Integer The desired count at which you wish to start placing your service using Fargate Spot capacity providers. count. cpu_percentage Integer Scale up or down based on the average CPU your service should maintain. count. memory_percentage Integer Scale up or down based on the average memory your service should maintain. count. requests Integer Scale up or down based on the request count handled per tasks. count. response_time Duration Scale up or down based on the service average response time. exec Boolean Enable running commands in your container. The default is false . Required for $ copilot svc exec . Info Exec is not supported for containers running on Windows OS. entrypoint String or Array of Strings Override the default entrypoint in the image. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the image. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] network Map The network section contains parameters for connecting to AWS resources in a VPC. network. vpc Map Subnets and security groups attached to your tasks. network.vpc. placement String Must be one of 'public' or 'private' . Defaults to launching your tasks in public subnets. Info If you launch tasks in 'private' subnets and use a Copilot-generated VPC, Copilot will automatically add NAT Gateways to your environment for internet connectivity. (See pricing .) Alternatively, when running copilot env init , you can import an existing VPC with NAT Gateways, or one with VPC endpoints for isolated workloads. See our custom environment resources page for more. network.vpc. security_groups Array of Strings Additional security group IDs associated with your tasks. Copilot always includes a security group so containers within your environment can communicate with each other. variables Map Key-value pairs that represent environment variables that will be passed to your service. Copilot will include a number of environment variables by default for you. secrets Map Key-value pairs that represent secret values from AWS Systems Manager Parameter Store that will be securely passed to your service as environment variables. storage Map The Storage section lets you specify external EFS volumes for your containers and sidecars to mount. This allows you to access persistent storage across availability zones in a region for data processing or CMS workloads. For more detail, see the storage page. You can also specify extensible ephemeral storage at the task level. storage. ephemeral Int Specify how much ephemeral task storage to provision in GiB. The default value and minimum is 20 GiB. The maximum size is 200 GiB. Sizes above 20 GiB incur additional charges. To create a shared filesystem context between an essential container and a sidecar, you can use an empty volume: storage : ephemeral : 100 volumes : scratch : path : /var/data read_only : false sidecars : mySidecar : image : public.ecr.aws/my-image:latest mount_points : - source_volume : scratch path : /var/data read_only : false This example will provision 100 GiB of storage to be shared between the sidecar and the task container. This can be useful for large datasets, or for using a sidecar to transfer data from EFS into task storage for workloads with high disk I/O requirements. storage. volumes Map Specify the name and configuration of any EFS volumes you would like to attach. The volumes field is specified as a map of the form: volumes : <volume name> : path : \"/etc/mountpath\" efs : ... storage.volumes. volume Map Specify the configuration of a volume. volume. path String Required. Specify the location in the container where you would like your volume to be mounted. Must be fewer than 242 characters and must consist only of the characters a-zA-Z0-9.-_/ . volume. read_only Boolean Optional. Defaults to true . Defines whether the volume is read-only or not. If false, the container is granted elasticfilesystem:ClientWrite permissions to the filesystem and the volume is writable. volume. efs Boolean or Map Specify more detailed EFS configuration. If specified as a boolean, or using only the uid and gid subfields, creates a managed EFS filesystem and dedicated Access Point for this workload. // Simple managed EFS efs : true // Managed EFS with custom POSIX info efs : uid : 10000 gid : 110000 volume.efs. id String Required. The ID of the filesystem you would like to mount. volume.efs. root_dir String Optional. Defaults to / . Specify the location in the EFS filesystem you would like to use as the root of your volume. Must be fewer than 255 characters and must consist only of the characters a-zA-Z0-9.-_/ . If using an access point, root_dir must be either empty or / and auth.iam must be true . volume.efs. uid Uint32 Optional. Must be specified with gid . Mutually exclusive with root_dir , auth , and id . The POSIX UID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. gid Uint32 Optional. Must be specified with uid . Mutually exclusive with root_dir , auth , and id . The POSIX GID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. auth Map Specify advanced authorization configuration for EFS. volume.efs.auth. iam Boolean Optional. Defaults to true . Whether or not to use IAM authorization to determine whether the volume is allowed to connect to EFS. volume.efs.auth. access_point_id String Optional. Defaults to \"\" . The ID of the EFS access point to connect to. If using an access point, root_dir must be either empty or / and auth.iam must be true . publish Map The publish section allows services to publish messages to one or more SNS topics. publish : topics : - name : order-events In the example above, this manifest declares an SNS topic named order-events that other worker services which are deployed to the Copilot environment can subscribe to. publish. topics Array of topics List of topic objects. publish.topics. topic Map Holds configuration for a single SNS topic. topic. name String Required. The name of the SNS topic. Must contain only upper and lowercase letters, numbers, hyphens, and underscores. logging Map The logging section contains log configuration. You can also configure parameters for your container's FireLens log driver in this section (see examples here ). logging. retention Integer Optional. The number of days to retain the log events. See this page for all accepted values. If omitted, the default is 30. logging. image Map Optional. The Fluent Bit image to use. Defaults to amazon/aws-for-fluent-bit:latest . logging. destination Map Optional. The configuration options to send to the FireLens log driver. logging. enableMetadata Map Optional. Whether to include ECS metadata in logs. Defaults to true . logging. secretOptions Map Optional. The secrets to pass to the log configuration. logging. configFilePath Map Optional. The full config file path in your custom Fluent Bit image. taskdef_overrides Array of Rules The taskdef_overrides section allows users to apply overriding rules to their ECS Task Definitions (see examples here ). taskdef_overrides. path String Required. Path to the Task Definition field to override. taskdef_overrides. value Any Required. Value of the Task Definition field to override. environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the count parameter so that we can run 2 copies of our service in our 'prod' environment, and 2 copies using Fargate Spot capacity in our 'staging' environment.","title":"Load Balanced Web Service"},{"location":"docs/manifest/overview","text":"Manifest The AWS Copilot CLI manifest describes a service\u2019s or job's architecture as infrastructure-as-code. It is a file generated from copilot init , copilot svc init , or copilot job init that gets converted to an AWS CloudFormation template. Unlike raw CloudFormation templates, the manifest allows you to focus on the most common settings for the architecture of your service or job, and not the individual resources. Manifest files are stored under copilot/<your service or job name>/manifest.yml .","title":"Overview"},{"location":"docs/manifest/overview#manifest","text":"The AWS Copilot CLI manifest describes a service\u2019s or job's architecture as infrastructure-as-code. It is a file generated from copilot init , copilot svc init , or copilot job init that gets converted to an AWS CloudFormation template. Unlike raw CloudFormation templates, the manifest allows you to focus on the most common settings for the architecture of your service or job, and not the individual resources. Manifest files are stored under copilot/<your service or job name>/manifest.yml .","title":"Manifest"},{"location":"docs/manifest/pipeline","text":"List of all available properties for a Copilot pipeline manifest. To learn more about pipelines, see the Pipelines concept page. Sample manifest for a pipeline triggered from a GitHub repo name : pipeline-sample-app-frontend version : 1 source : provider : GitHub properties : branch : main repository : https://github.com/<user>/sample-app-frontend # Optional: specify the name of an existing CodeStar Connections connection. connection_name : a-connection build : image : aws/codebuild/amazonlinux2-x86_64-standard:3.0 stages : - name : test test_commands : - make test - echo \"woo! Tests passed\" - name : prod requires_approval : true name String The name of your pipeline. version String The schema version for the template. There is only one version, 1 , supported at the moment. source Map Configuration for how your pipeline is triggered. source. provider String The name of your provider. Currently, GitHub , Bitbucket , and CodeCommit are supported. source. properties Map Provider-specific configuration on how the pipeline is triggered. source.properties. access_token_secret String The name of AWS Secrets Manager secret that holds the GitHub access token to trigger the pipeline if your provider is GitHub and you created your pipeline with a personal access token. Info As of AWS Copilot v1.4.0, the access token is no longer needed for GitHub repository sources. Instead, Copilot will trigger the pipeline using AWS CodeStar connections . source.properties. branch String The name of the branch in your repository that triggers the pipeline. The default branch name is main . source.properties. repository String The URL of your repository. source.properties. connection_name String The name of an existing CodeStar Connections connection. If omitted, Copilot will generate a connection for you. source.properties. output_artifact_format String Optional. The output artifact format. Values can be either CODEBUILD_CLONE_REF or CODE_ZIP . If omitted, the default is CODE_ZIP . Info This property is not available for pipelines with GitHub version 1 source actions, which use access_token_secret . build Map Configuration for CodeBuild project. build. image String The URI that identifies the Docker image to use for this build project. As of now, aws/codebuild/amazonlinux2-x86_64-standard:3.0 is used by default. stages Array of Maps Ordered list of environments that your pipeline will deploy to. stages. name String The name of an environment to deploy your services to. stages. requires_approval Boolean Indicates whether to add a manual approval step before the deployment. stages. test_commands Array of Strings Commands to run integration or end-to-end tests after deployment.","title":"Pipeline"},{"location":"docs/manifest/rd-web-service","text":"List of all available properties for a 'Request-Driven Web Service' manifest. Sample manifest for a frontend service # Your service name will be used in naming your resources like log groups, App Runner services, etc. name : frontend # The \"architecture\" of the service you're running. type : Request-Driven Web Service http : healthcheck : path : '/_healthcheck' healthy_threshold : 3 unhealthy_threshold : 5 interval : 10s timeout : 5s alias : web.example.com # Configuration for your containers and service. image : build : ./frontend/Dockerfile port : 80 cpu : 1024 memory : 2048 variables : LOG_LEVEL : info tags : owner : frontend-team environments : test : LOG_LEVEL : debug name String The name of your service. type String The architecture type for your service. A Request-Driven Web Service is an internet-facing service that is deployed on AWS App Runner. http Map The http section contains parameters related to the managed load balancer. http. healthcheck String or Map If you specify a string, Copilot interprets it as the path exposed in your container to handle target group health check requests. The default is \"/\". http : healthcheck : '/' You can also specify healthcheck as a map: http : healthcheck : path : '/' healthy_threshold : 3 unhealthy_threshold : 2 interval : 15s timeout : 10s http.healthcheck. path String The destination that the health check requests are sent to. http.healthcheck. healthy_threshold Integer The number of consecutive health check successes required before considering an unhealthy target healthy. The default is 3. Range: 1-20. http.healthcheck. unhealthy_threshold Integer The number of consecutive health check failures required before considering a target unhealthy. The default is 3. Range: 1-20. http.healthcheck. interval Duration The approximate amount of time, in seconds, between health checks of an individual target. The default is 5s. Range: 1s\u201320s. http.healthcheck. timeout Duration The amount of time, in seconds, during which no response from a target means a failed health check. The default is 2s. Range 1s-20s. http. alias String Assign a friendly domain name to your request-driven web services. To learn more see developing/domain . image Map The image section contains parameters relating to the Docker build configuration and exposed port. image. build String or Map If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . Note Only public images stored in Amazon ECR Public are available with AWS App Runner. image. port Integer The port exposed in your Dockerfile. Copilot should parse this value for you from your EXPOSE instruction. cpu Integer Number of CPU units reserved for each instance of your service. See the AWS App Runner docs for valid CPU values. memory Integer Amount of memory in MiB reserved for each instance of your service. See the AWS App Runner docs for valid memory values. command String Optional. Override the default command in the image. variables Map Key-value pairs that represent environment variables that will be passed to your service. Copilot will include a number of environment variables by default for you. publish Map The publish section allows services to publish messages to one or more SNS topics. publish : topics : - name : order-events In the example above, this manifest declares an SNS topic named order-events that other worker services which are deployed to the Copilot environment can subscribe to. publish. topics Array of topics List of topic objects. publish.topics. topic Map Holds configuration for a single SNS topic. topic. name String Required. The name of the SNS topic. Must contain only upper and lowercase letters, numbers, hyphens, and underscores. tags Map Key-value pairs representing AWS tags that are passed down to your AWS App Runner resources. environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the LOG_LEVEL environment variable in our 'test' environment.","title":"Request-Driven Web Service"},{"location":"docs/manifest/scheduled-job","text":"List of all available properties for a 'Scheduled Job' manifest. To learn about Copilot jobs, see the Jobs concept page. Sample manifest for a report generator cronjob # Your job name will be used in naming your resources like log groups, ECS Tasks, etc. name : report-generator type : Scheduled Job on : schedule : \"@daily\" cpu : 256 memory : 512 retries : 3 timeout : 1h image : # Path to your service's Dockerfile. build : ./Dockerfile variables : LOG_LEVEL : info secrets : GITHUB_TOKEN : GITHUB_TOKEN # You can override any of the values defined above by environment. environments : prod : cpu : 2048 # Larger CPU value for prod environment memory : 4096 name String The name of your job. type String The architecture type for your job. Currently, Copilot only supports the \"Scheduled Job\" type for tasks that are triggered either on a fixed schedule or periodically. on Map The configuration for the event that triggers your job. on. schedule String You can specify a rate to periodically trigger your job. Supported rates: Rate Identical to In human-readable text and UTC , it runs ... \"@yearly\" \"cron(0 * * * ? *)\" at midnight on January 1st \"@monthly\" \"cron(0 0 1 * ? *)\" at midnight on the first day of the month \"@weekly\" \"cron(0 0 ? * 1 *)\" at midnight on Sunday \"@daily\" \"cron(0 0 * * ? *)\" at midnight \"@hourly\" \"cron(0 * * * ? *)\" at minute 0 \"@every {duration}\" (For example, \"1m\", \"5m\") \"rate({duration})\" based on CloudWatch's rate expressions Alternatively, you can specify a cron schedule if you'd like to trigger the job at a specific time: \"* * * * *\" based on the standard cron format . \"cron({fields})\" based on CloudWatch's cron expressions with six fields. image Map The image section contains parameters relating to the Docker build configuration and exposed port. image. build String or Map Build a container from a Dockerfile with optional arguments. Mutually exclusive with image.location . If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . The location field follows the same definition as the image parameter in the Amazon ECS task definition. Warning If you are passing in a Windows image, you must add platform: windows/x86_64 to your manifest. If you are passing in an ARM architecture-based image, you must add platform: linux/arm64 to your manifest. image. credentials String An optional credentials ARN for a private repository. The credentials field follows the same definition as the credentialsParameter in the Amazon ECS task definition. image. labels Map An optional key/value map of Docker labels to add to the container. image. depends_on Map An optional key/value map of Container Dependencies to add to the container. The key of the map is a container name and the value is the condition to depend on. Valid conditions are: start , healthy , complete , and success . You cannot specify a complete or success dependency on an essential container. For example: image : build : ./Dockerfile depends_on : nginx : start startup : success In the above example, the task's main container will only start after the nginx sidecar has started and the startup container has completed successfully. entrypoint String or Array of Strings Override the default entrypoint in the image. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the image. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] cpu Integer Number of CPU units for the task. See the Amazon ECS docs for valid CPU values. memory Integer Amount of memory in MiB used by the task. See the Amazon ECS docs for valid memory values. platform String Operating system and architecture (formatted as [os]/[arch] ) to pass with docker build --platform . For example, linux/arm64 or windows/x86_64 . The default is linux/x86_64 . Override the generated string to build with a different valid osfamily or architecture . For example, Windows users might change the string platform : windows/x86_64 which defaults to WINDOWS_SERVER_2019_CORE , using a map: platform : osfamily : windows_server_2019_full architecture : x86_64 retries Integer The number of times to retry the job before failing. timeout Duration How long the job should run before it aborts and fails. You can use the units: h , m , or s . network Map The network section contains parameters for connecting to AWS resources in a VPC. network. vpc Map Subnets and security groups attached to your tasks. network.vpc. placement String Must be one of 'public' or 'private' . Defaults to launching your tasks in public subnets. Info If you launch tasks in 'private' subnets and use a Copilot-generated VPC, Copilot will automatically add NAT Gateways to your environment for internet connectivity. (See pricing .) Alternatively, when running copilot env init , you can import an existing VPC with NAT Gateways, or one with VPC endpoints for isolated workloads. See our custom environment resources page for more. network.vpc. security_groups Array of Strings Additional security group IDs associated with your tasks. Copilot always includes a security group so containers within your environment can communicate with each other. variables Map Key-value pairs that represent environment variables that will be passed to your job. Copilot will include a number of environment variables by default for you. secrets Map Key-value pairs that represent secret values from AWS Systems Manager Parameter Store that will be securely passed to your job as environment variables. storage Map The Storage section lets you specify external EFS volumes for your containers and sidecars to mount. This allows you to access persistent storage across regions for data processing or CMS workloads. For more detail, see the storage page. storage. volumes Map Specify the name and configuration of any EFS volumes you would like to attach. The volumes field is specified as a map of the form: volumes : <volume name> : path : \"/etc/mountpath\" efs : ... storage.volumes. volume Map Specify the configuration of a volume. volume. path String Required. Specify the location in the container where you would like your volume to be mounted. Must be fewer than 242 characters and must consist only of the characters a-zA-Z0-9.-_/ . volume. read_only Boolean Optional. Defaults to true . Defines whether the volume is read-only or not. If false, the container is granted elasticfilesystem:ClientWrite permissions to the filesystem and the volume is writable. volume. efs Map Specify more detailed EFS configuration. volume.efs. id String Required. The ID of the filesystem you would like to mount. volume.efs. root_dir String Optional. Defaults to / . Specify the location in the EFS filesystem you would like to use as the root of your volume. Must be fewer than 255 characters and must consist only of the characters a-zA-Z0-9.-_/ . If using an access point, root_dir must be either empty or / and auth.iam must be true . volume.efs. auth Map Specify advanced authorization configuration for EFS. volume.efs.auth. iam Boolean Optional. Defaults to true . Whether or not to use IAM authorization to determine whether the volume is allowed to connect to EFS. volume.efs.auth. access_point_id String Optional. Defaults to \"\" . The ID of the EFS access point to connect to. If using an access point, root_dir must be either empty or / and auth.iam must be true . logging Map The logging section contains log configuration parameters for your container's FireLens log driver (see examples here ). logging. image Map Optional. The Fluent Bit image to use. Defaults to amazon/aws-for-fluent-bit:latest . logging. destination Map Optional. The configuration options to send to the FireLens log driver. logging. enableMetadata Map Optional. Whether to include ECS metadata in logs. Defaults to true . logging. secretOptions Map Optional. The secrets to pass to the log configuration. logging. configFilePath Map Optional. The full config file path in your custom Fluent Bit image. publish Map The publish section allows services to publish messages to one or more SNS topics. publish : topics : - name : order-events In the example above, this manifest declares an SNS topic named order-events that other worker services which are deployed to the Copilot environment can subscribe to. publish. topics Array of topics List of topic objects. publish.topics. topic Map Holds configuration for a single SNS topic. topic. name String Required. The name of the SNS topic. Must contain only upper and lowercase letters, numbers, hyphens, and underscores. environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the CPU parameter so that our production container is more performant.","title":"Scheduled Job"},{"location":"docs/manifest/worker-service","text":"List of all available properties for a 'Worker Service' manifest. To learn about Copilot services, see the Services concept page. Sample manifest for a worker service # Your service name will be used in naming your resources like log groups, ECS services, etc. name : orders-worker type : Worker Service image : build : ./orders/Dockerfile subscribe : topics : - name : events service : api - name : events service : fe queue : retention : 96h timeout : 30s dead_letter : tries : 10 cpu : 256 memory : 512 count : 1 variables : LOG_LEVEL : info secrets : GITHUB_TOKEN : GITHUB_TOKEN # You can override any of the values defined above by environment. environments : production : count : range : min : 1 max : 50 spot_from : 26 queue_delay : acceptable_latency : 1m msg_processing_time : 250ms name String The name of your service. type String The architecture type for your service. Worker Services are not reachable from the internet or elsewhere in the VPC. They are designed to pull messages from their associated SQS queues, which are populated by their subscriptions to SNS topics created by other Copilot services' publish fields. subscribe Map The subscribe section allows worker services to create subscriptions to the SNS topics exposed by other Copilot services in the same application and environment. Each topic can define its own SQS queue, but by default all topics are subscribed to the worker service's default queue. subscribe : topics : - name : events service : api queue : # Define a topic-specific queue for the api-events topic. timeout : 20s - name : events service : fe queue : # By default, messages from all topics will go to a shared queue. timeout : 45s retention : 96h delay : 30s subscribe. queue Map By default, a service level queue is always created. queue allows customization of certain attributes of that default queue. subscribe.queue. delay Duration The time in seconds for which the delivery of all messages in the queue is delayed. Default 0s. Range 0s-15m. subscribe.queue. retention Duration Retention specifies the time a message will remain in the queue before being deleted. Default 4d. Range 60s-336h. subscribe.queue. timeout Duration Timeout defines the length of time a message is unavailable after being delivered. Default 30s. Range 0s-12h. subscribe.queue.dead_letter. tries Integer If specified, creates a dead letter queue and a redrive policy which routes messages to the DLQ after tries attempts. That is, if a worker service fails to process a message successfully tries times, it will be routed to the DLQ for examination instead of redriven. subscribe. topics Array of topic s Contains information about which SNS topics the worker service should subscribe to. topic. name String Required. The name of the SNS topic to subscribe to. topic. service String Required. The service this SNS topic is exposed by. Together with the topic name, this uniquely identifies an SNS topic in the copilot environment. topic. queue Boolean or Map Optional. Specify SQS queue configuration for the topic. If specified as true , the queue will be created with default configuration. Specify this field as a map for customization of certain attributes for this topic-specific queue. image Map The image section contains parameters relating to the Docker build configuration and exposed port. image. build String or Map Build a container from a Dockerfile with optional arguments. Mutually exclusive with image.location . If you specify a string, Copilot interprets it as the path to your Dockerfile. It will assume that the dirname of the string you specify should be the build context. The manifest: image : build : path/to/dockerfile will result in the following call to docker build: $ docker build --file path/to/dockerfile path/to You can also specify build as a map: image : build : dockerfile : path/to/dockerfile context : context/dir target : build-stage cache_from : - image:tag args : key : value In this case, Copilot will use the context directory you specified and convert the key-value pairs under args to --build-arg overrides. The equivalent docker build call will be: $ docker build --file path/to/dockerfile --target build-stage --cache-from image:tag --build-arg key=value context/dir . You can omit fields and Copilot will do its best to understand what you mean. For example, if you specify context but not dockerfile , Copilot will run Docker in the context directory and assume that your Dockerfile is named \"Dockerfile.\" If you specify dockerfile but no context , Copilot assumes you want to run Docker in the directory that contains dockerfile . All paths are relative to your workspace root. image. location String Instead of building a container from a Dockerfile, you can specify an existing image name. Mutually exclusive with image.build . The location field follows the same definition as the image parameter in the Amazon ECS task definition. Warning If you are passing in a Windows image, you must add platform: windows/x86_64 to your manifest. If you are passing in an ARM architecture-based image, you must add platform: linux/arm64 to your manifest. image. credentials String An optional credentials ARN for a private repository. The credentials field follows the same definition as the credentialsParameter in the Amazon ECS task definition. image. labels Map An optional key/value map of Docker labels to add to the container. image. depends_on Map An optional key/value map of Container Dependencies to add to the container. The key of the map is a container name and the value is the condition to depend on. Valid conditions are: start , healthy , complete , and success . You cannot specify a complete or success dependency on an essential container. For example: image : build : ./Dockerfile depends_on : nginx : start startup : success In the above example, the task's main container will only start after the nginx sidecar has started and the startup container has completed successfully. image. healthcheck Map Optional configuration for container health checks. image.healthcheck. command Array of Strings The command to run to determine if the container is healthy. The string array can start with CMD to execute the command arguments directly, or CMD-SHELL to run the command with the container's default shell. image.healthcheck. interval Duration Time period between health checks, in seconds. Default is 10s. image.healthcheck. retries Integer Number of times to retry before container is deemed unhealthy. Default is 2. image.healthcheck. timeout Duration How long to wait before considering the health check failed, in seconds. Default is 5s. image.healthcheck. start_period Duration Length of grace period for containers to bootstrap before failed health checks count towards the maximum number of retries. Default is 0s. cpu Integer Number of CPU units for the task. See the Amazon ECS docs for valid CPU values. memory Integer Amount of memory in MiB used by the task. See the Amazon ECS docs for valid memory values. platform String Operating system and architecture (formatted as [os]/[arch] ) to pass with docker build --platform . For example, linux/arm64 or windows/x86_64 . The default is linux/x86_64 . Override the generated string to build with a different valid osfamily or architecture . For example, Windows users might change the string platform : windows/x86_64 which defaults to WINDOWS_SERVER_2019_CORE , using a map: platform : osfamily : windows_server_2019_full architecture : x86_64 count Integer or Map If you specify a number: count : 5 The service will set the desired count to 5 and maintain 5 tasks in your service. count. spot Integer If you want to use Fargate Spot capacity to run your services, you can specify a number under the spot subfield: count : spot : 5 Info Fargate Spot is not supported for containers running on ARM architecture. Alternatively, you can specify a map for setting up autoscaling: count : range : 1-10 cpu_percentage : 70 memory_percentage : 80 queue_delay : acceptable_latency : 10m msg_processing_time : 250ms count. range String or Map You can specify a minimum and maximum bound for the number of tasks your service should maintain, based on the values you specify for the metrics. count : range : n-m This will set up an Application Autoscaling Target with the MinCapacity of n and MaxCapacity of m . Alternatively, if you wish to scale your service onto Fargate Spot instances, specify min and max under range and then specify spot_from with the desired count you wish to start placing your services onto Spot capacity. For example: count : range : min : 1 max : 10 spot_from : 3 This will set your range as 1-10 as above, but will place the first two copies of your service on dedicated Fargate capacity. If your service scales to 3 or higher, the third and any additional copies will be placed on Spot until the maximum is reached. range. min Integer The minimum desired count for your service using autoscaling. range. max Integer The maximum desired count for your service using autoscaling. range. spot_from Integer The desired count at which you wish to start placing your service using Fargate Spot capacity providers. count. cpu_percentage Integer Scale up or down based on the average CPU your service should maintain. count. memory_percentage Integer Scale up or down based on the average memory your service should maintain. count. queue_delay Integer Scale up or down to maintain an acceptable queue latency by tracking against the acceptable backlog per task. The acceptable backlog per task is calculated by dividing acceptable_latency by msg_processing_time . For example, if you can tolerate consuming a message within 10 minutes of its arrival and it takes your task on average 250 milliseconds to process a message, then acceptableBacklogPerTask = 10 * 60 / 0.25 = 2400 . Therefore, each task can hold up to 2,400 messages. A target tracking policy is set up on your behalf to ensure your service scales up and down to maintain <= 2400 messages per task. To learn more see docs . count.queue_delay. acceptable_latency Duration The acceptable amount of time that a message can sit in the queue. For example, \"45s\" , \"5m\" , 10h . count.queue_delay. msg_processing_time Duration The average amount of time it takes to process an SQS message. For example, \"250ms\" , \"1s\" . exec Boolean Enable running commands in your container. The default is false . Required for $ copilot svc exec . Info Exec is not supported for containers running on Windows OS. entrypoint String or Array of Strings Override the default entrypoint in the image. # String version. entrypoint : \"/bin/entrypoint --p1 --p2\" # Alteratively, as an array of strings. entrypoint : [ \"/bin/entrypoint\" , \"--p1\" , \"--p2\" ] command String or Array of Strings Override the default command in the image. # String version. command : ps au # Alteratively, as an array of strings. command : [ \"ps\" , \"au\" ] network Map The network section contains parameters for connecting to AWS resources in a VPC. network. vpc Map Subnets and security groups attached to your tasks. network.vpc. placement String Must be one of 'public' or 'private' . Defaults to launching your tasks in public subnets. Info If you launch tasks in 'private' subnets and use a Copilot-generated VPC, Copilot will automatically add NAT Gateways to your environment for internet connectivity. (See pricing .) Alternatively, when running copilot env init , you can import an existing VPC with NAT Gateways, or one with VPC endpoints for isolated workloads. See our custom environment resources page for more. network.vpc. security_groups Array of Strings Additional security group IDs associated with your tasks. Copilot always includes a security group so containers within your environment can communicate with each other. variables Map Key-value pairs that represent environment variables that will be passed to your service. Copilot will include a number of environment variables by default for you. secrets Map Key-value pairs that represent secret values from AWS Systems Manager Parameter Store that will be securely passed to your service as environment variables. storage Map The Storage section lets you specify external EFS volumes for your containers and sidecars to mount. This allows you to access persistent storage across availability zones in a region for data processing or CMS workloads. For more detail, see the storage page. You can also specify extensible ephemeral storage at the task level. storage. ephemeral Int Specify how much ephemeral task storage to provision in GiB. The default value and minimum is 20 GiB. The maximum size is 200 GiB. Sizes above 20 GiB incur additional charges. To create a shared filesystem context between an essential container and a sidecar, you can use an empty volume: storage : ephemeral : 100 volumes : scratch : path : /var/data read_only : false sidecars : mySidecar : image : public.ecr.aws/my-image:latest mount_points : - source_volume : scratch path : /var/data read_only : false This example will provision 100 GiB of storage to be shared between the sidecar and the task container. This can be useful for large datasets, or for using a sidecar to transfer data from EFS into task storage for workloads with high disk I/O requirements. storage. volumes Map Specify the name and configuration of any EFS volumes you would like to attach. The volumes field is specified as a map of the form: volumes : <volume name> : path : \"/etc/mountpath\" efs : ... storage.volumes. volume Map Specify the configuration of a volume. volume. path String Required. Specify the location in the container where you would like your volume to be mounted. Must be fewer than 242 characters and must consist only of the characters a-zA-Z0-9.-_/ . volume. read_only Boolean Optional. Defaults to true . Defines whether the volume is read-only or not. If false, the container is granted elasticfilesystem:ClientWrite permissions to the filesystem and the volume is writable. volume. efs Boolean or Map Specify more detailed EFS configuration. If specified as a boolean, or using only the uid and gid subfields, creates a managed EFS filesystem and dedicated Access Point for this workload. // Simple managed EFS efs : true // Managed EFS with custom POSIX info efs : uid : 10000 gid : 110000 volume.efs. id String Required. The ID of the filesystem you would like to mount. volume.efs. root_dir String Optional. Defaults to / . Specify the location in the EFS filesystem you would like to use as the root of your volume. Must be fewer than 255 characters and must consist only of the characters a-zA-Z0-9.-_/ . If using an access point, root_dir must be either empty or / and auth.iam must be true . volume.efs. uid Uint32 Optional. Must be specified with gid . Mutually exclusive with root_dir , auth , and id . The POSIX UID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. gid Uint32 Optional. Must be specified with uid . Mutually exclusive with root_dir , auth , and id . The POSIX GID to use for the dedicated access point created for the managed EFS filesystem. volume.efs. auth Map Specify advanced authorization configuration for EFS. volume.efs.auth. iam Boolean Optional. Defaults to true . Whether or not to use IAM authorization to determine whether the volume is allowed to connect to EFS. volume.efs.auth. access_point_id String Optional. Defaults to \"\" . The ID of the EFS access point to connect to. If using an access point, root_dir must be either empty or / and auth.iam must be true . publish Map The publish section allows services to publish messages to one or more SNS topics. publish : topics : - name : order-events In the example above, this manifest declares an SNS topic named order-events that other worker services which are deployed to the Copilot environment can subscribe to. publish. topics Array of topics List of topic objects. publish.topics. topic Map Holds configuration for a single SNS topic. topic. name String Required. The name of the SNS topic. Must contain only upper and lowercase letters, numbers, hyphens, and underscores. logging Map The logging section contains log configuration. You can also configure parameters for your container's FireLens log driver in this section (see examples here ). logging. retention Integer Optional. The number of days to retain the log events. See this page for all accepted values. If omitted, the default is 30. logging. image Map Optional. The Fluent Bit image to use. Defaults to amazon/aws-for-fluent-bit:latest . logging. destination Map Optional. The configuration options to send to the FireLens log driver. logging. enableMetadata Map Optional. Whether to include ECS metadata in logs. Defaults to true . logging. secretOptions Map Optional. The secrets to pass to the log configuration. logging. configFilePath Map Optional. The full config file path in your custom Fluent Bit image. taskdef_overrides Array of Rules The taskdef_overrides section allows users to apply overriding rules to their ECS Task Definitions (see examples here ). taskdef_overrides. path String Required. Path to the Task Definition field to override. taskdef_overrides. value Any Required. Value of the Task Definition field to override. environments Map The environment section lets you override any value in your manifest based on the environment you're in. In the example manifest above, we're overriding the count parameter so that we can run 2 copies of our service in our 'prod' environment, and 2 copies using Fargate Spot capacity in our 'staging' environment.","title":"Worker Service"}]}